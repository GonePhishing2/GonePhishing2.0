{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML libraries\n",
    "import pycaret\n",
    "import xgboost\n",
    "\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.functions import datediff, to_date, col, expr\n",
    "\n",
    "# Import Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "# from pycaret.classification import setup, compare_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open(\"connection.json\"))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PURCHASE_DOCUMENT_ITEM_ID\"  |\"CREATE_DATE\"  |\"COMPANY_CODE_ID\"  |\"VENDOR_ID\"  |\"POSTAL_CD\"  |\"MATERIAL_ID\"  |\"SUB_COMMODITY_DESC\"                    |\"MRP_TYPE_ID\"  |\"PLANT_ID\"  |\"REQUESTED_DELIVERY_DATE\"  |\"INBOUND_DELIVERY_ID\"  |\"INBOUND_DELIVERY_ITEM_ID\"  |\"PLANNED_DELIVERY_DAYS\"  |\"FIRST_GR_POSTING_DATE\"  |\"TARGET_FEATURE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|10                           |20210210       |US39               |8010000984   |60045-5202   |NULL           |Tools                                   |NULL           |4120        |20210215                   |NULL                   |0                           |0.0                      |20210211                 |-4                |\n",
      "|20                           |20210210       |US10               |8010000984   |60045-5202   |NULL           |Machinery & Equipment                   |NULL           |4014        |20210214                   |185520728              |20                          |1.0                      |20210212                 |-2                |\n",
      "|20                           |20210210       |CA10               |V4138        |19973        |2100032775     |Tolling                                 |1              |4007        |20210216                   |185529650              |10                          |2.0                      |20210216                 |0                 |\n",
      "|10                           |20210210       |CA10               |8010002419   |H9J 4A1      |1100119629     |Valves                                  |1              |4036        |20210224                   |NULL                   |0                           |14.0                     |20210406                 |41                |\n",
      "|30                           |20210210       |CA10               |8010002454   |K7L 4Y5      |3100006053     |Tubes & Cores                           |1              |4036        |20210209                   |NULL                   |0                           |1.0                      |20210209                 |0                 |\n",
      "|10                           |20210210       |US10               |NULL         |NULL         |2100013224     |Additives, Colorants & Catalysts        |1              |4014        |20210211                   |185519500              |900002                      |1.0                      |20210216                 |5                 |\n",
      "|10                           |20210210       |US40               |8010099572   |53821        |NULL           |Transportation, Storage, Mail Services  |NULL           |4138        |20211231                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010005511   |77630-1818   |NULL           |Pumps & Compressors                     |NULL           |4120        |20210210                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010001167   |77705-1129   |NULL           |Maintenance Services                    |NULL           |4120        |20211129                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010004468   |77507        |1100162838     |Piping & Tubing                         |1              |4120        |20210216                   |185562735              |10                          |6.0                      |20210305                 |17                |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PURCHASE_DOCUMENT_ITEM_ID\"  |\"CREATE_DATE\"  |\"COMPANY_CODE_ID\"  |\"VENDOR_ID\"  |\"POSTAL_CD\"  |\"MATERIAL_ID\"  |\"SUB_COMMODITY_DESC\"                    |\"MRP_TYPE_ID\"  |\"PLANT_ID\"  |\"REQUESTED_DELIVERY_DATE\"  |\"INBOUND_DELIVERY_ID\"  |\"INBOUND_DELIVERY_ITEM_ID\"  |\"PLANNED_DELIVERY_DAYS\"  |\"FIRST_GR_POSTING_DATE\"  |\"TARGET_FEATURE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|30                           |20210703       |CA10               |NULL         |NULL         |2100032311     |Tolling                                 |1              |4007        |20210703                   |185821762              |30                          |0.0                      |20210703                 |0                 |\n",
      "|30                           |20210703       |CA10               |NULL         |NULL         |2100032311     |Tolling                                 |1              |4007        |20210705                   |185821762              |30                          |0.0                      |20210703                 |-2                |\n",
      "|20                           |20210703       |GB10               |8010023829   |59229        |1100197903     |Spinning                                |1              |3009        |20210724                   |NULL                   |0                           |30.0                     |20210716                 |-8                |\n",
      "|10                           |20210703       |GB10               |NULL         |NULL         |2100015291     |Resins & Polymers                       |2              |3024        |20210705                   |185822217              |10                          |0.0                      |20210703                 |-2                |\n",
      "|10                           |20210703       |GB10               |8010023829   |59229        |1100197899     |Spinning                                |1              |3009        |20210714                   |NULL                   |0                           |28.0                     |20210716                 |2                 |\n",
      "|50                           |20210703       |CA10               |NULL         |NULL         |2100011410     |Tolling                                 |1              |4007        |20210706                   |185823356              |50                          |0.0                      |20210704                 |-2                |\n",
      "|10                           |20210703       |CA10               |8010001620   |M5W 1P1      |NULL           |Transportation, Storage, Mail Services  |NULL           |4036        |20210703                   |NULL                   |0                           |0.0                      |20210703                 |0                 |\n",
      "|50                           |20210703       |CA10               |NULL         |NULL         |2100021799     |Tolling                                 |1              |4007        |20210705                   |185821762              |50                          |0.0                      |20210703                 |-2                |\n",
      "|10                           |20210703       |GB10               |NULL         |NULL         |2100015294     |Resins & Polymers                       |2              |3024        |20210703                   |185822220              |10                          |0.0                      |20210703                 |0                 |\n",
      "|10                           |20210703       |CA10               |8010001620   |M5W 1P1      |NULL           |Transportation, Storage, Mail Services  |NULL           |4036        |20210703                   |NULL                   |0                           |0.0                      |20210703                 |0                 |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to the correct table\n",
    "tableName = 'PURCHASE_ORDER_HISTORY'\n",
    "dataframe = session.table(tableName)\n",
    "\n",
    "# Calculation to find the lag between Planned Delivery from Actual Delivery\n",
    "dataframe = dataframe.withColumn(\"target_feature\",\n",
    "                                    datediff('day', \n",
    "                                            col(\"DELIVERY_DATE_ML\"), \n",
    "                                            col(\"FIRST_GR_POSTING_DATE_ML\")))\n",
    "\n",
    "\n",
    "# Example: Selecting specific columns\n",
    "# This selects only a subset of columns. Adjust the column names as needed.\n",
    "filtered_dataframe = dataframe.select(\n",
    "    col(\"PURCHASE_DOCUMENT_ITEM_ID\"), # ID for purchase order\n",
    "    col(\"CREATE_DATE\"),            # day purchase order was created\n",
    "    col(\"COMPANY_CODE_ID\"),           # copmany w/in INVISTA making purchase\n",
    "    col(\"VENDOR_ID\"),                 # ID of the vendor \"we\" are purchasing from\n",
    "    col(\"POSTAL_CD\"),                 # postal code associated w company code ID\n",
    "    col(\"MATERIAL_ID\"),               # ID of material being purchase\n",
    "    col(\"SUB_COMMODITY_DESC\"),        # description of sub commodity\n",
    "    col(\"MRP_TYPE_ID\"),               # determined if material is reordered manually or automatically\n",
    "    col(\"PLANT_ID\"),                  # ID of plant making purchase\n",
    "    col(\"REQUESTED_DELIVERY_DATE\"),# delivery date from requisition\n",
    "    col(\"INBOUND_DELIVERY_ID\"),       # ID for delivery\n",
    "    col(\"INBOUND_DELIVERY_ITEM_ID\"),  # ID of item w/in delivery\n",
    "    col(\"PLANNED_DELIVERY_DAYS\"),     # Amount of days expected to take\n",
    "    col(\"FIRST_GR_POSTING_DATE\"),  # expected delivery date        \n",
    "    col(\"target_feature\")             # Lag between Planned Delivery from Actual Delivery \n",
    ")\n",
    "\n",
    "\n",
    "# Print a sample of the filtered dataframe to standard output.\n",
    "filtered_dataframe.show()\n",
    "\n",
    "# Optionally, you might want to filter rows based on some conditions\n",
    "# Example: Filtering out rows where FIRST_GR_POSTING_DATE_ML is NULL\n",
    "filtered_dataframe = filtered_dataframe.filter(col(\"FIRST_GR_POSTING_DATE\").is_not_null())\n",
    "\n",
    "# filtered_dataframe = filtered_dataframe[filtered_dataframe['PLANNED_DELIVERY_DAYS'] < 6]\n",
    "\n",
    "# Show the DataFrame after filtering\n",
    "filtered_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"COMPANY_CODE_ID\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"VENDOR_ID\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"POSTAL_CD\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"SUB_COMMODITY_DESC\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"PLANNED_DELIVERY_DAYS\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'filtered_dataframe' is the DataFrame you've prepared in Snowflake\n",
    "# Convert the Snowpark DataFrame to a Pandas DataFrame with consideration for NULL values\n",
    "\n",
    "# Convert DataFrame to Pandas, handling NULL values by allowing float conversion\n",
    "df = filtered_dataframe.fillna(0).to_pandas()  # This replaces NULL with 0 before conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_DOCUMENT_ITEM_ID</th>\n",
       "      <th>CREATE_DATE</th>\n",
       "      <th>COMPANY_CODE_ID</th>\n",
       "      <th>VENDOR_ID</th>\n",
       "      <th>POSTAL_CD</th>\n",
       "      <th>MATERIAL_ID</th>\n",
       "      <th>SUB_COMMODITY_DESC</th>\n",
       "      <th>MRP_TYPE_ID</th>\n",
       "      <th>PLANT_ID</th>\n",
       "      <th>REQUESTED_DELIVERY_DATE</th>\n",
       "      <th>INBOUND_DELIVERY_ID</th>\n",
       "      <th>INBOUND_DELIVERY_ITEM_ID</th>\n",
       "      <th>PLANNED_DELIVERY_DAYS</th>\n",
       "      <th>FIRST_GR_POSTING_DATE</th>\n",
       "      <th>TARGET_FEATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CA10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2100021412</td>\n",
       "      <td>Tolling</td>\n",
       "      <td>1</td>\n",
       "      <td>4007</td>\n",
       "      <td>20210331</td>\n",
       "      <td>185610163</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210330</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN20</td>\n",
       "      <td>8010094262</td>\n",
       "      <td>201799</td>\n",
       "      <td>0</td>\n",
       "      <td>Telecommunications media services</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20210331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN20</td>\n",
       "      <td>8010098163</td>\n",
       "      <td>200333</td>\n",
       "      <td>0</td>\n",
       "      <td>Maintenance Services</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20210406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210511</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN16</td>\n",
       "      <td>8010019798</td>\n",
       "      <td>201604</td>\n",
       "      <td>0</td>\n",
       "      <td>Power Generation Equipment</td>\n",
       "      <td>0</td>\n",
       "      <td>1026</td>\n",
       "      <td>20210406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210409</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>US10</td>\n",
       "      <td>8010099718</td>\n",
       "      <td>30165</td>\n",
       "      <td>2300006415</td>\n",
       "      <td>Custom Manufacturing</td>\n",
       "      <td>1</td>\n",
       "      <td>4016</td>\n",
       "      <td>20210409</td>\n",
       "      <td>185639199</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20210413</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PURCHASE_DOCUMENT_ITEM_ID  CREATE_DATE COMPANY_CODE_ID   VENDOR_ID  \\\n",
       "0                         20     20210330            CA10        None   \n",
       "1                         10     20210330            CN20  8010094262   \n",
       "2                         20     20210330            CN20  8010098163   \n",
       "3                         10     20210330            CN16  8010019798   \n",
       "4                         10     20210330            US10  8010099718   \n",
       "\n",
       "  POSTAL_CD  MATERIAL_ID                 SUB_COMMODITY_DESC  MRP_TYPE_ID  \\\n",
       "0      None   2100021412                            Tolling            1   \n",
       "1    201799            0  Telecommunications media services            0   \n",
       "2    200333            0               Maintenance Services            0   \n",
       "3    201604            0         Power Generation Equipment            0   \n",
       "4     30165   2300006415               Custom Manufacturing            1   \n",
       "\n",
       "   PLANT_ID  REQUESTED_DELIVERY_DATE  INBOUND_DELIVERY_ID  \\\n",
       "0      4007                 20210331            185610163   \n",
       "1      1032                 20210331                    0   \n",
       "2      1032                 20210406                    0   \n",
       "3      1026                 20210406                    0   \n",
       "4      4016                 20210409            185639199   \n",
       "\n",
       "   INBOUND_DELIVERY_ITEM_ID PLANNED_DELIVERY_DAYS  FIRST_GR_POSTING_DATE  \\\n",
       "0                        20                   0.0               20210330   \n",
       "1                         0                   0.0                      0   \n",
       "2                         0                   0.0               20210511   \n",
       "3                         0                   0.0               20210409   \n",
       "4                        10                   5.0               20210413   \n",
       "\n",
       "   TARGET_FEATURE  \n",
       "0              -1  \n",
       "1               0  \n",
       "2              35  \n",
       "3               3  \n",
       "4               4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          -1\n",
       "1           0\n",
       "2          35\n",
       "3           3\n",
       "4           4\n",
       "           ..\n",
       "1139387    -2\n",
       "1139388     0\n",
       "1139389    -6\n",
       "1139390    29\n",
       "1139391    -1\n",
       "Name: TARGET_FEATURE, Length: 1139392, dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET_FEATURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle missing values\n",
    "# data.fillna(0, inplace=True)  \n",
    "\n",
    "# Impute missing SUB_COMMODITY_DESC\n",
    "df['SUB_COMMODITY_DESC'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Convert categorical columns to one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['SUB_COMMODITY_DESC'])\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encoder = OneHotEncoder(sparse=False)  # Dense output\n",
    "# encoded_data = encoder.fit_transform(df[['SUB_COMMODITY_DESC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_delivery_days(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove leading/trailing whitespace\n",
    "        value = value.strip() \n",
    "\n",
    "        # Check for timestamp format and handle separately\n",
    "        if re.match(r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\", value):  \n",
    "            return \"\"  # Replace timestamps with NA or another placeholder\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "df['PLANNED_DELIVERY_DAYS'] = df['PLANNED_DELIVERY_DAYS'].apply(clean_delivery_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash Alphanumeric columns\n",
    "df['VENDOR_ID'] = df['VENDOR_ID'].apply(hash)\n",
    "df['POSTAL_CD'] = df['POSTAL_CD'].apply(hash)\n",
    "df['COMPANY_CODE_ID'] = df['COMPANY_CODE_ID'].apply(hash)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'str'> '0'\n",
      "5 <class 'str'> '5'\n",
      "45 <class 'str'> '45'\n",
      "14 <class 'str'> '14'\n",
      "90 <class 'str'> '90'\n",
      "1 <class 'str'> '1'\n",
      "2 <class 'str'> '2'\n",
      "10 <class 'str'> '10'\n",
      "20 <class 'str'> '20'\n",
      "100 <class 'str'> '100'\n",
      "30 <class 'str'> '30'\n",
      "3 <class 'str'> '3'\n",
      "46 <class 'str'> '46'\n",
      "6 <class 'str'> '6'\n",
      "50 <class 'str'> '50'\n",
      "4 <class 'str'> '4'\n",
      "120 <class 'str'> '120'\n",
      "60 <class 'str'> '60'\n",
      "15 <class 'str'> '15'\n",
      "31 <class 'str'> '31'\n",
      "21 <class 'str'> '21'\n",
      "7 <class 'str'> '7'\n",
      "97 <class 'str'> '97'\n",
      "19 <class 'str'> '19'\n",
      "23 <class 'str'> '23'\n",
      "13 <class 'str'> '13'\n",
      "16 <class 'str'> '16'\n",
      "17 <class 'str'> '17'\n",
      "11 <class 'str'> '11'\n",
      "59 <class 'str'> '59'\n",
      "12 <class 'str'> '12'\n",
      "34 <class 'str'> '34'\n",
      "38 <class 'str'> '38'\n",
      "25 <class 'str'> '25'\n",
      "28 <class 'str'> '28'\n",
      "9 <class 'str'> '9'\n",
      "115 <class 'str'> '115'\n",
      "87 <class 'str'> '87'\n",
      "999 <class 'str'> '999'\n",
      "18 <class 'str'> '18'\n",
      "42 <class 'str'> '42'\n",
      "24 <class 'str'> '24'\n",
      "40 <class 'str'> '40'\n",
      "65 <class 'str'> '65'\n",
      "35 <class 'str'> '35'\n",
      "8 <class 'str'> '8'\n",
      "67 <class 'str'> '67'\n",
      "29 <class 'str'> '29'\n",
      "26 <class 'str'> '26'\n",
      "27 <class 'str'> '27'\n",
      "22 <class 'str'> '22'\n",
      "52 <class 'str'> '52'\n",
      "43 <class 'str'> '43'\n",
      "140 <class 'str'> '140'\n",
      "98 <class 'str'> '98'\n",
      "55 <class 'str'> '55'\n",
      "70 <class 'str'> '70'\n",
      "180 <class 'str'> '180'\n",
      "78 <class 'str'> '78'\n",
      "37 <class 'str'> '37'\n",
      "47 <class 'str'> '47'\n",
      "48 <class 'str'> '48'\n",
      "61 <class 'str'> '61'\n",
      "117 <class 'str'> '117'\n",
      "56 <class 'str'> '56'\n",
      "41 <class 'str'> '41'\n",
      "63 <class 'str'> '63'\n",
      "119 <class 'str'> '119'\n",
      "113 <class 'str'> '113'\n",
      "88 <class 'str'> '88'\n",
      "104 <class 'str'> '104'\n",
      "39 <class 'str'> '39'\n",
      "36 <class 'str'> '36'\n",
      "84 <class 'str'> '84'\n",
      "150 <class 'str'> '150'\n",
      "138 <class 'str'> '138'\n",
      "135 <class 'str'> '135'\n",
      "80 <class 'str'> '80'\n",
      "170 <class 'str'> '170'\n",
      "141 <class 'str'> '141'\n",
      "210 <class 'str'> '210'\n",
      "105 <class 'str'> '105'\n",
      "185 <class 'str'> '185'\n",
      "75 <class 'str'> '75'\n",
      "146 <class 'str'> '146'\n",
      "85 <class 'str'> '85'\n",
      "255 <class 'str'> '255'\n",
      "99 <class 'str'> '99'\n",
      "33 <class 'str'> '33'\n",
      "64 <class 'str'> '64'\n",
      "49 <class 'str'> '49'\n",
      "211 <class 'str'> '211'\n",
      "160 <class 'str'> '160'\n",
      "74 <class 'str'> '74'\n",
      "44 <class 'str'> '44'\n",
      "32 <class 'str'> '32'\n",
      "365 <class 'str'> '365'\n",
      "57 <class 'str'> '57'\n",
      "101 <class 'str'> '101'\n",
      "69 <class 'str'> '69'\n",
      "81 <class 'str'> '81'\n",
      "73 <class 'str'> '73'\n",
      "142 <class 'str'> '142'\n",
      "139 <class 'str'> '139'\n",
      "112 <class 'str'> '112'\n",
      "256 <class 'str'> '256'\n",
      "196 <class 'str'> '196'\n",
      "125 <class 'str'> '125'\n",
      "66 <class 'str'> '66'\n",
      "71 <class 'str'> '71'\n",
      "147 <class 'str'> '147'\n",
      "154 <class 'str'> '154'\n",
      "200 <class 'str'> '200'\n",
      "62 <class 'str'> '62'\n",
      "240 <class 'str'> '240'\n",
      "77 <class 'str'> '77'\n",
      "300 <class 'str'> '300'\n",
      "54 <class 'str'> '54'\n",
      "51 <class 'str'> '51'\n",
      "109 <class 'str'> '109'\n",
      "126 <class 'str'> '126'\n",
      "86 <class 'str'> '86'\n",
      "401 <class 'str'> '401'\n",
      "195 <class 'str'> '195'\n",
      "58 <class 'str'> '58'\n",
      "53 <class 'str'> '53'\n",
      "93 <class 'str'> '93'\n",
      "123 <class 'str'> '123'\n",
      "82 <class 'str'> '82'\n",
      "91 <class 'str'> '91'\n",
      "72 <class 'str'> '72'\n",
      "83 <class 'str'> '83'\n",
      "181 <class 'str'> '181'\n",
      "122 <class 'str'> '122'\n",
      "124 <class 'str'> '124'\n",
      "79 <class 'str'> '79'\n",
      "149 <class 'str'> '149'\n",
      "130 <class 'str'> '130'\n",
      "420 <class 'str'> '420'\n",
      "161 <class 'str'> '161'\n",
      "110 <class 'str'> '110'\n",
      "184 <class 'str'> '184'\n",
      "95 <class 'str'> '95'\n",
      "68 <class 'str'> '68'\n",
      "151 <class 'str'> '151'\n",
      "280 <class 'str'> '280'\n",
      "254 <class 'str'> '254'\n",
      "94 <class 'str'> '94'\n",
      "163 <class 'str'> '163'\n",
      "92 <class 'str'> '92'\n",
      "76 <class 'str'> '76'\n",
      "168 <class 'str'> '168'\n",
      "175 <class 'str'> '175'\n",
      "118 <class 'str'> '118'\n",
      "230 <class 'str'> '230'\n",
      "107 <class 'str'> '107'\n",
      "114 <class 'str'> '114'\n",
      "102 <class 'str'> '102'\n",
      "281 <class 'str'> '281'\n",
      "108 <class 'str'> '108'\n",
      "157 <class 'str'> '157'\n",
      "221 <class 'str'> '221'\n",
      " <class 'str'> ''\n",
      "96 <class 'str'> '96'\n",
      "103 <class 'str'> '103'\n",
      "153 <class 'str'> '153'\n",
      "158 <class 'str'> '158'\n",
      "89 <class 'str'> '89'\n",
      "225 <class 'str'> '225'\n",
      "152 <class 'str'> '152'\n",
      "350 <class 'str'> '350'\n",
      "270 <class 'str'> '270'\n",
      "192 <class 'str'> '192'\n",
      "106 <class 'str'> '106'\n",
      "337 <class 'str'> '337'\n",
      "134 <class 'str'> '134'\n",
      "155 <class 'str'> '155'\n",
      "250 <class 'str'> '250'\n",
      "136 <class 'str'> '136'\n",
      "143 <class 'str'> '143'\n",
      "209 <class 'str'> '209'\n",
      "219 <class 'str'> '219'\n",
      "190 <class 'str'> '190'\n",
      "128 <class 'str'> '128'\n",
      "182 <class 'str'> '182'\n",
      "585 <class 'str'> '585'\n",
      "220 <class 'str'> '220'\n",
      "207 <class 'str'> '207'\n",
      "267 <class 'str'> '267'\n",
      "224 <class 'str'> '224'\n",
      "121 <class 'str'> '121'\n",
      "390 <class 'str'> '390'\n",
      "395 <class 'str'> '395'\n",
      "212 <class 'str'> '212'\n",
      "165 <class 'str'> '165'\n",
      "223 <class 'str'> '223'\n",
      "116 <class 'str'> '116'\n",
      "189 <class 'str'> '189'\n",
      "127 <class 'str'> '127'\n",
      "238 <class 'str'> '238'\n",
      "177 <class 'str'> '177'\n",
      "144 <class 'str'> '144'\n",
      "111 <class 'str'> '111'\n",
      "252 <class 'str'> '252'\n",
      "173 <class 'str'> '173'\n",
      "178 <class 'str'> '178'\n",
      "137 <class 'str'> '137'\n",
      "169 <class 'str'> '169'\n",
      "253 <class 'str'> '253'\n",
      "145 <class 'str'> '145'\n",
      "166 <class 'str'> '166'\n",
      "203 <class 'str'> '203'\n",
      "217 <class 'str'> '217'\n",
      "317 <class 'str'> '317'\n",
      "260 <class 'str'> '260'\n",
      "167 <class 'str'> '167'\n",
      "133 <class 'str'> '133'\n",
      "235 <class 'str'> '235'\n",
      "340 <class 'str'> '340'\n",
      "198 <class 'str'> '198'\n",
      "380 <class 'str'> '380'\n",
      "129 <class 'str'> '129'\n",
      "265 <class 'str'> '265'\n",
      "245 <class 'str'> '245'\n",
      "305 <class 'str'> '305'\n",
      "355 <class 'str'> '355'\n",
      "228 <class 'str'> '228'\n",
      "164 <class 'str'> '164'\n",
      "171 <class 'str'> '171'\n",
      "435 <class 'str'> '435'\n",
      "289 <class 'str'> '289'\n",
      "188 <class 'str'> '188'\n",
      "172 <class 'str'> '172'\n",
      "295 <class 'str'> '295'\n",
      "820 <class 'str'> '820'\n",
      "234 <class 'str'> '234'\n",
      "205 <class 'str'> '205'\n",
      "297 <class 'str'> '297'\n",
      "148 <class 'str'> '148'\n",
      "294 <class 'str'> '294'\n",
      "186 <class 'str'> '186'\n",
      "162 <class 'str'> '162'\n",
      "208 <class 'str'> '208'\n",
      "462 <class 'str'> '462'\n",
      "392 <class 'str'> '392'\n",
      "308 <class 'str'> '308'\n",
      "500 <class 'str'> '500'\n",
      "204 <class 'str'> '204'\n",
      "197 <class 'str'> '197'\n",
      "131 <class 'str'> '131'\n",
      "156 <class 'str'> '156'\n",
      "183 <class 'str'> '183'\n",
      "304 <class 'str'> '304'\n",
      "259 <class 'str'> '259'\n",
      "132 <class 'str'> '132'\n",
      "206 <class 'str'> '206'\n",
      "216 <class 'str'> '216'\n",
      "310 <class 'str'> '310'\n",
      "320 <class 'str'> '320'\n",
      "179 <class 'str'> '179'\n"
     ]
    }
   ],
   "source": [
    "def remove_decimal(value):\n",
    "    return value.split('.')[0]  # Split by the decimal and keep the integer part\n",
    "\n",
    "df['PLANNED_DELIVERY_DAYS'] = df['PLANNED_DELIVERY_DAYS'].apply(remove_decimal)\n",
    "\n",
    "for value in df['PLANNED_DELIVERY_DAYS'].unique():\n",
    "    print(value, type(value), repr(value)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(col):\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except ValueError:\n",
    "        print(f\"Error converting column '{col}': Contains non-numeric values\")\n",
    "\n",
    "convert_to_numeric('PURCHASE_DOCUMENT_ITEM_ID')\n",
    "convert_to_numeric('CREATE_DATE')\n",
    "convert_to_numeric('VENDOR_ID')\n",
    "convert_to_numeric('POSTAL_CD')\n",
    "convert_to_numeric('MATERIAL_ID')\n",
    "convert_to_numeric('MRP_TYPE_ID')\n",
    "convert_to_numeric('PLANT_ID')\n",
    "convert_to_numeric('INBOUND_DELIVERY_ID')\n",
    "convert_to_numeric('INBOUND_DELIVERY_ITEM_ID')\n",
    "convert_to_numeric('PLANNED_DELIVERY_DAYS')\n",
    "convert_to_numeric('FIRST_GR_POSTING_DATE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_DOCUMENT_ITEM_ID</th>\n",
       "      <th>CREATE_DATE</th>\n",
       "      <th>COMPANY_CODE_ID</th>\n",
       "      <th>VENDOR_ID</th>\n",
       "      <th>POSTAL_CD</th>\n",
       "      <th>MATERIAL_ID</th>\n",
       "      <th>MRP_TYPE_ID</th>\n",
       "      <th>PLANT_ID</th>\n",
       "      <th>REQUESTED_DELIVERY_DATE</th>\n",
       "      <th>INBOUND_DELIVERY_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>SUB_COMMODITY_DESC_Tools</th>\n",
       "      <th>SUB_COMMODITY_DESC_Transport operations services</th>\n",
       "      <th>SUB_COMMODITY_DESC_Transportation, Storage, Mail Services</th>\n",
       "      <th>SUB_COMMODITY_DESC_Travel Services</th>\n",
       "      <th>SUB_COMMODITY_DESC_Tubes &amp; Cores</th>\n",
       "      <th>SUB_COMMODITY_DESC_Unknown</th>\n",
       "      <th>SUB_COMMODITY_DESC_Valves</th>\n",
       "      <th>SUB_COMMODITY_DESC_Vehicles</th>\n",
       "      <th>SUB_COMMODITY_DESC_Waste Disposal Services</th>\n",
       "      <th>SUB_COMMODITY_DESC_Water Treatment Chemicals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20210330</td>\n",
       "      <td>-9134099118741362072</td>\n",
       "      <td>8794021623985</td>\n",
       "      <td>8794021623985</td>\n",
       "      <td>2100021412</td>\n",
       "      <td>1</td>\n",
       "      <td>4007</td>\n",
       "      <td>20210331</td>\n",
       "      <td>185610163</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>-5651926800411073115</td>\n",
       "      <td>-6786898889203162180</td>\n",
       "      <td>-6490539584081541328</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20210331</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20210330</td>\n",
       "      <td>-5651926800411073115</td>\n",
       "      <td>6351911316643222019</td>\n",
       "      <td>3899556115142291491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20210406</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>5345649740314450313</td>\n",
       "      <td>-5892833219993490273</td>\n",
       "      <td>-6223389847109689459</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1026</td>\n",
       "      <td>20210406</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>-9066240345675022972</td>\n",
       "      <td>2189267853882615626</td>\n",
       "      <td>4393863124668179275</td>\n",
       "      <td>2300006415</td>\n",
       "      <td>1</td>\n",
       "      <td>4016</td>\n",
       "      <td>20210409</td>\n",
       "      <td>185639199</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PURCHASE_DOCUMENT_ITEM_ID  CREATE_DATE      COMPANY_CODE_ID  \\\n",
       "0                         20     20210330 -9134099118741362072   \n",
       "1                         10     20210330 -5651926800411073115   \n",
       "2                         20     20210330 -5651926800411073115   \n",
       "3                         10     20210330  5345649740314450313   \n",
       "4                         10     20210330 -9066240345675022972   \n",
       "\n",
       "             VENDOR_ID            POSTAL_CD  MATERIAL_ID  MRP_TYPE_ID  \\\n",
       "0        8794021623985        8794021623985   2100021412            1   \n",
       "1 -6786898889203162180 -6490539584081541328            0            0   \n",
       "2  6351911316643222019  3899556115142291491            0            0   \n",
       "3 -5892833219993490273 -6223389847109689459            0            0   \n",
       "4  2189267853882615626  4393863124668179275   2300006415            1   \n",
       "\n",
       "   PLANT_ID  REQUESTED_DELIVERY_DATE  INBOUND_DELIVERY_ID  ...  \\\n",
       "0      4007                 20210331            185610163  ...   \n",
       "1      1032                 20210331                    0  ...   \n",
       "2      1032                 20210406                    0  ...   \n",
       "3      1026                 20210406                    0  ...   \n",
       "4      4016                 20210409            185639199  ...   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Tools  SUB_COMMODITY_DESC_Transport operations services  \\\n",
       "0                         0                                                 0   \n",
       "1                         0                                                 0   \n",
       "2                         0                                                 0   \n",
       "3                         0                                                 0   \n",
       "4                         0                                                 0   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Transportation, Storage, Mail Services  \\\n",
       "0                                                  0           \n",
       "1                                                  0           \n",
       "2                                                  0           \n",
       "3                                                  0           \n",
       "4                                                  0           \n",
       "\n",
       "   SUB_COMMODITY_DESC_Travel Services  SUB_COMMODITY_DESC_Tubes & Cores  \\\n",
       "0                                   0                                 0   \n",
       "1                                   0                                 0   \n",
       "2                                   0                                 0   \n",
       "3                                   0                                 0   \n",
       "4                                   0                                 0   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Unknown  SUB_COMMODITY_DESC_Valves  \\\n",
       "0                           0                          0   \n",
       "1                           0                          0   \n",
       "2                           0                          0   \n",
       "3                           0                          0   \n",
       "4                           0                          0   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Vehicles  SUB_COMMODITY_DESC_Waste Disposal Services  \\\n",
       "0                            0                                           0   \n",
       "1                            0                                           0   \n",
       "2                            0                                           0   \n",
       "3                            0                                           0   \n",
       "4                            0                                           0   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Water Treatment Chemicals  \n",
       "0                                             0  \n",
       "1                                             0  \n",
       "2                                             0  \n",
       "3                                             0  \n",
       "4                                             0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1139392 entries, 0 to 1139391\n",
      "Data columns (total 87 columns):\n",
      " #   Column                                                     Non-Null Count    Dtype  \n",
      "---  ------                                                     --------------    -----  \n",
      " 0   PURCHASE_DOCUMENT_ITEM_ID                                  1139392 non-null  int16  \n",
      " 1   CREATE_DATE                                                1139392 non-null  int32  \n",
      " 2   COMPANY_CODE_ID                                            1139392 non-null  int64  \n",
      " 3   VENDOR_ID                                                  1139392 non-null  int64  \n",
      " 4   POSTAL_CD                                                  1139392 non-null  int64  \n",
      " 5   MATERIAL_ID                                                1139392 non-null  int64  \n",
      " 6   MRP_TYPE_ID                                                1139392 non-null  int8   \n",
      " 7   PLANT_ID                                                   1139392 non-null  int16  \n",
      " 8   REQUESTED_DELIVERY_DATE                                    1139392 non-null  int32  \n",
      " 9   INBOUND_DELIVERY_ID                                        1139392 non-null  int32  \n",
      " 10  INBOUND_DELIVERY_ITEM_ID                                   1139392 non-null  int32  \n",
      " 11  PLANNED_DELIVERY_DAYS                                      1139388 non-null  float64\n",
      " 12  FIRST_GR_POSTING_DATE                                      1139392 non-null  int32  \n",
      " 13  TARGET_FEATURE                                             1139392 non-null  int32  \n",
      " 14  SUB_COMMODITY_DESC_Additives, Colorants & Catalysts        1139392 non-null  uint8  \n",
      " 15  SUB_COMMODITY_DESC_Admin Services                          1139392 non-null  uint8  \n",
      " 16  SUB_COMMODITY_DESC_Blowers & Dryers                        1139392 non-null  uint8  \n",
      " 17  SUB_COMMODITY_DESC_Building Materials                      1139392 non-null  uint8  \n",
      " 18  SUB_COMMODITY_DESC_Building, Construction Services         1139392 non-null  uint8  \n",
      " 19  SUB_COMMODITY_DESC_CONTAINMENTS                            1139392 non-null  uint8  \n",
      " 20  SUB_COMMODITY_DESC_CORRUGATED                              1139392 non-null  uint8  \n",
      " 21  SUB_COMMODITY_DESC_Communications Devices Accessories      1139392 non-null  uint8  \n",
      " 22  SUB_COMMODITY_DESC_Computer Equipment Accessories          1139392 non-null  uint8  \n",
      " 23  SUB_COMMODITY_DESC_Computer services                       1139392 non-null  uint8  \n",
      " 24  SUB_COMMODITY_DESC_Consulting Services                     1139392 non-null  uint8  \n",
      " 25  SUB_COMMODITY_DESC_Corporate Services                      1139392 non-null  uint8  \n",
      " 26  SUB_COMMODITY_DESC_Custom Manufacturing                    1139392 non-null  uint8  \n",
      " 27  SUB_COMMODITY_DESC_Cylinder Gases                          1139392 non-null  uint8  \n",
      " 28  SUB_COMMODITY_DESC_Electric & Electronics                  1139392 non-null  uint8  \n",
      " 29  SUB_COMMODITY_DESC_Elements & Industrial Gases             1139392 non-null  uint8  \n",
      " 30  SUB_COMMODITY_DESC_Engineering Services                    1139392 non-null  uint8  \n",
      " 31  SUB_COMMODITY_DESC_Environmental Services                  1139392 non-null  uint8  \n",
      " 32  SUB_COMMODITY_DESC_Feedstock's                             1139392 non-null  uint8  \n",
      " 33  SUB_COMMODITY_DESC_Film, Strapping, Wrapping               1139392 non-null  uint8  \n",
      " 34  SUB_COMMODITY_DESC_Filtration                              1139392 non-null  uint8  \n",
      " 35  SUB_COMMODITY_DESC_Fuels                                   1139392 non-null  uint8  \n",
      " 36  SUB_COMMODITY_DESC_HR Services                             1139392 non-null  uint8  \n",
      " 37  SUB_COMMODITY_DESC_Health Services                         1139392 non-null  uint8  \n",
      " 38  SUB_COMMODITY_DESC_Industrial Cleaning Services            1139392 non-null  uint8  \n",
      " 39  SUB_COMMODITY_DESC_Infrastructure Services                 1139392 non-null  uint8  \n",
      " 40  SUB_COMMODITY_DESC_Instrumentation & Process Control       1139392 non-null  uint8  \n",
      " 41  SUB_COMMODITY_DESC_Lab Supplies                            1139392 non-null  uint8  \n",
      " 42  SUB_COMMODITY_DESC_Lubricants                              1139392 non-null  uint8  \n",
      " 43  SUB_COMMODITY_DESC_Machinery & Equipment                   1139392 non-null  uint8  \n",
      " 44  SUB_COMMODITY_DESC_Maintenance Services                    1139392 non-null  uint8  \n",
      " 45  SUB_COMMODITY_DESC_Manufacturing Components and Supplies   1139392 non-null  uint8  \n",
      " 46  SUB_COMMODITY_DESC_Manufacturing Services                  1139392 non-null  uint8  \n",
      " 47  SUB_COMMODITY_DESC_Marketing Communications                1139392 non-null  uint8  \n",
      " 48  SUB_COMMODITY_DESC_Marketing Consulting Services           1139392 non-null  uint8  \n",
      " 49  SUB_COMMODITY_DESC_Material Handling                       1139392 non-null  uint8  \n",
      " 50  SUB_COMMODITY_DESC_Mill Supplies                           1139392 non-null  uint8  \n",
      " 51  SUB_COMMODITY_DESC_Motors                                  1139392 non-null  uint8  \n",
      " 52  SUB_COMMODITY_DESC_Office Supplies                         1139392 non-null  uint8  \n",
      " 53  SUB_COMMODITY_DESC_Pads, Trays & Locators                  1139392 non-null  uint8  \n",
      " 54  SUB_COMMODITY_DESC_Pallets                                 1139392 non-null  uint8  \n",
      " 55  SUB_COMMODITY_DESC_Piping & Tubing                         1139392 non-null  uint8  \n",
      " 56  SUB_COMMODITY_DESC_Power & Utilities Services              1139392 non-null  uint8  \n",
      " 57  SUB_COMMODITY_DESC_Power Generation & Distribution         1139392 non-null  uint8  \n",
      " 58  SUB_COMMODITY_DESC_Power Generation Equipment              1139392 non-null  uint8  \n",
      " 59  SUB_COMMODITY_DESC_Power Transmission                      1139392 non-null  uint8  \n",
      " 60  SUB_COMMODITY_DESC_Precious Metals, Alloys and oxides      1139392 non-null  uint8  \n",
      " 61  SUB_COMMODITY_DESC_Process Control Equipment               1139392 non-null  uint8  \n",
      " 62  SUB_COMMODITY_DESC_Promotion & Advertising                 1139392 non-null  uint8  \n",
      " 63  SUB_COMMODITY_DESC_Public Power & Utility Services         1139392 non-null  uint8  \n",
      " 64  SUB_COMMODITY_DESC_Pumps & Compressors                     1139392 non-null  uint8  \n",
      " 65  SUB_COMMODITY_DESC_Real Estate Services                    1139392 non-null  uint8  \n",
      " 66  SUB_COMMODITY_DESC_Rental & Lease services                 1139392 non-null  uint8  \n",
      " 67  SUB_COMMODITY_DESC_Resins & Polymers                       1139392 non-null  uint8  \n",
      " 68  SUB_COMMODITY_DESC_Safety Supplies                         1139392 non-null  uint8  \n",
      " 69  SUB_COMMODITY_DESC_Security Services                       1139392 non-null  uint8  \n",
      " 70  SUB_COMMODITY_DESC_Software                                1139392 non-null  uint8  \n",
      " 71  SUB_COMMODITY_DESC_Spinning                                1139392 non-null  uint8  \n",
      " 72  SUB_COMMODITY_DESC_Storage & Handling Equipment            1139392 non-null  uint8  \n",
      " 73  SUB_COMMODITY_DESC_Tanks and Process Equipment             1139392 non-null  uint8  \n",
      " 74  SUB_COMMODITY_DESC_Telecommunications media services       1139392 non-null  uint8  \n",
      " 75  SUB_COMMODITY_DESC_Textile & Spinning Machinery            1139392 non-null  uint8  \n",
      " 76  SUB_COMMODITY_DESC_Tolling                                 1139392 non-null  uint8  \n",
      " 77  SUB_COMMODITY_DESC_Tools                                   1139392 non-null  uint8  \n",
      " 78  SUB_COMMODITY_DESC_Transport operations services           1139392 non-null  uint8  \n",
      " 79  SUB_COMMODITY_DESC_Transportation, Storage, Mail Services  1139392 non-null  uint8  \n",
      " 80  SUB_COMMODITY_DESC_Travel Services                         1139392 non-null  uint8  \n",
      " 81  SUB_COMMODITY_DESC_Tubes & Cores                           1139392 non-null  uint8  \n",
      " 82  SUB_COMMODITY_DESC_Unknown                                 1139392 non-null  uint8  \n",
      " 83  SUB_COMMODITY_DESC_Valves                                  1139392 non-null  uint8  \n",
      " 84  SUB_COMMODITY_DESC_Vehicles                                1139392 non-null  uint8  \n",
      " 85  SUB_COMMODITY_DESC_Waste Disposal Services                 1139392 non-null  uint8  \n",
      " 86  SUB_COMMODITY_DESC_Water Treatment Chemicals               1139392 non-null  uint8  \n",
      "dtypes: float64(1), int16(2), int32(6), int64(4), int8(1), uint8(73)\n",
      "memory usage: 154.3 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show types of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "df = df[(df['TARGET_FEATURE'] < 100) & (df['TARGET_FEATURE'] > -100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0000e+01,  2.0210e+07, -9.1341e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0000e+01,  2.0210e+07, -5.6519e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e+01,  2.0210e+07, -5.6519e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 1.0000e+01,  2.0201e+07,  7.4181e+17,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0000e+01,  2.0201e+07,  1.3417e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.0000e+01,  2.0201e+07, -9.1341e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create feature & target tensors on GPU\n",
    "features = df.drop('TARGET_FEATURE', axis=1)\n",
    "targets = df['TARGET_FEATURE']\n",
    "X = torch.tensor(features.values.astype(np.float32))\n",
    "y = torch.tensor(targets.values.astype(np.float32))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Now X_normalized contains the normalized data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0., 35.,  ..., -6., 29., -1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Add training and test data to the device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RegressionNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size1 = 128  \n",
    "        self.hidden_size2 = 64  \n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, self.hidden_size1)\n",
    "        self.linear2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "\n",
    "        # Output layer for regression (no activation)\n",
    "        self.output_layer = nn.Linear(self.hidden_size2, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))  \n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "input_size = X.shape[1]  # Number of features\n",
    "\n",
    "\n",
    "model = RegressionNetwork(input_size) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Mean Absolute Error (MAE) loss\n",
    "loss_fn = nn.L1Loss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1Loss()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data Preparation (If not using a DataLoader)\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# port to GPU\n",
    "model.to(device)\n",
    "loss_fn.to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/13958], Loss: 3880996873175040.0000\n",
      "Epoch [1/100], Step [200/13958], Loss: 2591036194947072.0000\n",
      "Epoch [1/100], Step [300/13958], Loss: 4565754246594560.0000\n",
      "Epoch [1/100], Step [400/13958], Loss: 1880706720464896.0000\n",
      "Epoch [1/100], Step [500/13958], Loss: 1831160917262336.0000\n",
      "Epoch [1/100], Step [600/13958], Loss: 1652394714726400.0000\n",
      "Epoch [1/100], Step [700/13958], Loss: 1810927057895424.0000\n",
      "Epoch [1/100], Step [800/13958], Loss: 1157363763511296.0000\n",
      "Epoch [1/100], Step [900/13958], Loss: 2458042532626432.0000\n",
      "Epoch [1/100], Step [1000/13958], Loss: 2640325776506880.0000\n",
      "Epoch [1/100], Step [1100/13958], Loss: 2031414304309248.0000\n",
      "Epoch [1/100], Step [1200/13958], Loss: 641778038341632.0000\n",
      "Epoch [1/100], Step [1300/13958], Loss: 584688057974784.0000\n",
      "Epoch [1/100], Step [1400/13958], Loss: 1328862747164672.0000\n",
      "Epoch [1/100], Step [1500/13958], Loss: 1546635976900608.0000\n",
      "Epoch [1/100], Step [1600/13958], Loss: 549643205214208.0000\n",
      "Epoch [1/100], Step [1700/13958], Loss: 1355575833133056.0000\n",
      "Epoch [1/100], Step [1800/13958], Loss: 839729255809024.0000\n",
      "Epoch [1/100], Step [1900/13958], Loss: 411290296123392.0000\n",
      "Epoch [1/100], Step [2000/13958], Loss: 516599337451520.0000\n",
      "Epoch [1/100], Step [2100/13958], Loss: 636319537561600.0000\n",
      "Epoch [1/100], Step [2200/13958], Loss: 1046839960797184.0000\n",
      "Epoch [1/100], Step [2300/13958], Loss: 395844587094016.0000\n",
      "Epoch [1/100], Step [2400/13958], Loss: 273478318030848.0000\n",
      "Epoch [1/100], Step [2500/13958], Loss: 520846456127488.0000\n",
      "Epoch [1/100], Step [2600/13958], Loss: 158413627588608.0000\n",
      "Epoch [1/100], Step [2700/13958], Loss: 382684706635776.0000\n",
      "Epoch [1/100], Step [2800/13958], Loss: 279446024093696.0000\n",
      "Epoch [1/100], Step [2900/13958], Loss: 380586246012928.0000\n",
      "Epoch [1/100], Step [3000/13958], Loss: 377295864856576.0000\n",
      "Epoch [1/100], Step [3100/13958], Loss: 508739815735296.0000\n",
      "Epoch [1/100], Step [3200/13958], Loss: 226627204153344.0000\n",
      "Epoch [1/100], Step [3300/13958], Loss: 431865974489088.0000\n",
      "Epoch [1/100], Step [3400/13958], Loss: 157619561955328.0000\n",
      "Epoch [1/100], Step [3500/13958], Loss: 155402620633088.0000\n",
      "Epoch [1/100], Step [3600/13958], Loss: 158289375526912.0000\n",
      "Epoch [1/100], Step [3700/13958], Loss: 209811534774272.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [2/100], Step [100/13958], Loss: 111559326040064.0000\n",
      "Epoch [2/100], Step [200/13958], Loss: 197007566176256.0000\n",
      "Epoch [2/100], Step [300/13958], Loss: 54422377857024.0000\n",
      "Epoch [2/100], Step [400/13958], Loss: 109455194718208.0000\n",
      "Epoch [2/100], Step [500/13958], Loss: 112245858107392.0000\n",
      "Epoch [2/100], Step [600/13958], Loss: 56472255856640.0000\n",
      "Epoch [2/100], Step [700/13958], Loss: 69094753697792.0000\n",
      "Epoch [2/100], Step [800/13958], Loss: 46153500459008.0000\n",
      "Epoch [2/100], Step [900/13958], Loss: 36975868379136.0000\n",
      "Epoch [2/100], Step [1000/13958], Loss: 76953063260160.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [3/100], Step [100/13958], Loss: 23760505667584.0000\n",
      "Epoch [3/100], Step [200/13958], Loss: 30162515132416.0000\n",
      "Epoch [3/100], Step [300/13958], Loss: 11328756383744.0000\n",
      "Epoch [3/100], Step [400/13958], Loss: 20214609608704.0000\n",
      "Epoch [3/100], Step [500/13958], Loss: 19946243358720.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [4/100], Step [100/13958], Loss: 3866728333312.0000\n",
      "Epoch [4/100], Step [200/13958], Loss: 2423282204672.0000\n",
      "Epoch [4/100], Step [300/13958], Loss: 2447454240768.0000\n",
      "Epoch [4/100], Step [400/13958], Loss: 1941375811584.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [5/100], Step [100/13958], Loss: 2976872136704.0000\n",
      "Epoch [5/100], Step [200/13958], Loss: 1950890721280.0000\n",
      "Epoch [5/100], Step [300/13958], Loss: 1871101165568.0000\n",
      "Epoch [5/100], Step [400/13958], Loss: 1950069817344.0000\n",
      "Epoch [5/100], Step [500/13958], Loss: 1453678657536.0000\n",
      "Epoch [5/100], Step [600/13958], Loss: 1536474218496.0000\n",
      "Epoch [5/100], Step [700/13958], Loss: 1503321915392.0000\n",
      "Epoch [5/100], Step [800/13958], Loss: 70982344704.0000\n",
      "Epoch [5/100], Step [900/13958], Loss: 133937758208.0000\n",
      "Epoch [5/100], Step [1000/13958], Loss: 64699465728.0000\n",
      "Epoch [5/100], Step [1100/13958], Loss: 132174774272.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [6/100], Step [100/13958], Loss: 7220990976.0000\n",
      "Epoch [6/100], Step [200/13958], Loss: 9878818816.0000\n",
      "Epoch [6/100], Step [300/13958], Loss: 25198399488.0000\n",
      "Epoch [6/100], Step [400/13958], Loss: 103316471808.0000\n",
      "Epoch [6/100], Step [500/13958], Loss: 17771800576.0000\n",
      "Epoch [6/100], Step [600/13958], Loss: 17070547968.0000\n",
      "Epoch [6/100], Step [700/13958], Loss: 34773737472.0000\n",
      "Epoch [6/100], Step [800/13958], Loss: 47676203008.0000\n",
      "Epoch [6/100], Step [900/13958], Loss: 10664835072.0000\n",
      "Epoch [6/100], Step [1000/13958], Loss: 8904529920.0000\n",
      "Epoch [6/100], Step [1100/13958], Loss: 11433824256.0000\n",
      "Epoch [6/100], Step [1200/13958], Loss: 6143540736.0000\n",
      "Epoch [6/100], Step [1300/13958], Loss: 13448229888.0000\n",
      "Epoch [6/100], Step [1400/13958], Loss: 1456959232.0000\n",
      "Epoch [6/100], Step [1500/13958], Loss: 16911429632.0000\n",
      "Epoch [6/100], Step [1600/13958], Loss: 21439975424.0000\n",
      "Epoch [6/100], Step [1700/13958], Loss: 553770432.0000\n",
      "Epoch [6/100], Step [1800/13958], Loss: 9576894464.0000\n",
      "Epoch [6/100], Step [1900/13958], Loss: 10863200256.0000\n",
      "Epoch [6/100], Step [2000/13958], Loss: 3027953408.0000\n",
      "Epoch [6/100], Step [2100/13958], Loss: 632257792.0000\n",
      "Epoch [6/100], Step [2200/13958], Loss: 411871680.0000\n",
      "Epoch [6/100], Step [2300/13958], Loss: 1067360128.0000\n",
      "Epoch [6/100], Step [2400/13958], Loss: 193659136.0000\n",
      "Epoch [6/100], Step [2500/13958], Loss: 142066080.0000\n",
      "Epoch [6/100], Step [2600/13958], Loss: 4607601.0000\n",
      "Epoch [6/100], Step [2700/13958], Loss: 1172973568.0000\n",
      "Epoch [6/100], Step [2800/13958], Loss: 678953728.0000\n",
      "Epoch [6/100], Step [2900/13958], Loss: 6.7188\n",
      "Epoch [6/100], Step [3000/13958], Loss: 7.2031\n",
      "Epoch [6/100], Step [3100/13958], Loss: 1253680000.0000\n",
      "Epoch [6/100], Step [3200/13958], Loss: 196772224.0000\n",
      "Epoch [6/100], Step [3300/13958], Loss: 10681401344.0000\n",
      "Epoch [6/100], Step [3400/13958], Loss: 7.6094\n",
      "Epoch [6/100], Step [3500/13958], Loss: 1027566464.0000\n",
      "Epoch [6/100], Step [3600/13958], Loss: 74626776.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [7/100], Step [100/13958], Loss: 6343.3228\n",
      "Epoch [7/100], Step [200/13958], Loss: 5.2346\n",
      "Epoch [7/100], Step [300/13958], Loss: 8.0469\n",
      "Epoch [7/100], Step [400/13958], Loss: 7.0313\n",
      "Epoch [7/100], Step [500/13958], Loss: 28235186.0000\n",
      "Epoch [7/100], Step [600/13958], Loss: 5.9844\n",
      "Epoch [7/100], Step [700/13958], Loss: 4.9219\n",
      "Epoch [7/100], Step [800/13958], Loss: 8.3126\n",
      "Epoch [7/100], Step [900/13958], Loss: 7.2032\n",
      "Epoch [7/100], Step [1000/13958], Loss: 58595868.0000\n",
      "Epoch [7/100], Step [1100/13958], Loss: 3614945.7500\n",
      "Epoch [7/100], Step [1200/13958], Loss: 5.9844\n",
      "Epoch [7/100], Step [1300/13958], Loss: 12.1094\n",
      "Epoch [7/100], Step [1400/13958], Loss: 10.2031\n",
      "Epoch [7/100], Step [1500/13958], Loss: 4.1564\n",
      "Epoch [7/100], Step [1600/13958], Loss: 529002720.0000\n",
      "Epoch [7/100], Step [1700/13958], Loss: 5.9219\n",
      "Epoch [7/100], Step [1800/13958], Loss: 8.4063\n",
      "Epoch [7/100], Step [1900/13958], Loss: 6.6250\n",
      "Epoch [7/100], Step [2000/13958], Loss: 5.5625\n",
      "Epoch [7/100], Step [2100/13958], Loss: 1989511552.0000\n",
      "Epoch [7/100], Step [2200/13958], Loss: 8.3438\n",
      "Epoch [7/100], Step [2300/13958], Loss: 7.9219\n",
      "Epoch [7/100], Step [2400/13958], Loss: 5.2344\n",
      "Epoch [7/100], Step [2500/13958], Loss: 8.9375\n",
      "Epoch [7/100], Step [2600/13958], Loss: 8.4063\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [8/100], Step [100/13958], Loss: 7.9532\n",
      "Epoch [8/100], Step [200/13958], Loss: 6.8751\n",
      "Epoch [8/100], Step [300/13958], Loss: 4.5000\n",
      "Epoch [8/100], Step [400/13958], Loss: 10.0156\n",
      "Epoch [8/100], Step [500/13958], Loss: 9.6250\n",
      "Epoch [8/100], Step [600/13958], Loss: 6.1719\n",
      "Epoch [8/100], Step [700/13958], Loss: 4.0782\n",
      "Epoch [8/100], Step [800/13958], Loss: 11.1250\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [9/100], Step [100/13958], Loss: 8.6407\n",
      "Epoch [9/100], Step [200/13958], Loss: 8.9845\n",
      "Epoch [9/100], Step [300/13958], Loss: 4.9375\n",
      "Epoch [9/100], Step [400/13958], Loss: 9.0625\n",
      "Epoch [9/100], Step [500/13958], Loss: 7.2500\n",
      "Epoch [9/100], Step [600/13958], Loss: 8.2500\n",
      "Epoch [9/100], Step [700/13958], Loss: 8.5157\n",
      "Epoch [9/100], Step [800/13958], Loss: 7.6875\n",
      "Epoch [9/100], Step [900/13958], Loss: 9.3282\n",
      "Epoch [9/100], Step [1000/13958], Loss: 8.4219\n",
      "Epoch [9/100], Step [1100/13958], Loss: 5.9688\n",
      "Epoch [9/100], Step [1200/13958], Loss: 8.9688\n",
      "Epoch [9/100], Step [1300/13958], Loss: 9.3438\n",
      "Epoch [9/100], Step [1400/13958], Loss: 8.2188\n",
      "Epoch [9/100], Step [1500/13958], Loss: 8.7969\n",
      "Epoch [9/100], Step [1600/13958], Loss: 7.8282\n",
      "Epoch [9/100], Step [1700/13958], Loss: 6.5781\n",
      "Epoch [9/100], Step [1800/13958], Loss: 7.2031\n",
      "Epoch [9/100], Step [1900/13958], Loss: 10.0157\n",
      "Epoch [9/100], Step [2000/13958], Loss: 4.4845\n",
      "Epoch [9/100], Step [2100/13958], Loss: 10.3438\n",
      "Epoch [9/100], Step [2200/13958], Loss: 7.7344\n",
      "Epoch [9/100], Step [2300/13958], Loss: 8.3752\n",
      "Epoch [9/100], Step [2400/13958], Loss: 8.9064\n",
      "Epoch [9/100], Step [2500/13958], Loss: 10.0626\n",
      "Epoch [9/100], Step [2600/13958], Loss: 6.2813\n",
      "Epoch [9/100], Step [2700/13958], Loss: 7.7188\n",
      "Epoch [9/100], Step [2800/13958], Loss: 5.7188\n",
      "Epoch [9/100], Step [2900/13958], Loss: 8.7658\n",
      "Epoch [9/100], Step [3000/13958], Loss: 6.9219\n",
      "Epoch [9/100], Step [3100/13958], Loss: 7.8281\n",
      "Epoch [9/100], Step [3200/13958], Loss: 8.5000\n",
      "Epoch [9/100], Step [3300/13958], Loss: 6.0938\n",
      "Epoch [9/100], Step [3400/13958], Loss: 10.7500\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [10/100], Step [100/13958], Loss: 57479572.0000\n",
      "Epoch [10/100], Step [200/13958], Loss: 7.2813\n",
      "Epoch [10/100], Step [300/13958], Loss: 6.6563\n",
      "Epoch [10/100], Step [400/13958], Loss: 9.6250\n",
      "Epoch [10/100], Step [500/13958], Loss: 6.5157\n",
      "Epoch [10/100], Step [600/13958], Loss: 11.2344\n",
      "Epoch [10/100], Step [700/13958], Loss: 9.6563\n",
      "Epoch [10/100], Step [800/13958], Loss: 5.8126\n",
      "Epoch [10/100], Step [900/13958], Loss: 10.1407\n",
      "Epoch [10/100], Step [1000/13958], Loss: 7.7032\n",
      "Epoch [10/100], Step [1100/13958], Loss: 9.4219\n",
      "Epoch [10/100], Step [1200/13958], Loss: 7.2345\n",
      "Epoch [10/100], Step [1300/13958], Loss: 7.8595\n",
      "Epoch [10/100], Step [1400/13958], Loss: 7.7500\n",
      "Epoch [10/100], Step [1500/13958], Loss: 8.1876\n",
      "Epoch [10/100], Step [1600/13958], Loss: 10.4064\n",
      "Epoch [10/100], Step [1700/13958], Loss: 7.1250\n",
      "Epoch [10/100], Step [1800/13958], Loss: 8.6563\n",
      "Epoch [10/100], Step [1900/13958], Loss: 9.5313\n",
      "Epoch [10/100], Step [2000/13958], Loss: 7.1251\n",
      "Epoch [10/100], Step [2100/13958], Loss: 8.0000\n",
      "Epoch [10/100], Step [2200/13958], Loss: 10.4219\n",
      "Epoch [10/100], Step [2300/13958], Loss: 7.4844\n",
      "Epoch [10/100], Step [2400/13958], Loss: 6.0000\n",
      "Epoch [10/100], Step [2500/13958], Loss: 7.8282\n",
      "Epoch [10/100], Step [2600/13958], Loss: 6.0157\n",
      "Epoch [10/100], Step [2700/13958], Loss: 8.7971\n",
      "Epoch [10/100], Step [2800/13958], Loss: 6.2813\n",
      "Epoch [10/100], Step [2900/13958], Loss: 8.8125\n",
      "Epoch [10/100], Step [3000/13958], Loss: 10.4844\n",
      "Epoch [10/100], Step [3100/13958], Loss: 10.1094\n",
      "Epoch [10/100], Step [3200/13958], Loss: 9.8125\n",
      "Epoch [10/100], Step [3300/13958], Loss: 7.5000\n",
      "Epoch [10/100], Step [3400/13958], Loss: 6.9689\n",
      "Epoch [10/100], Step [3500/13958], Loss: 10.7188\n",
      "Epoch [10/100], Step [3600/13958], Loss: 7.5938\n",
      "Epoch [10/100], Step [3700/13958], Loss: 6.6875\n",
      "Epoch [10/100], Step [3800/13958], Loss: 10.5002\n",
      "Epoch [10/100], Step [3900/13958], Loss: 8.2188\n",
      "Epoch [10/100], Step [4000/13958], Loss: 7.5157\n",
      "Epoch [10/100], Step [4100/13958], Loss: 4.4536\n",
      "Epoch [10/100], Step [4200/13958], Loss: 6.5000\n",
      "Epoch [10/100], Step [4300/13958], Loss: 6.0313\n",
      "Epoch [10/100], Step [4400/13958], Loss: 8.5156\n",
      "Epoch [10/100], Step [4500/13958], Loss: 9.7501\n",
      "Epoch [10/100], Step [4600/13958], Loss: 6.5782\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [11/100], Step [100/13958], Loss: 6.6407\n",
      "Epoch [11/100], Step [200/13958], Loss: 6.0313\n",
      "Epoch [11/100], Step [300/13958], Loss: 6.7344\n",
      "Epoch [11/100], Step [400/13958], Loss: 9.1406\n",
      "Epoch [11/100], Step [500/13958], Loss: 6.2500\n",
      "Epoch [11/100], Step [600/13958], Loss: 7.2500\n",
      "Epoch [11/100], Step [700/13958], Loss: 10.0001\n",
      "Epoch [11/100], Step [800/13958], Loss: 7.1251\n",
      "Epoch [11/100], Step [900/13958], Loss: 8.2659\n",
      "Epoch [11/100], Step [1000/13958], Loss: 6.9219\n",
      "Epoch [11/100], Step [1100/13958], Loss: 6.0000\n",
      "Epoch [11/100], Step [1200/13958], Loss: 8.7188\n",
      "Epoch [11/100], Step [1300/13958], Loss: 8.5938\n",
      "Epoch [11/100], Step [1400/13958], Loss: 5.6251\n",
      "Epoch [11/100], Step [1500/13958], Loss: 6.5781\n",
      "Epoch [11/100], Step [1600/13958], Loss: 7.3908\n",
      "Epoch [11/100], Step [1700/13958], Loss: 6.9219\n",
      "Epoch [11/100], Step [1800/13958], Loss: 7.0469\n",
      "Epoch [11/100], Step [1900/13958], Loss: 10.4063\n",
      "Epoch [11/100], Step [2000/13958], Loss: 7.5469\n",
      "Epoch [11/100], Step [2100/13958], Loss: 8.6563\n",
      "Epoch [11/100], Step [2200/13958], Loss: 8.0157\n",
      "Epoch [11/100], Step [2300/13958], Loss: 7.9062\n",
      "Epoch [11/100], Step [2400/13958], Loss: 9.0939\n",
      "Epoch [11/100], Step [2500/13958], Loss: 8.5469\n",
      "Epoch [11/100], Step [2600/13958], Loss: 9.0626\n",
      "Epoch [11/100], Step [2700/13958], Loss: 7.8906\n",
      "Epoch [11/100], Step [2800/13958], Loss: 9.1407\n",
      "Epoch [11/100], Step [2900/13958], Loss: 9.6564\n",
      "Epoch [11/100], Step [3000/13958], Loss: 9.0782\n",
      "Epoch [11/100], Step [3100/13958], Loss: 8.7032\n",
      "Epoch [11/100], Step [3200/13958], Loss: 8.3438\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [12/100], Step [100/13958], Loss: 10.3594\n",
      "Epoch [12/100], Step [200/13958], Loss: 10.7500\n",
      "Epoch [12/100], Step [300/13958], Loss: 6.4375\n",
      "Epoch [12/100], Step [400/13958], Loss: 7.9063\n",
      "Epoch [12/100], Step [500/13958], Loss: 7.5469\n",
      "Epoch [12/100], Step [600/13958], Loss: 9.9689\n",
      "Epoch [12/100], Step [700/13958], Loss: 8.0782\n",
      "Epoch [12/100], Step [800/13958], Loss: 7.6563\n",
      "Epoch [12/100], Step [900/13958], Loss: 8.7344\n",
      "Epoch [12/100], Step [1000/13958], Loss: 8.2969\n",
      "Epoch [12/100], Step [1100/13958], Loss: 8.3906\n",
      "Epoch [12/100], Step [1200/13958], Loss: 6.9063\n",
      "Epoch [12/100], Step [1300/13958], Loss: 7.0469\n",
      "Epoch [12/100], Step [1400/13958], Loss: 9.3596\n",
      "Epoch [12/100], Step [1500/13958], Loss: 7.0001\n",
      "Epoch [12/100], Step [1600/13958], Loss: 5.0000\n",
      "Epoch [12/100], Step [1700/13958], Loss: 7.1250\n",
      "Epoch [12/100], Step [1800/13958], Loss: 7.8125\n",
      "Epoch [12/100], Step [1900/13958], Loss: 5.0156\n",
      "Epoch [12/100], Step [2000/13958], Loss: 9.8907\n",
      "Epoch [12/100], Step [2100/13958], Loss: 9.8594\n",
      "Epoch [12/100], Step [2200/13958], Loss: 6.5781\n",
      "Epoch [12/100], Step [2300/13958], Loss: 11.6094\n",
      "Epoch [12/100], Step [2400/13958], Loss: 7.0469\n",
      "Epoch [12/100], Step [2500/13958], Loss: 7.9063\n",
      "Epoch [12/100], Step [2600/13958], Loss: 5.5157\n",
      "Epoch [12/100], Step [2700/13958], Loss: 4.4531\n",
      "Epoch [12/100], Step [2800/13958], Loss: 4.9846\n",
      "Epoch [12/100], Step [2900/13958], Loss: 8.9375\n",
      "Epoch [12/100], Step [3000/13958], Loss: 5.6875\n",
      "Epoch [12/100], Step [3100/13958], Loss: 9.6563\n",
      "Epoch [12/100], Step [3200/13958], Loss: 8.0313\n",
      "Epoch [12/100], Step [3300/13958], Loss: 5.1407\n",
      "Epoch [12/100], Step [3400/13958], Loss: 7.3281\n",
      "Epoch [12/100], Step [3500/13958], Loss: 7.6876\n",
      "Epoch [12/100], Step [3600/13958], Loss: 8.5156\n",
      "Epoch [12/100], Step [3700/13958], Loss: 7.2344\n",
      "Epoch [12/100], Step [3800/13958], Loss: 7.8282\n",
      "Epoch [12/100], Step [3900/13958], Loss: 6.8908\n",
      "Epoch [12/100], Step [4000/13958], Loss: 5.7344\n",
      "Epoch [12/100], Step [4100/13958], Loss: 6.1407\n",
      "Epoch [12/100], Step [4200/13958], Loss: 10.7033\n",
      "Epoch [12/100], Step [4300/13958], Loss: 9.5313\n",
      "Epoch [12/100], Step [4400/13958], Loss: 8.2969\n",
      "Epoch [12/100], Step [4500/13958], Loss: 5.6721\n",
      "Epoch [12/100], Step [4600/13958], Loss: 5.0000\n",
      "Epoch [12/100], Step [4700/13958], Loss: 5.3906\n",
      "Epoch [12/100], Step [4800/13958], Loss: 6.0938\n",
      "Epoch [12/100], Step [4900/13958], Loss: 6.4844\n",
      "Epoch [12/100], Step [5000/13958], Loss: 6.0782\n",
      "Epoch [12/100], Step [5100/13958], Loss: 8.2813\n",
      "Epoch [12/100], Step [5200/13958], Loss: 8.9219\n",
      "Epoch [12/100], Step [5300/13958], Loss: 6.1094\n",
      "Epoch [12/100], Step [5400/13958], Loss: 6.2031\n",
      "Epoch [12/100], Step [5500/13958], Loss: 6.8752\n",
      "Epoch [12/100], Step [5600/13958], Loss: 5.7032\n",
      "Epoch [12/100], Step [5700/13958], Loss: 8.5469\n",
      "Epoch [12/100], Step [5800/13958], Loss: 5.7813\n",
      "Epoch [12/100], Step [5900/13958], Loss: 6.6877\n",
      "Epoch [12/100], Step [6000/13958], Loss: 11.4064\n",
      "Epoch [12/100], Step [6100/13958], Loss: 6.2500\n",
      "Epoch [12/100], Step [6200/13958], Loss: 10.7188\n",
      "Epoch [12/100], Step [6300/13958], Loss: 10.0782\n",
      "Epoch [12/100], Step [6400/13958], Loss: 8.5469\n",
      "Epoch [12/100], Step [6500/13958], Loss: 6.5313\n",
      "Epoch [12/100], Step [6600/13958], Loss: 6.8438\n",
      "Epoch [12/100], Step [6700/13958], Loss: 9.5469\n",
      "Epoch [12/100], Step [6800/13958], Loss: 8.4064\n",
      "Epoch [12/100], Step [6900/13958], Loss: 9.9688\n",
      "Epoch [12/100], Step [7000/13958], Loss: 5.4375\n",
      "Epoch [12/100], Step [7100/13958], Loss: 8.2813\n",
      "Epoch [12/100], Step [7200/13958], Loss: 9.2032\n",
      "Epoch [12/100], Step [7300/13958], Loss: 9.3906\n",
      "Epoch [12/100], Step [7400/13958], Loss: 5.8594\n",
      "Epoch [12/100], Step [7500/13958], Loss: 7.1406\n",
      "Epoch [12/100], Step [7600/13958], Loss: 7.7188\n",
      "Epoch [12/100], Step [7700/13958], Loss: 5.0939\n",
      "Epoch [12/100], Step [7800/13958], Loss: 7.1407\n",
      "Epoch [12/100], Step [7900/13958], Loss: 6.9844\n",
      "Epoch [12/100], Step [8000/13958], Loss: 5.8750\n",
      "Epoch [12/100], Step [8100/13958], Loss: 6.7188\n",
      "Epoch [12/100], Step [8200/13958], Loss: 7.5000\n",
      "Epoch [12/100], Step [8300/13958], Loss: 6.3282\n",
      "Epoch [12/100], Step [8400/13958], Loss: 8.2501\n",
      "Epoch [12/100], Step [8500/13958], Loss: 6.5626\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [13/100], Step [100/13958], Loss: 6.2969\n",
      "Epoch [13/100], Step [200/13958], Loss: 8.7969\n",
      "Epoch [13/100], Step [300/13958], Loss: 8.5782\n",
      "Epoch [13/100], Step [400/13958], Loss: 6.9531\n",
      "Epoch [13/100], Step [500/13958], Loss: 9.9375\n",
      "Epoch [13/100], Step [600/13958], Loss: 9.7032\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [14/100], Step [100/13958], Loss: 8.4844\n",
      "Epoch [14/100], Step [200/13958], Loss: 7.4688\n",
      "Epoch [14/100], Step [300/13958], Loss: 10.6719\n",
      "Epoch [14/100], Step [400/13958], Loss: 7.0939\n",
      "Epoch [14/100], Step [500/13958], Loss: 5.6563\n",
      "Epoch [14/100], Step [600/13958], Loss: 9.5782\n",
      "Epoch [14/100], Step [700/13958], Loss: 9.2501\n",
      "Epoch [14/100], Step [800/13958], Loss: 5.0313\n",
      "Epoch [14/100], Step [900/13958], Loss: 6.2500\n",
      "Epoch [14/100], Step [1000/13958], Loss: 8.8906\n",
      "Epoch [14/100], Step [1100/13958], Loss: 8.6406\n",
      "Epoch [14/100], Step [1200/13958], Loss: 7.5000\n",
      "Epoch [14/100], Step [1300/13958], Loss: 7.8438\n",
      "Epoch [14/100], Step [1400/13958], Loss: 7.0469\n",
      "Epoch [14/100], Step [1500/13958], Loss: 5.4531\n",
      "Epoch [14/100], Step [1600/13958], Loss: 6.1406\n",
      "Epoch [14/100], Step [1700/13958], Loss: 8.5469\n",
      "Epoch [14/100], Step [1800/13958], Loss: 6.3281\n",
      "Epoch [14/100], Step [1900/13958], Loss: 10.7657\n",
      "Epoch [14/100], Step [2000/13958], Loss: 8.8594\n",
      "Epoch [14/100], Step [2100/13958], Loss: 5.8907\n",
      "Epoch [14/100], Step [2200/13958], Loss: 10.6407\n",
      "Epoch [14/100], Step [2300/13958], Loss: 10.4844\n",
      "Epoch [14/100], Step [2400/13958], Loss: 8.2031\n",
      "Epoch [14/100], Step [2500/13958], Loss: 8.1408\n",
      "Epoch [14/100], Step [2600/13958], Loss: 5.1875\n",
      "Epoch [14/100], Step [2700/13958], Loss: 8.1094\n",
      "Epoch [14/100], Step [2800/13958], Loss: 6.2344\n",
      "Epoch [14/100], Step [2900/13958], Loss: 7.1094\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [15/100], Step [100/13958], Loss: 5.7344\n",
      "Epoch [15/100], Step [200/13958], Loss: 8.3282\n",
      "Epoch [15/100], Step [300/13958], Loss: 7.2500\n",
      "Epoch [15/100], Step [400/13958], Loss: 6.3594\n",
      "Epoch [15/100], Step [500/13958], Loss: 6.7660\n",
      "Epoch [15/100], Step [600/13958], Loss: 7.4688\n",
      "Epoch [15/100], Step [700/13958], Loss: 8.9844\n",
      "Epoch [15/100], Step [800/13958], Loss: 7.1407\n",
      "Epoch [15/100], Step [900/13958], Loss: 8.1251\n",
      "Epoch [15/100], Step [1000/13958], Loss: 10.2032\n",
      "Epoch [15/100], Step [1100/13958], Loss: 9.2657\n",
      "Epoch [15/100], Step [1200/13958], Loss: 6.5313\n",
      "Epoch [15/100], Step [1300/13958], Loss: 10.3282\n",
      "Epoch [15/100], Step [1400/13958], Loss: 8.5781\n",
      "Epoch [15/100], Step [1500/13958], Loss: 7.1250\n",
      "Epoch [15/100], Step [1600/13958], Loss: 5.5157\n",
      "Epoch [15/100], Step [1700/13958], Loss: 9.9532\n",
      "Epoch [15/100], Step [1800/13958], Loss: 6.8282\n",
      "Epoch [15/100], Step [1900/13958], Loss: 5.3438\n",
      "Epoch [15/100], Step [2000/13958], Loss: 6.0469\n",
      "Epoch [15/100], Step [2100/13958], Loss: 6.7500\n",
      "Epoch [15/100], Step [2200/13958], Loss: 9.4063\n",
      "Epoch [15/100], Step [2300/13958], Loss: 6.9844\n",
      "Epoch [15/100], Step [2400/13958], Loss: 9.6719\n",
      "Epoch [15/100], Step [2500/13958], Loss: 6.9063\n",
      "Epoch [15/100], Step [2600/13958], Loss: 6.4688\n",
      "Epoch [15/100], Step [2700/13958], Loss: 6.1719\n",
      "Epoch [15/100], Step [2800/13958], Loss: 5.1564\n",
      "Epoch [15/100], Step [2900/13958], Loss: 8.9688\n",
      "Epoch [15/100], Step [3000/13958], Loss: 4.4532\n",
      "Epoch [15/100], Step [3100/13958], Loss: 9.9219\n",
      "Epoch [15/100], Step [3200/13958], Loss: 5.9688\n",
      "Epoch [15/100], Step [3300/13958], Loss: 7.2344\n",
      "Epoch [15/100], Step [3400/13958], Loss: 6.3594\n",
      "Epoch [15/100], Step [3500/13958], Loss: 7.2813\n",
      "Epoch [15/100], Step [3600/13958], Loss: 5.8750\n",
      "Epoch [15/100], Step [3700/13958], Loss: 9.4062\n",
      "Epoch [15/100], Step [3800/13958], Loss: 5.8751\n",
      "Epoch [15/100], Step [3900/13958], Loss: 8.6406\n",
      "Epoch [15/100], Step [4000/13958], Loss: 5.8595\n",
      "Epoch [15/100], Step [4100/13958], Loss: 5.2188\n",
      "Epoch [15/100], Step [4200/13958], Loss: 7.3125\n",
      "Epoch [15/100], Step [4300/13958], Loss: 8.4063\n",
      "Epoch [15/100], Step [4400/13958], Loss: 6.2501\n",
      "Epoch [15/100], Step [4500/13958], Loss: 9.2344\n",
      "Epoch [15/100], Step [4600/13958], Loss: 9.4844\n",
      "Epoch [15/100], Step [4700/13958], Loss: 8.3281\n",
      "Epoch [15/100], Step [4800/13958], Loss: 8.7189\n",
      "Epoch [15/100], Step [4900/13958], Loss: 9.3594\n",
      "Epoch [15/100], Step [5000/13958], Loss: 12.0938\n",
      "Epoch [15/100], Step [5100/13958], Loss: 10.9531\n",
      "Epoch [15/100], Step [5200/13958], Loss: 8.7657\n",
      "Epoch [15/100], Step [5300/13958], Loss: 7.8438\n",
      "Epoch [15/100], Step [5400/13958], Loss: 6.0313\n",
      "Epoch [15/100], Step [5500/13958], Loss: 7.3438\n",
      "Epoch [15/100], Step [5600/13958], Loss: 7.7188\n",
      "Epoch [15/100], Step [5700/13958], Loss: 4.8126\n",
      "Epoch [15/100], Step [5800/13958], Loss: 7.0938\n",
      "Epoch [15/100], Step [5900/13958], Loss: 7.4844\n",
      "Epoch [15/100], Step [6000/13958], Loss: 6.0313\n",
      "Epoch [15/100], Step [6100/13958], Loss: 6.2813\n",
      "Epoch [15/100], Step [6200/13958], Loss: 8.2344\n",
      "Epoch [15/100], Step [6300/13958], Loss: 9.2501\n",
      "Epoch [15/100], Step [6400/13958], Loss: 7.4531\n",
      "Epoch [15/100], Step [6500/13958], Loss: 5.0469\n",
      "Epoch [15/100], Step [6600/13958], Loss: 7.6407\n",
      "Epoch [15/100], Step [6700/13958], Loss: 10.0626\n",
      "Epoch [15/100], Step [6800/13958], Loss: 5.6251\n",
      "Epoch [15/100], Step [6900/13958], Loss: 6.6563\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [16/100], Step [100/13958], Loss: 6.4844\n",
      "Epoch [16/100], Step [200/13958], Loss: 6.7657\n",
      "Epoch [16/100], Step [300/13958], Loss: 6.3125\n",
      "Epoch [16/100], Step [400/13958], Loss: 6.1250\n",
      "Epoch [16/100], Step [500/13958], Loss: 11.2188\n",
      "Epoch [16/100], Step [600/13958], Loss: 7.0782\n",
      "Epoch [16/100], Step [700/13958], Loss: 4.9688\n",
      "Epoch [16/100], Step [800/13958], Loss: 9.4063\n",
      "Epoch [16/100], Step [900/13958], Loss: 5.8125\n",
      "Epoch [16/100], Step [1000/13958], Loss: 6.7189\n",
      "Epoch [16/100], Step [1100/13958], Loss: 6.1250\n",
      "Epoch [16/100], Step [1200/13958], Loss: 4.9845\n",
      "Epoch [16/100], Step [1300/13958], Loss: 6.9219\n",
      "Epoch [16/100], Step [1400/13958], Loss: 7.2344\n",
      "Epoch [16/100], Step [1500/13958], Loss: 8.1719\n",
      "Epoch [16/100], Step [1600/13958], Loss: 7.3282\n",
      "Epoch [16/100], Step [1700/13958], Loss: 9.6564\n",
      "Epoch [16/100], Step [1800/13958], Loss: 6.9219\n",
      "Epoch [16/100], Step [1900/13958], Loss: 5.9375\n",
      "Epoch [16/100], Step [2000/13958], Loss: 6.5313\n",
      "Epoch [16/100], Step [2100/13958], Loss: 6.5313\n",
      "Epoch [16/100], Step [2200/13958], Loss: 8.5001\n",
      "Epoch [16/100], Step [2300/13958], Loss: 7.1094\n",
      "Epoch [16/100], Step [2400/13958], Loss: 7.2813\n",
      "Epoch [16/100], Step [2500/13958], Loss: 10.7813\n",
      "Epoch [16/100], Step [2600/13958], Loss: 6.8752\n",
      "Epoch [16/100], Step [2700/13958], Loss: 8.3750\n",
      "Epoch [16/100], Step [2800/13958], Loss: 6.9219\n",
      "Epoch [16/100], Step [2900/13958], Loss: 10.0313\n",
      "Epoch [16/100], Step [3000/13958], Loss: 6.4844\n",
      "Epoch [16/100], Step [3100/13958], Loss: 8.5000\n",
      "Epoch [16/100], Step [3200/13958], Loss: 6.9219\n",
      "Epoch [16/100], Step [3300/13958], Loss: 10.2969\n",
      "Epoch [16/100], Step [3400/13958], Loss: 8.2813\n",
      "Epoch [16/100], Step [3500/13958], Loss: 10.6406\n",
      "Epoch [16/100], Step [3600/13958], Loss: 9.0313\n",
      "Epoch [16/100], Step [3700/13958], Loss: 6.5782\n",
      "Epoch [16/100], Step [3800/13958], Loss: 8.9063\n",
      "Epoch [16/100], Step [3900/13958], Loss: 5.1407\n",
      "Epoch [16/100], Step [4000/13958], Loss: 8.2969\n",
      "Epoch [16/100], Step [4100/13958], Loss: 5.6407\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [17/100], Step [100/13958], Loss: 6.8127\n",
      "Epoch [17/100], Step [200/13958], Loss: 8.0938\n",
      "Epoch [17/100], Step [300/13958], Loss: 7.6406\n",
      "Epoch [17/100], Step [400/13958], Loss: 7.2657\n",
      "Epoch [17/100], Step [500/13958], Loss: 12.5782\n",
      "Epoch [17/100], Step [600/13958], Loss: 10.8751\n",
      "Epoch [17/100], Step [700/13958], Loss: 9.1251\n",
      "Epoch [17/100], Step [800/13958], Loss: 7.8750\n",
      "Epoch [17/100], Step [900/13958], Loss: 9.5625\n",
      "Epoch [17/100], Step [1000/13958], Loss: 9.0156\n",
      "Epoch [17/100], Step [1100/13958], Loss: 9.1719\n",
      "Epoch [17/100], Step [1200/13958], Loss: 6.2969\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [18/100], Step [100/13958], Loss: 4.1095\n",
      "Epoch [18/100], Step [200/13958], Loss: 8.7657\n",
      "Epoch [18/100], Step [300/13958], Loss: 6.5313\n",
      "Epoch [18/100], Step [400/13958], Loss: 7.0000\n",
      "Epoch [18/100], Step [500/13958], Loss: 7.2188\n",
      "Epoch [18/100], Step [600/13958], Loss: 7.7969\n",
      "Epoch [18/100], Step [700/13958], Loss: 11.2344\n",
      "Epoch [18/100], Step [800/13958], Loss: 7.7188\n",
      "Epoch [18/100], Step [900/13958], Loss: 10.8281\n",
      "Epoch [18/100], Step [1000/13958], Loss: 9.2344\n",
      "Epoch [18/100], Step [1100/13958], Loss: 6.6095\n",
      "Epoch [18/100], Step [1200/13958], Loss: 6.0625\n",
      "Epoch [18/100], Step [1300/13958], Loss: 10.9219\n",
      "Epoch [18/100], Step [1400/13958], Loss: 12.1875\n",
      "Epoch [18/100], Step [1500/13958], Loss: 8.0783\n",
      "Epoch [18/100], Step [1600/13958], Loss: 9.0938\n",
      "Epoch [18/100], Step [1700/13958], Loss: 9.6719\n",
      "Epoch [18/100], Step [1800/13958], Loss: 9.6406\n",
      "Epoch [18/100], Step [1900/13958], Loss: 6.3594\n",
      "Epoch [18/100], Step [2000/13958], Loss: 3.8438\n",
      "Epoch [18/100], Step [2100/13958], Loss: 5.5938\n",
      "Epoch [18/100], Step [2200/13958], Loss: 8.2344\n",
      "Epoch [18/100], Step [2300/13958], Loss: 7.8594\n",
      "Epoch [18/100], Step [2400/13958], Loss: 6.4063\n",
      "Epoch [18/100], Step [2500/13958], Loss: 5.5938\n",
      "Epoch [18/100], Step [2600/13958], Loss: 6.4844\n",
      "Epoch [18/100], Step [2700/13958], Loss: 7.7502\n",
      "Epoch [18/100], Step [2800/13958], Loss: 5.2658\n",
      "Epoch [18/100], Step [2900/13958], Loss: 8.2344\n",
      "Epoch [18/100], Step [3000/13958], Loss: 5.0000\n",
      "Epoch [18/100], Step [3100/13958], Loss: 10.1720\n",
      "Epoch [18/100], Step [3200/13958], Loss: 8.0000\n",
      "Epoch [18/100], Step [3300/13958], Loss: 6.7034\n",
      "Epoch [18/100], Step [3400/13958], Loss: 7.0625\n",
      "Epoch [18/100], Step [3500/13958], Loss: 9.1250\n",
      "Epoch [18/100], Step [3600/13958], Loss: 6.2813\n",
      "Epoch [18/100], Step [3700/13958], Loss: 7.4532\n",
      "Epoch [18/100], Step [3800/13958], Loss: 7.9219\n",
      "Epoch [18/100], Step [3900/13958], Loss: 6.6719\n",
      "Epoch [18/100], Step [4000/13958], Loss: 8.7344\n",
      "Epoch [18/100], Step [4100/13958], Loss: 6.6876\n",
      "Epoch [18/100], Step [4200/13958], Loss: 3.9376\n",
      "Epoch [18/100], Step [4300/13958], Loss: 7.3281\n",
      "Epoch [18/100], Step [4400/13958], Loss: 8.5313\n",
      "Epoch [18/100], Step [4500/13958], Loss: 7.7188\n",
      "Epoch [18/100], Step [4600/13958], Loss: 7.5625\n",
      "Epoch [18/100], Step [4700/13958], Loss: 8.0782\n",
      "Epoch [18/100], Step [4800/13958], Loss: 9.3751\n",
      "Epoch [18/100], Step [4900/13958], Loss: 9.8438\n",
      "Epoch [18/100], Step [5000/13958], Loss: 9.9375\n",
      "Epoch [18/100], Step [5100/13958], Loss: 5.5938\n",
      "Epoch [18/100], Step [5200/13958], Loss: 6.8750\n",
      "Epoch [18/100], Step [5300/13958], Loss: 5.5313\n",
      "Epoch [18/100], Step [5400/13958], Loss: 7.6719\n",
      "Epoch [18/100], Step [5500/13958], Loss: 7.4375\n",
      "Epoch [18/100], Step [5600/13958], Loss: 7.3126\n",
      "Epoch [18/100], Step [5700/13958], Loss: 9.0313\n",
      "Epoch [18/100], Step [5800/13958], Loss: 8.0938\n",
      "Epoch [18/100], Step [5900/13958], Loss: 10.0626\n",
      "Epoch [18/100], Step [6000/13958], Loss: 8.4063\n",
      "Epoch [18/100], Step [6100/13958], Loss: 7.0469\n",
      "Epoch [18/100], Step [6200/13958], Loss: 5.9688\n",
      "Epoch [18/100], Step [6300/13958], Loss: 8.9220\n",
      "Epoch [18/100], Step [6400/13958], Loss: 10.7501\n",
      "Epoch [18/100], Step [6500/13958], Loss: 10.2969\n",
      "Epoch [18/100], Step [6600/13958], Loss: 6.5314\n",
      "Epoch [18/100], Step [6700/13958], Loss: 6.5938\n",
      "Epoch [18/100], Step [6800/13958], Loss: 6.3283\n",
      "Epoch [18/100], Step [6900/13958], Loss: 6.2188\n",
      "Epoch [18/100], Step [7000/13958], Loss: 7.5001\n",
      "Epoch [18/100], Step [7100/13958], Loss: 5.4063\n",
      "Epoch [18/100], Step [7200/13958], Loss: 6.1407\n",
      "Epoch [18/100], Step [7300/13958], Loss: 8.6875\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [19/100], Step [100/13958], Loss: 9.6719\n",
      "Epoch [19/100], Step [200/13958], Loss: 6.3907\n",
      "Epoch [19/100], Step [300/13958], Loss: 6.3125\n",
      "Epoch [19/100], Step [400/13958], Loss: 7.9220\n",
      "Epoch [19/100], Step [500/13958], Loss: 6.7031\n",
      "Epoch [19/100], Step [600/13958], Loss: 10.0938\n",
      "Epoch [19/100], Step [700/13958], Loss: 6.2969\n",
      "Epoch [19/100], Step [800/13958], Loss: 8.8594\n",
      "Epoch [19/100], Step [900/13958], Loss: 9.9531\n",
      "Epoch [19/100], Step [1000/13958], Loss: 7.2813\n",
      "Epoch [19/100], Step [1100/13958], Loss: 10.0313\n",
      "Epoch [19/100], Step [1200/13958], Loss: 9.7656\n",
      "Epoch [19/100], Step [1300/13958], Loss: 8.5938\n",
      "Epoch [19/100], Step [1400/13958], Loss: 10.2813\n",
      "Epoch [19/100], Step [1500/13958], Loss: 8.8438\n",
      "Epoch [19/100], Step [1600/13958], Loss: 10.5315\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [20/100], Step [100/13958], Loss: 7.5157\n",
      "Epoch [20/100], Step [200/13958], Loss: 10.3281\n",
      "Epoch [20/100], Step [300/13958], Loss: 10.8126\n",
      "Epoch [20/100], Step [400/13958], Loss: 7.2031\n",
      "Epoch [20/100], Step [500/13958], Loss: 4.3594\n",
      "Epoch [20/100], Step [600/13958], Loss: 7.6563\n",
      "Epoch [20/100], Step [700/13958], Loss: 6.3438\n",
      "Epoch [20/100], Step [800/13958], Loss: 4.6719\n",
      "Epoch [20/100], Step [900/13958], Loss: 7.5000\n",
      "Epoch [20/100], Step [1000/13958], Loss: 6.1250\n",
      "Epoch [20/100], Step [1100/13958], Loss: 9.5313\n",
      "Epoch [20/100], Step [1200/13958], Loss: 5.2031\n",
      "Epoch [20/100], Step [1300/13958], Loss: 7.9220\n",
      "Epoch [20/100], Step [1400/13958], Loss: 8.5001\n",
      "Epoch [20/100], Step [1500/13958], Loss: 8.1408\n",
      "Epoch [20/100], Step [1600/13958], Loss: 12.5001\n",
      "Epoch [20/100], Step [1700/13958], Loss: 6.7812\n",
      "Epoch [20/100], Step [1800/13958], Loss: 5.9531\n",
      "Epoch [20/100], Step [1900/13958], Loss: 5.6875\n",
      "Epoch [20/100], Step [2000/13958], Loss: 6.4375\n",
      "Epoch [20/100], Step [2100/13958], Loss: 9.0313\n",
      "Epoch [20/100], Step [2200/13958], Loss: 7.4688\n",
      "Epoch [20/100], Step [2300/13958], Loss: 8.6875\n",
      "Epoch [20/100], Step [2400/13958], Loss: 4.8125\n",
      "Epoch [20/100], Step [2500/13958], Loss: 6.7345\n",
      "Epoch [20/100], Step [2600/13958], Loss: 7.3594\n",
      "Epoch [20/100], Step [2700/13958], Loss: 8.1406\n",
      "Epoch [20/100], Step [2800/13958], Loss: 6.2188\n",
      "Epoch [20/100], Step [2900/13958], Loss: 6.7969\n",
      "Epoch [20/100], Step [3000/13958], Loss: 7.2188\n",
      "Epoch [20/100], Step [3100/13958], Loss: 7.5626\n",
      "Epoch [20/100], Step [3200/13958], Loss: 6.9063\n",
      "Epoch [20/100], Step [3300/13958], Loss: 9.3595\n",
      "Epoch [20/100], Step [3400/13958], Loss: 8.6719\n",
      "Epoch [20/100], Step [3500/13958], Loss: 6.3283\n",
      "Epoch [20/100], Step [3600/13958], Loss: 8.2657\n",
      "Epoch [20/100], Step [3700/13958], Loss: 8.9688\n",
      "Epoch [20/100], Step [3800/13958], Loss: 9.5313\n",
      "Epoch [20/100], Step [3900/13958], Loss: 9.0000\n",
      "Epoch [20/100], Step [4000/13958], Loss: 5.7188\n",
      "Epoch [20/100], Step [4100/13958], Loss: 7.9688\n",
      "Epoch [20/100], Step [4200/13958], Loss: 8.0470\n",
      "Epoch [20/100], Step [4300/13958], Loss: 7.5469\n",
      "Epoch [20/100], Step [4400/13958], Loss: 6.7032\n",
      "Epoch [20/100], Step [4500/13958], Loss: 7.9844\n",
      "Epoch [20/100], Step [4600/13958], Loss: 6.2031\n",
      "Epoch [20/100], Step [4700/13958], Loss: 9.0782\n",
      "Epoch [20/100], Step [4800/13958], Loss: 7.7657\n",
      "Epoch [20/100], Step [4900/13958], Loss: 9.9844\n",
      "Epoch [20/100], Step [5000/13958], Loss: 5.4533\n",
      "Epoch [20/100], Step [5100/13958], Loss: 7.9844\n",
      "Epoch [20/100], Step [5200/13958], Loss: 7.1875\n",
      "Epoch [20/100], Step [5300/13958], Loss: 8.5000\n",
      "Epoch [20/100], Step [5400/13958], Loss: 5.4376\n",
      "Epoch [20/100], Step [5500/13958], Loss: 11.1875\n",
      "Epoch [20/100], Step [5600/13958], Loss: 9.0782\n",
      "Epoch [20/100], Step [5700/13958], Loss: 7.7657\n",
      "Epoch [20/100], Step [5800/13958], Loss: 8.6563\n",
      "Epoch [20/100], Step [5900/13958], Loss: 8.5000\n",
      "Epoch [20/100], Step [6000/13958], Loss: 6.8438\n",
      "Epoch [20/100], Step [6100/13958], Loss: 9.4063\n",
      "Epoch [20/100], Step [6200/13958], Loss: 6.4532\n",
      "Epoch [20/100], Step [6300/13958], Loss: 7.1563\n",
      "Epoch [20/100], Step [6400/13958], Loss: 10.6094\n",
      "Epoch [20/100], Step [6500/13958], Loss: 11.0626\n",
      "Epoch [20/100], Step [6600/13958], Loss: 8.7188\n",
      "Epoch [20/100], Step [6700/13958], Loss: 6.2344\n",
      "Epoch [20/100], Step [6800/13958], Loss: 7.4063\n",
      "Epoch [20/100], Step [6900/13958], Loss: 7.5625\n",
      "Epoch [20/100], Step [7000/13958], Loss: 7.0313\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [21/100], Step [100/13958], Loss: 10.1407\n",
      "Epoch [21/100], Step [200/13958], Loss: 9.7500\n",
      "Epoch [21/100], Step [300/13958], Loss: 9.8594\n",
      "Epoch [21/100], Step [400/13958], Loss: 9.3282\n",
      "Epoch [21/100], Step [500/13958], Loss: 7.5626\n",
      "Epoch [21/100], Step [600/13958], Loss: 8.6719\n",
      "Epoch [21/100], Step [700/13958], Loss: 7.5470\n",
      "Epoch [21/100], Step [800/13958], Loss: 13.4844\n",
      "Epoch [21/100], Step [900/13958], Loss: 9.9688\n",
      "Epoch [21/100], Step [1000/13958], Loss: 6.4219\n",
      "Epoch [21/100], Step [1100/13958], Loss: 7.0469\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [22/100], Step [100/13958], Loss: 4.4376\n",
      "Epoch [22/100], Step [200/13958], Loss: 10.1719\n",
      "Epoch [22/100], Step [300/13958], Loss: 5.8125\n",
      "Epoch [22/100], Step [400/13958], Loss: 6.8125\n",
      "Epoch [22/100], Step [500/13958], Loss: 5.5939\n",
      "Epoch [22/100], Step [600/13958], Loss: 5.8595\n",
      "Epoch [22/100], Step [700/13958], Loss: 9.0782\n",
      "Epoch [22/100], Step [800/13958], Loss: 5.5157\n",
      "Epoch [22/100], Step [900/13958], Loss: 8.2500\n",
      "Epoch [22/100], Step [1000/13958], Loss: 7.7031\n",
      "Epoch [22/100], Step [1100/13958], Loss: 7.1875\n",
      "Epoch [22/100], Step [1200/13958], Loss: 4.6876\n",
      "Epoch [22/100], Step [1300/13958], Loss: 5.5313\n",
      "Epoch [22/100], Step [1400/13958], Loss: 6.8750\n",
      "Epoch [22/100], Step [1500/13958], Loss: 7.0625\n",
      "Epoch [22/100], Step [1600/13958], Loss: 7.5782\n",
      "Epoch [22/100], Step [1700/13958], Loss: 6.0625\n",
      "Epoch [22/100], Step [1800/13958], Loss: 8.4375\n",
      "Epoch [22/100], Step [1900/13958], Loss: 6.9531\n",
      "Epoch [22/100], Step [2000/13958], Loss: 10.6094\n",
      "Epoch [22/100], Step [2100/13958], Loss: 4.9844\n",
      "Epoch [22/100], Step [2200/13958], Loss: 6.4375\n",
      "Epoch [22/100], Step [2300/13958], Loss: 7.6408\n",
      "Epoch [22/100], Step [2400/13958], Loss: 8.1564\n",
      "Epoch [22/100], Step [2500/13958], Loss: 9.0313\n",
      "Epoch [22/100], Step [2600/13958], Loss: 8.6094\n",
      "Epoch [22/100], Step [2700/13958], Loss: 5.3127\n",
      "Epoch [22/100], Step [2800/13958], Loss: 6.3282\n",
      "Epoch [22/100], Step [2900/13958], Loss: 9.9844\n",
      "Epoch [22/100], Step [3000/13958], Loss: 5.9844\n",
      "Epoch [22/100], Step [3100/13958], Loss: 7.4063\n",
      "Epoch [22/100], Step [3200/13958], Loss: 9.7813\n",
      "Epoch [22/100], Step [3300/13958], Loss: 9.3750\n",
      "Epoch [22/100], Step [3400/13958], Loss: 7.4375\n",
      "Epoch [22/100], Step [3500/13958], Loss: 10.1094\n",
      "Epoch [22/100], Step [3600/13958], Loss: 10.7031\n",
      "Epoch [22/100], Step [3700/13958], Loss: 7.1719\n",
      "Epoch [22/100], Step [3800/13958], Loss: 7.8281\n",
      "Epoch [22/100], Step [3900/13958], Loss: 6.1719\n",
      "Epoch [22/100], Step [4000/13958], Loss: 9.7813\n",
      "Epoch [22/100], Step [4100/13958], Loss: 7.7032\n",
      "Epoch [22/100], Step [4200/13958], Loss: 9.2657\n",
      "Epoch [22/100], Step [4300/13958], Loss: 9.4688\n",
      "Epoch [22/100], Step [4400/13958], Loss: 9.3751\n",
      "Epoch [22/100], Step [4500/13958], Loss: 7.6407\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [23/100], Step [100/13958], Loss: 5.7969\n",
      "Epoch [23/100], Step [200/13958], Loss: 7.9844\n",
      "Epoch [23/100], Step [300/13958], Loss: 7.7657\n",
      "Epoch [23/100], Step [400/13958], Loss: 5.1407\n",
      "Epoch [23/100], Step [500/13958], Loss: 7.0313\n",
      "Epoch [23/100], Step [600/13958], Loss: 6.3596\n",
      "Epoch [23/100], Step [700/13958], Loss: 5.6250\n",
      "Epoch [23/100], Step [800/13958], Loss: 12.9531\n",
      "Epoch [23/100], Step [900/13958], Loss: 8.2969\n",
      "Epoch [23/100], Step [1000/13958], Loss: 5.1875\n",
      "Epoch [23/100], Step [1100/13958], Loss: 7.9688\n",
      "Epoch [23/100], Step [1200/13958], Loss: 10.0156\n",
      "Epoch [23/100], Step [1300/13958], Loss: 7.2344\n",
      "Epoch [23/100], Step [1400/13958], Loss: 9.0000\n",
      "Epoch [23/100], Step [1500/13958], Loss: 8.0000\n",
      "Epoch [23/100], Step [1600/13958], Loss: 8.6407\n",
      "Epoch [23/100], Step [1700/13958], Loss: 10.4844\n",
      "Epoch [23/100], Step [1800/13958], Loss: 8.6250\n",
      "Epoch [23/100], Step [1900/13958], Loss: 5.5469\n",
      "Epoch [23/100], Step [2000/13958], Loss: 4.5938\n",
      "Epoch [23/100], Step [2100/13958], Loss: 8.9531\n",
      "Epoch [23/100], Step [2200/13958], Loss: 9.9220\n",
      "Epoch [23/100], Step [2300/13958], Loss: 7.6719\n",
      "Epoch [23/100], Step [2400/13958], Loss: 6.0000\n",
      "Epoch [23/100], Step [2500/13958], Loss: 8.7813\n",
      "Epoch [23/100], Step [2600/13958], Loss: 9.2190\n",
      "Epoch [23/100], Step [2700/13958], Loss: 5.1408\n",
      "Epoch [23/100], Step [2800/13958], Loss: 9.1875\n",
      "Epoch [23/100], Step [2900/13958], Loss: 7.1094\n",
      "Epoch [23/100], Step [3000/13958], Loss: 5.5782\n",
      "Epoch [23/100], Step [3100/13958], Loss: 7.6875\n",
      "Epoch [23/100], Step [3200/13958], Loss: 5.0471\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [24/100], Step [100/13958], Loss: 6.3127\n",
      "Epoch [24/100], Step [200/13958], Loss: 5.9688\n",
      "Epoch [24/100], Step [300/13958], Loss: 11.8282\n",
      "Epoch [24/100], Step [400/13958], Loss: 7.6250\n",
      "Epoch [24/100], Step [500/13958], Loss: 8.6719\n",
      "Epoch [24/100], Step [600/13958], Loss: 7.5158\n",
      "Epoch [24/100], Step [700/13958], Loss: 4.5000\n",
      "Epoch [24/100], Step [800/13958], Loss: 7.4532\n",
      "Epoch [24/100], Step [900/13958], Loss: 7.8594\n",
      "Epoch [24/100], Step [1000/13958], Loss: 8.4375\n",
      "Epoch [24/100], Step [1100/13958], Loss: 11.7970\n",
      "Epoch [24/100], Step [1200/13958], Loss: 4.5313\n",
      "Epoch [24/100], Step [1300/13958], Loss: 5.2813\n",
      "Epoch [24/100], Step [1400/13958], Loss: 8.0156\n",
      "Epoch [24/100], Step [1500/13958], Loss: 5.5469\n",
      "Epoch [24/100], Step [1600/13958], Loss: 10.5470\n",
      "Epoch [24/100], Step [1700/13958], Loss: 6.7500\n",
      "Epoch [24/100], Step [1800/13958], Loss: 8.2032\n",
      "Epoch [24/100], Step [1900/13958], Loss: 5.5157\n",
      "Epoch [24/100], Step [2000/13958], Loss: 5.2657\n",
      "Epoch [24/100], Step [2100/13958], Loss: 6.6875\n",
      "Epoch [24/100], Step [2200/13958], Loss: 7.1407\n",
      "Epoch [24/100], Step [2300/13958], Loss: 8.4531\n",
      "Epoch [24/100], Step [2400/13958], Loss: 9.1562\n",
      "Epoch [24/100], Step [2500/13958], Loss: 8.4688\n",
      "Epoch [24/100], Step [2600/13958], Loss: 5.5001\n",
      "Epoch [24/100], Step [2700/13958], Loss: 8.5000\n",
      "Epoch [24/100], Step [2800/13958], Loss: 9.2501\n",
      "Epoch [24/100], Step [2900/13958], Loss: 10.1094\n",
      "Epoch [24/100], Step [3000/13958], Loss: 5.7813\n",
      "Epoch [24/100], Step [3100/13958], Loss: 7.7501\n",
      "Epoch [24/100], Step [3200/13958], Loss: 9.7344\n",
      "Epoch [24/100], Step [3300/13958], Loss: 10.0625\n",
      "Epoch [24/100], Step [3400/13958], Loss: 7.5470\n",
      "Epoch [24/100], Step [3500/13958], Loss: 10.3284\n",
      "Epoch [24/100], Step [3600/13958], Loss: 9.5781\n",
      "Epoch [24/100], Step [3700/13958], Loss: 5.4375\n",
      "Epoch [24/100], Step [3800/13958], Loss: 8.6408\n",
      "Epoch [24/100], Step [3900/13958], Loss: 11.2813\n",
      "Epoch [24/100], Step [4000/13958], Loss: 6.7188\n",
      "Epoch [24/100], Step [4100/13958], Loss: 5.8751\n",
      "Epoch [24/100], Step [4200/13958], Loss: 12.6719\n",
      "Epoch [24/100], Step [4300/13958], Loss: 7.9688\n",
      "Epoch [24/100], Step [4400/13958], Loss: 8.7969\n",
      "Epoch [24/100], Step [4500/13958], Loss: 6.3594\n",
      "Epoch [24/100], Step [4600/13958], Loss: 6.5470\n",
      "Epoch [24/100], Step [4700/13958], Loss: 11.1563\n",
      "Epoch [24/100], Step [4800/13958], Loss: 6.5782\n",
      "Epoch [24/100], Step [4900/13958], Loss: 6.4533\n",
      "Epoch [24/100], Step [5000/13958], Loss: 8.2344\n",
      "Epoch [24/100], Step [5100/13958], Loss: 8.4219\n",
      "Epoch [24/100], Step [5200/13958], Loss: 4.9532\n",
      "Epoch [24/100], Step [5300/13958], Loss: 9.4688\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [25/100], Step [100/13958], Loss: 6.9375\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [26/100], Step [100/13958], Loss: 6.3439\n",
      "Epoch [26/100], Step [200/13958], Loss: 7.3438\n",
      "Epoch [26/100], Step [300/13958], Loss: 5.8125\n",
      "Epoch [26/100], Step [400/13958], Loss: 5.7969\n",
      "Epoch [26/100], Step [500/13958], Loss: 6.1407\n",
      "Epoch [26/100], Step [600/13958], Loss: 8.0939\n",
      "Epoch [26/100], Step [700/13958], Loss: 8.1875\n",
      "Epoch [26/100], Step [800/13958], Loss: 5.3594\n",
      "Epoch [26/100], Step [900/13958], Loss: 6.9221\n",
      "Epoch [26/100], Step [1000/13958], Loss: 6.8907\n",
      "Epoch [26/100], Step [1100/13958], Loss: 6.9221\n",
      "Epoch [26/100], Step [1200/13958], Loss: 9.1720\n",
      "Epoch [26/100], Step [1300/13958], Loss: 7.5469\n",
      "Epoch [26/100], Step [1400/13958], Loss: 9.9063\n",
      "Epoch [26/100], Step [1500/13958], Loss: 9.1095\n",
      "Epoch [26/100], Step [1600/13958], Loss: 5.9688\n",
      "Epoch [26/100], Step [1700/13958], Loss: 3.4688\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [27/100], Step [100/13958], Loss: 7.1719\n",
      "Epoch [27/100], Step [200/13958], Loss: 6.4375\n",
      "Epoch [27/100], Step [300/13958], Loss: 6.1875\n",
      "Epoch [27/100], Step [400/13958], Loss: 7.2500\n",
      "Epoch [27/100], Step [500/13958], Loss: 9.5157\n",
      "Epoch [27/100], Step [600/13958], Loss: 7.6407\n",
      "Epoch [27/100], Step [700/13958], Loss: 7.7969\n",
      "Epoch [27/100], Step [800/13958], Loss: 7.6406\n",
      "Epoch [27/100], Step [900/13958], Loss: 7.0470\n",
      "Epoch [27/100], Step [1000/13958], Loss: 8.7656\n",
      "Epoch [27/100], Step [1100/13958], Loss: 3.5470\n",
      "Epoch [27/100], Step [1200/13958], Loss: 6.6563\n",
      "Epoch [27/100], Step [1300/13958], Loss: 8.3907\n",
      "Epoch [27/100], Step [1400/13958], Loss: 7.6406\n",
      "Epoch [27/100], Step [1500/13958], Loss: 6.2345\n",
      "Epoch [27/100], Step [1600/13958], Loss: 7.1876\n",
      "Epoch [27/100], Step [1700/13958], Loss: 7.7188\n",
      "Epoch [27/100], Step [1800/13958], Loss: 7.7344\n",
      "Epoch [27/100], Step [1900/13958], Loss: 5.7969\n",
      "Epoch [27/100], Step [2000/13958], Loss: 8.1875\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [28/100], Step [100/13958], Loss: 7.0313\n",
      "Epoch [28/100], Step [200/13958], Loss: 8.1408\n",
      "Epoch [28/100], Step [300/13958], Loss: 6.1406\n",
      "Epoch [28/100], Step [400/13958], Loss: 6.9531\n",
      "Epoch [28/100], Step [500/13958], Loss: 8.9376\n",
      "Epoch [28/100], Step [600/13958], Loss: 4.9689\n",
      "Epoch [28/100], Step [700/13958], Loss: 6.5157\n",
      "Epoch [28/100], Step [800/13958], Loss: 5.0783\n",
      "Epoch [28/100], Step [900/13958], Loss: 6.4376\n",
      "Epoch [28/100], Step [1000/13958], Loss: 7.8281\n",
      "Epoch [28/100], Step [1100/13958], Loss: 6.3907\n",
      "Epoch [28/100], Step [1200/13958], Loss: 6.8126\n",
      "Epoch [28/100], Step [1300/13958], Loss: 10.1875\n",
      "Epoch [28/100], Step [1400/13958], Loss: 8.4688\n",
      "Epoch [28/100], Step [1500/13958], Loss: 7.4688\n",
      "Epoch [28/100], Step [1600/13958], Loss: 7.1876\n",
      "Epoch [28/100], Step [1700/13958], Loss: 10.9219\n",
      "Epoch [28/100], Step [1800/13958], Loss: 12.5313\n",
      "Epoch [28/100], Step [1900/13958], Loss: 7.3281\n",
      "Epoch [28/100], Step [2000/13958], Loss: 10.5782\n",
      "Epoch [28/100], Step [2100/13958], Loss: 7.0001\n",
      "Epoch [28/100], Step [2200/13958], Loss: 9.2500\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [29/100], Step [100/13958], Loss: 9.5782\n",
      "Epoch [29/100], Step [200/13958], Loss: 4.5940\n",
      "Epoch [29/100], Step [300/13958], Loss: 8.5001\n",
      "Epoch [29/100], Step [400/13958], Loss: 7.5625\n",
      "Epoch [29/100], Step [500/13958], Loss: 9.5156\n",
      "Epoch [29/100], Step [600/13958], Loss: 4.8908\n",
      "Epoch [29/100], Step [700/13958], Loss: 10.3907\n",
      "Epoch [29/100], Step [800/13958], Loss: 9.5938\n",
      "Epoch [29/100], Step [900/13958], Loss: 8.2500\n",
      "Epoch [29/100], Step [1000/13958], Loss: 8.5626\n",
      "Epoch [29/100], Step [1100/13958], Loss: 7.9688\n",
      "Epoch [29/100], Step [1200/13958], Loss: 7.1406\n",
      "Epoch [29/100], Step [1300/13958], Loss: 6.8750\n",
      "Epoch [29/100], Step [1400/13958], Loss: 8.8125\n",
      "Epoch [29/100], Step [1500/13958], Loss: 7.8125\n",
      "Epoch [29/100], Step [1600/13958], Loss: 4.3596\n",
      "Epoch [29/100], Step [1700/13958], Loss: 6.8594\n",
      "Epoch [29/100], Step [1800/13958], Loss: 6.9376\n",
      "Epoch [29/100], Step [1900/13958], Loss: 6.5156\n",
      "Epoch [29/100], Step [2000/13958], Loss: 9.0469\n",
      "Epoch [29/100], Step [2100/13958], Loss: 7.7969\n",
      "Epoch [29/100], Step [2200/13958], Loss: 7.6719\n",
      "Epoch [29/100], Step [2300/13958], Loss: 6.6563\n",
      "Epoch [29/100], Step [2400/13958], Loss: 9.1876\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [30/100], Step [100/13958], Loss: 6.9063\n",
      "Epoch [30/100], Step [200/13958], Loss: 6.5001\n",
      "Epoch [30/100], Step [300/13958], Loss: 6.2032\n",
      "Epoch [30/100], Step [400/13958], Loss: 6.8594\n",
      "Epoch [30/100], Step [500/13958], Loss: 9.2501\n",
      "Epoch [30/100], Step [600/13958], Loss: 6.6252\n",
      "Epoch [30/100], Step [700/13958], Loss: 6.8438\n",
      "Epoch [30/100], Step [800/13958], Loss: 5.5625\n",
      "Epoch [30/100], Step [900/13958], Loss: 5.4845\n",
      "Epoch [30/100], Step [1000/13958], Loss: 7.3907\n",
      "Epoch [30/100], Step [1100/13958], Loss: 6.2501\n",
      "Epoch [30/100], Step [1200/13958], Loss: 10.2969\n",
      "Epoch [30/100], Step [1300/13958], Loss: 7.5314\n",
      "Epoch [30/100], Step [1400/13958], Loss: 12.2656\n",
      "Epoch [30/100], Step [1500/13958], Loss: 6.4844\n",
      "Epoch [30/100], Step [1600/13958], Loss: 9.8594\n",
      "Epoch [30/100], Step [1700/13958], Loss: 7.6720\n",
      "Epoch [30/100], Step [1800/13958], Loss: 6.3125\n",
      "Epoch [30/100], Step [1900/13958], Loss: 6.3281\n",
      "Epoch [30/100], Step [2000/13958], Loss: 9.6875\n",
      "Epoch [30/100], Step [2100/13958], Loss: 11.3594\n",
      "Epoch [30/100], Step [2200/13958], Loss: 6.5469\n",
      "Epoch [30/100], Step [2300/13958], Loss: 5.4375\n",
      "Epoch [30/100], Step [2400/13958], Loss: 8.7188\n",
      "Epoch [30/100], Step [2500/13958], Loss: 7.3282\n",
      "Epoch [30/100], Step [2600/13958], Loss: 9.2970\n",
      "Epoch [30/100], Step [2700/13958], Loss: 11.2658\n",
      "Epoch [30/100], Step [2800/13958], Loss: 5.3750\n",
      "Epoch [30/100], Step [2900/13958], Loss: 7.6094\n",
      "Epoch [30/100], Step [3000/13958], Loss: 9.0157\n",
      "Epoch [30/100], Step [3100/13958], Loss: 12.1563\n",
      "Epoch [30/100], Step [3200/13958], Loss: 6.9688\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [31/100], Step [100/13958], Loss: 10.6719\n",
      "Epoch [31/100], Step [200/13958], Loss: 7.9219\n",
      "Epoch [31/100], Step [300/13958], Loss: 5.8597\n",
      "Epoch [31/100], Step [400/13958], Loss: 9.4063\n",
      "Epoch [31/100], Step [500/13958], Loss: 8.4532\n",
      "Epoch [31/100], Step [600/13958], Loss: 6.7188\n",
      "Epoch [31/100], Step [700/13958], Loss: 7.0469\n",
      "Epoch [31/100], Step [800/13958], Loss: 8.1094\n",
      "Epoch [31/100], Step [900/13958], Loss: 6.0626\n",
      "Epoch [31/100], Step [1000/13958], Loss: 7.3125\n",
      "Epoch [31/100], Step [1100/13958], Loss: 9.5625\n",
      "Epoch [31/100], Step [1200/13958], Loss: 11.4063\n",
      "Epoch [31/100], Step [1300/13958], Loss: 7.9688\n",
      "Epoch [31/100], Step [1400/13958], Loss: 7.5782\n",
      "Epoch [31/100], Step [1500/13958], Loss: 6.6564\n",
      "Epoch [31/100], Step [1600/13958], Loss: 9.8594\n",
      "Epoch [31/100], Step [1700/13958], Loss: 8.8126\n",
      "Epoch [31/100], Step [1800/13958], Loss: 6.0625\n",
      "Epoch [31/100], Step [1900/13958], Loss: 6.3750\n",
      "Epoch [31/100], Step [2000/13958], Loss: 9.6720\n",
      "Epoch [31/100], Step [2100/13958], Loss: 9.1406\n",
      "Epoch [31/100], Step [2200/13958], Loss: 6.4534\n",
      "Epoch [31/100], Step [2300/13958], Loss: 8.1252\n",
      "Epoch [31/100], Step [2400/13958], Loss: 9.1250\n",
      "Epoch [31/100], Step [2500/13958], Loss: 10.1251\n",
      "Epoch [31/100], Step [2600/13958], Loss: 8.6250\n",
      "Epoch [31/100], Step [2700/13958], Loss: 5.6719\n",
      "Epoch [31/100], Step [2800/13958], Loss: 8.0313\n",
      "Epoch [31/100], Step [2900/13958], Loss: 6.2501\n",
      "Epoch [31/100], Step [3000/13958], Loss: 6.7344\n",
      "Epoch [31/100], Step [3100/13958], Loss: 8.6563\n",
      "Epoch [31/100], Step [3200/13958], Loss: 7.3438\n",
      "Epoch [31/100], Step [3300/13958], Loss: 6.4219\n",
      "Epoch [31/100], Step [3400/13958], Loss: 6.9063\n",
      "Epoch [31/100], Step [3500/13958], Loss: 7.8594\n",
      "Epoch [31/100], Step [3600/13958], Loss: 6.5313\n",
      "Epoch [31/100], Step [3700/13958], Loss: 13.5000\n",
      "Epoch [31/100], Step [3800/13958], Loss: 5.2188\n",
      "Epoch [31/100], Step [3900/13958], Loss: 7.9689\n",
      "Epoch [31/100], Step [4000/13958], Loss: 8.4844\n",
      "Epoch [31/100], Step [4100/13958], Loss: 6.7969\n",
      "Epoch [31/100], Step [4200/13958], Loss: 6.2656\n",
      "Epoch [31/100], Step [4300/13958], Loss: 6.9688\n",
      "Epoch [31/100], Step [4400/13958], Loss: 8.6094\n",
      "Epoch [31/100], Step [4500/13958], Loss: 10.0781\n",
      "Epoch [31/100], Step [4600/13958], Loss: 9.6407\n",
      "Epoch [31/100], Step [4700/13958], Loss: 9.4688\n",
      "Epoch [31/100], Step [4800/13958], Loss: 4.5469\n",
      "Epoch [31/100], Step [4900/13958], Loss: 7.1720\n",
      "Epoch [31/100], Step [5000/13958], Loss: 9.8595\n",
      "Epoch [31/100], Step [5100/13958], Loss: 7.0001\n",
      "Epoch [31/100], Step [5200/13958], Loss: 8.8907\n",
      "Epoch [31/100], Step [5300/13958], Loss: 7.2188\n",
      "Epoch [31/100], Step [5400/13958], Loss: 6.0313\n",
      "Epoch [31/100], Step [5500/13958], Loss: 6.4689\n",
      "Epoch [31/100], Step [5600/13958], Loss: 6.8750\n",
      "Epoch [31/100], Step [5700/13958], Loss: 9.7344\n",
      "Epoch [31/100], Step [5800/13958], Loss: 7.5938\n",
      "Epoch [31/100], Step [5900/13958], Loss: 8.1562\n",
      "Epoch [31/100], Step [6000/13958], Loss: 3.5158\n",
      "Epoch [31/100], Step [6100/13958], Loss: 6.5469\n",
      "Epoch [31/100], Step [6200/13958], Loss: 6.6094\n",
      "Epoch [31/100], Step [6300/13958], Loss: 8.7969\n",
      "Epoch [31/100], Step [6400/13958], Loss: 8.4844\n",
      "Epoch [31/100], Step [6500/13958], Loss: 6.3438\n",
      "Epoch [31/100], Step [6600/13958], Loss: 7.9688\n",
      "Epoch [31/100], Step [6700/13958], Loss: 5.6250\n",
      "Epoch [31/100], Step [6800/13958], Loss: 7.0469\n",
      "Epoch [31/100], Step [6900/13958], Loss: 8.8751\n",
      "Epoch [31/100], Step [7000/13958], Loss: 7.2814\n",
      "Epoch [31/100], Step [7100/13958], Loss: 7.3594\n",
      "Epoch [31/100], Step [7200/13958], Loss: 12.2032\n",
      "Epoch [31/100], Step [7300/13958], Loss: 6.2344\n",
      "Epoch [31/100], Step [7400/13958], Loss: 10.3594\n",
      "Epoch [31/100], Step [7500/13958], Loss: 6.4219\n",
      "Epoch [31/100], Step [7600/13958], Loss: 6.7188\n",
      "Epoch [31/100], Step [7700/13958], Loss: 11.5781\n",
      "Epoch [31/100], Step [7800/13958], Loss: 8.5781\n",
      "Epoch [31/100], Step [7900/13958], Loss: 5.4377\n",
      "Epoch [31/100], Step [8000/13958], Loss: 6.5625\n",
      "Epoch [31/100], Step [8100/13958], Loss: 6.5625\n",
      "Epoch [31/100], Step [8200/13958], Loss: 5.0625\n",
      "Epoch [31/100], Step [8300/13958], Loss: 7.2344\n",
      "Epoch [31/100], Step [8400/13958], Loss: 7.7656\n",
      "Epoch [31/100], Step [8500/13958], Loss: 7.0470\n",
      "Epoch [31/100], Step [8600/13958], Loss: 6.8906\n",
      "Epoch [31/100], Step [8700/13958], Loss: 4.9689\n",
      "Epoch [31/100], Step [8800/13958], Loss: 6.6719\n",
      "Epoch [31/100], Step [8900/13958], Loss: 10.6095\n",
      "Epoch [31/100], Step [9000/13958], Loss: 10.5156\n",
      "Epoch [31/100], Step [9100/13958], Loss: 6.5938\n",
      "Epoch [31/100], Step [9200/13958], Loss: 7.5626\n",
      "Epoch [31/100], Step [9300/13958], Loss: 7.4376\n",
      "Epoch [31/100], Step [9400/13958], Loss: 7.1564\n",
      "Epoch [31/100], Step [9500/13958], Loss: 4.9375\n",
      "Epoch [31/100], Step [9600/13958], Loss: 8.4220\n",
      "Epoch [31/100], Step [9700/13958], Loss: 4.9375\n",
      "Epoch [31/100], Step [9800/13958], Loss: 7.5940\n",
      "Epoch [31/100], Step [9900/13958], Loss: 8.2031\n",
      "Epoch [31/100], Step [10000/13958], Loss: 7.1095\n",
      "Epoch [31/100], Step [10100/13958], Loss: 4.9844\n",
      "Epoch [31/100], Step [10200/13958], Loss: 6.4532\n",
      "Epoch [31/100], Step [10300/13958], Loss: 7.2501\n",
      "Epoch [31/100], Step [10400/13958], Loss: 10.1875\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [32/100], Step [100/13958], Loss: 9.6562\n",
      "Epoch [32/100], Step [200/13958], Loss: 8.7656\n",
      "Epoch [32/100], Step [300/13958], Loss: 7.8125\n",
      "Epoch [32/100], Step [400/13958], Loss: 6.0000\n",
      "Epoch [32/100], Step [500/13958], Loss: 5.7344\n",
      "Epoch [32/100], Step [600/13958], Loss: 9.9845\n",
      "Epoch [32/100], Step [700/13958], Loss: 11.7345\n",
      "Epoch [32/100], Step [800/13958], Loss: 7.9533\n",
      "Epoch [32/100], Step [900/13958], Loss: 7.0469\n",
      "Epoch [32/100], Step [1000/13958], Loss: 6.3438\n",
      "Epoch [32/100], Step [1100/13958], Loss: 5.4532\n",
      "Epoch [32/100], Step [1200/13958], Loss: 7.1875\n",
      "Epoch [32/100], Step [1300/13958], Loss: 7.2345\n",
      "Epoch [32/100], Step [1400/13958], Loss: 11.1094\n",
      "Epoch [32/100], Step [1500/13958], Loss: 5.5938\n",
      "Epoch [32/100], Step [1600/13958], Loss: 6.1407\n",
      "Epoch [32/100], Step [1700/13958], Loss: 5.8438\n",
      "Epoch [32/100], Step [1800/13958], Loss: 10.3125\n",
      "Epoch [32/100], Step [1900/13958], Loss: 8.7188\n",
      "Epoch [32/100], Step [2000/13958], Loss: 6.0471\n",
      "Epoch [32/100], Step [2100/13958], Loss: 5.6407\n",
      "Epoch [32/100], Step [2200/13958], Loss: 7.9688\n",
      "Epoch [32/100], Step [2300/13958], Loss: 8.6250\n",
      "Epoch [32/100], Step [2400/13958], Loss: 5.5313\n",
      "Epoch [32/100], Step [2500/13958], Loss: 10.4844\n",
      "Epoch [32/100], Step [2600/13958], Loss: 5.3910\n",
      "Epoch [32/100], Step [2700/13958], Loss: 5.7500\n",
      "Epoch [32/100], Step [2800/13958], Loss: 8.7188\n",
      "Epoch [32/100], Step [2900/13958], Loss: 7.0000\n",
      "Epoch [32/100], Step [3000/13958], Loss: 8.4063\n",
      "Epoch [32/100], Step [3100/13958], Loss: 6.7500\n",
      "Epoch [32/100], Step [3200/13958], Loss: 5.4531\n",
      "Epoch [32/100], Step [3300/13958], Loss: 8.6094\n",
      "Epoch [32/100], Step [3400/13958], Loss: 6.2188\n",
      "Epoch [32/100], Step [3500/13958], Loss: 9.9689\n",
      "Epoch [32/100], Step [3600/13958], Loss: 7.0782\n",
      "Epoch [32/100], Step [3700/13958], Loss: 5.3594\n",
      "Epoch [32/100], Step [3800/13958], Loss: 6.3906\n",
      "Epoch [32/100], Step [3900/13958], Loss: 8.6564\n",
      "Epoch [32/100], Step [4000/13958], Loss: 8.7969\n",
      "Epoch [32/100], Step [4100/13958], Loss: 4.7969\n",
      "Epoch [32/100], Step [4200/13958], Loss: 8.0938\n",
      "Epoch [32/100], Step [4300/13958], Loss: 10.0314\n",
      "Epoch [32/100], Step [4400/13958], Loss: 7.0000\n",
      "Epoch [32/100], Step [4500/13958], Loss: 10.1407\n",
      "Epoch [32/100], Step [4600/13958], Loss: 7.2657\n",
      "Epoch [32/100], Step [4700/13958], Loss: 8.8595\n",
      "Epoch [32/100], Step [4800/13958], Loss: 6.0939\n",
      "Epoch [32/100], Step [4900/13958], Loss: 9.4219\n",
      "Epoch [32/100], Step [5000/13958], Loss: 5.2656\n",
      "Epoch [32/100], Step [5100/13958], Loss: 5.1408\n",
      "Epoch [32/100], Step [5200/13958], Loss: 6.0938\n",
      "Epoch [32/100], Step [5300/13958], Loss: 7.9690\n",
      "Epoch [32/100], Step [5400/13958], Loss: 5.9221\n",
      "Epoch [32/100], Step [5500/13958], Loss: 7.0313\n",
      "Epoch [32/100], Step [5600/13958], Loss: 7.0781\n",
      "Epoch [32/100], Step [5700/13958], Loss: 6.7031\n",
      "Epoch [32/100], Step [5800/13958], Loss: 7.3750\n",
      "Epoch [32/100], Step [5900/13958], Loss: 5.1563\n",
      "Epoch [32/100], Step [6000/13958], Loss: 7.9688\n",
      "Epoch [32/100], Step [6100/13958], Loss: 9.5626\n",
      "Epoch [32/100], Step [6200/13958], Loss: 6.4531\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [33/100], Step [100/13958], Loss: 9.4219\n",
      "Epoch [33/100], Step [200/13958], Loss: 8.1094\n",
      "Epoch [33/100], Step [300/13958], Loss: 10.1407\n",
      "Epoch [33/100], Step [400/13958], Loss: 9.6251\n",
      "Epoch [33/100], Step [500/13958], Loss: 6.1094\n",
      "Epoch [33/100], Step [600/13958], Loss: 5.0313\n",
      "Epoch [33/100], Step [700/13958], Loss: 7.7970\n",
      "Epoch [33/100], Step [800/13958], Loss: 7.5469\n",
      "Epoch [33/100], Step [900/13958], Loss: 9.0625\n",
      "Epoch [33/100], Step [1000/13958], Loss: 7.5000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [34/100], Step [100/13958], Loss: 9.1250\n",
      "Epoch [34/100], Step [200/13958], Loss: 9.3906\n",
      "Epoch [34/100], Step [300/13958], Loss: 3.2813\n",
      "Epoch [34/100], Step [400/13958], Loss: 7.7656\n",
      "Epoch [34/100], Step [500/13958], Loss: 9.5625\n",
      "Epoch [34/100], Step [600/13958], Loss: 8.1250\n",
      "Epoch [34/100], Step [700/13958], Loss: 10.8751\n",
      "Epoch [34/100], Step [800/13958], Loss: 7.4064\n",
      "Epoch [34/100], Step [900/13958], Loss: 10.1875\n",
      "Epoch [34/100], Step [1000/13958], Loss: 7.9376\n",
      "Epoch [34/100], Step [1100/13958], Loss: 6.6876\n",
      "Epoch [34/100], Step [1200/13958], Loss: 6.6251\n",
      "Epoch [34/100], Step [1300/13958], Loss: 9.1406\n",
      "Epoch [34/100], Step [1400/13958], Loss: 9.3126\n",
      "Epoch [34/100], Step [1500/13958], Loss: 7.2814\n",
      "Epoch [34/100], Step [1600/13958], Loss: 5.4845\n",
      "Epoch [34/100], Step [1700/13958], Loss: 5.8125\n",
      "Epoch [34/100], Step [1800/13958], Loss: 7.3438\n",
      "Epoch [34/100], Step [1900/13958], Loss: 7.6719\n",
      "Epoch [34/100], Step [2000/13958], Loss: 10.5469\n",
      "Epoch [34/100], Step [2100/13958], Loss: 6.7501\n",
      "Epoch [34/100], Step [2200/13958], Loss: 9.7970\n",
      "Epoch [34/100], Step [2300/13958], Loss: 6.2500\n",
      "Epoch [34/100], Step [2400/13958], Loss: 10.8751\n",
      "Epoch [34/100], Step [2500/13958], Loss: 10.9063\n",
      "Epoch [34/100], Step [2600/13958], Loss: 6.3594\n",
      "Epoch [34/100], Step [2700/13958], Loss: 5.5939\n",
      "Epoch [34/100], Step [2800/13958], Loss: 6.1563\n",
      "Epoch [34/100], Step [2900/13958], Loss: 7.4688\n",
      "Epoch [34/100], Step [3000/13958], Loss: 5.2032\n",
      "Epoch [34/100], Step [3100/13958], Loss: 5.5470\n",
      "Epoch [34/100], Step [3200/13958], Loss: 7.4844\n",
      "Epoch [34/100], Step [3300/13958], Loss: 9.3596\n",
      "Epoch [34/100], Step [3400/13958], Loss: 4.6251\n",
      "Epoch [34/100], Step [3500/13958], Loss: 8.6875\n",
      "Epoch [34/100], Step [3600/13958], Loss: 7.0469\n",
      "Epoch [34/100], Step [3700/13958], Loss: 6.5158\n",
      "Epoch [34/100], Step [3800/13958], Loss: 7.5626\n",
      "Epoch [34/100], Step [3900/13958], Loss: 9.7189\n",
      "Epoch [34/100], Step [4000/13958], Loss: 9.4220\n",
      "Epoch [34/100], Step [4100/13958], Loss: 5.7813\n",
      "Epoch [34/100], Step [4200/13958], Loss: 8.0000\n",
      "Epoch [34/100], Step [4300/13958], Loss: 6.9221\n",
      "Epoch [34/100], Step [4400/13958], Loss: 11.0157\n",
      "Epoch [34/100], Step [4500/13958], Loss: 6.9375\n",
      "Epoch [34/100], Step [4600/13958], Loss: 6.7656\n",
      "Epoch [34/100], Step [4700/13958], Loss: 7.1563\n",
      "Epoch [34/100], Step [4800/13958], Loss: 8.8750\n",
      "Epoch [34/100], Step [4900/13958], Loss: 7.2813\n",
      "Epoch [34/100], Step [5000/13958], Loss: 6.9219\n",
      "Epoch [34/100], Step [5100/13958], Loss: 6.6406\n",
      "Epoch [34/100], Step [5200/13958], Loss: 9.6251\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [35/100], Step [100/13958], Loss: 7.4532\n",
      "Epoch [35/100], Step [200/13958], Loss: 8.4532\n",
      "Epoch [35/100], Step [300/13958], Loss: 7.9531\n",
      "Epoch [35/100], Step [400/13958], Loss: 13.7656\n",
      "Epoch [35/100], Step [500/13958], Loss: 6.5938\n",
      "Epoch [35/100], Step [600/13958], Loss: 6.7031\n",
      "Epoch [35/100], Step [700/13958], Loss: 5.5313\n",
      "Epoch [35/100], Step [800/13958], Loss: 8.5313\n",
      "Epoch [35/100], Step [900/13958], Loss: 8.1876\n",
      "Epoch [35/100], Step [1000/13958], Loss: 4.8281\n",
      "Epoch [35/100], Step [1100/13958], Loss: 5.5470\n",
      "Epoch [35/100], Step [1200/13958], Loss: 7.8281\n",
      "Epoch [35/100], Step [1300/13958], Loss: 10.1250\n",
      "Epoch [35/100], Step [1400/13958], Loss: 5.1876\n",
      "Epoch [35/100], Step [1500/13958], Loss: 5.4065\n",
      "Epoch [35/100], Step [1600/13958], Loss: 7.7657\n",
      "Epoch [35/100], Step [1700/13958], Loss: 6.4063\n",
      "Epoch [35/100], Step [1800/13958], Loss: 5.0625\n",
      "Epoch [35/100], Step [1900/13958], Loss: 9.5001\n",
      "Epoch [35/100], Step [2000/13958], Loss: 6.7500\n",
      "Epoch [35/100], Step [2100/13958], Loss: 9.4375\n",
      "Epoch [35/100], Step [2200/13958], Loss: 6.8752\n",
      "Epoch [35/100], Step [2300/13958], Loss: 8.5625\n",
      "Epoch [35/100], Step [2400/13958], Loss: 7.2969\n",
      "Epoch [35/100], Step [2500/13958], Loss: 5.1250\n",
      "Epoch [35/100], Step [2600/13958], Loss: 8.7814\n",
      "Epoch [35/100], Step [2700/13958], Loss: 9.1563\n",
      "Epoch [35/100], Step [2800/13958], Loss: 9.5781\n",
      "Epoch [35/100], Step [2900/13958], Loss: 9.7500\n",
      "Epoch [35/100], Step [3000/13958], Loss: 6.3750\n",
      "Epoch [35/100], Step [3100/13958], Loss: 10.9688\n",
      "Epoch [35/100], Step [3200/13958], Loss: 8.2188\n",
      "Epoch [35/100], Step [3300/13958], Loss: 4.2344\n",
      "Epoch [35/100], Step [3400/13958], Loss: 5.9688\n",
      "Epoch [35/100], Step [3500/13958], Loss: 4.7813\n",
      "Epoch [35/100], Step [3600/13958], Loss: 7.9064\n",
      "Epoch [35/100], Step [3700/13958], Loss: 6.0001\n",
      "Epoch [35/100], Step [3800/13958], Loss: 7.2969\n",
      "Epoch [35/100], Step [3900/13958], Loss: 5.5782\n",
      "Epoch [35/100], Step [4000/13958], Loss: 6.0940\n",
      "Epoch [35/100], Step [4100/13958], Loss: 9.4375\n",
      "Epoch [35/100], Step [4200/13958], Loss: 6.1407\n",
      "Epoch [35/100], Step [4300/13958], Loss: 9.1407\n",
      "Epoch [35/100], Step [4400/13958], Loss: 6.8595\n",
      "Epoch [35/100], Step [4500/13958], Loss: 7.0627\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [36/100], Step [100/13958], Loss: 7.7500\n",
      "Epoch [36/100], Step [200/13958], Loss: 7.3750\n",
      "Epoch [36/100], Step [300/13958], Loss: 8.5157\n",
      "Epoch [36/100], Step [400/13958], Loss: 7.4688\n",
      "Epoch [36/100], Step [500/13958], Loss: 6.7969\n",
      "Epoch [36/100], Step [600/13958], Loss: 6.5938\n",
      "Epoch [36/100], Step [700/13958], Loss: 8.5469\n",
      "Epoch [36/100], Step [800/13958], Loss: 9.5156\n",
      "Epoch [36/100], Step [900/13958], Loss: 7.8594\n",
      "Epoch [36/100], Step [1000/13958], Loss: 10.2344\n",
      "Epoch [36/100], Step [1100/13958], Loss: 9.5001\n",
      "Epoch [36/100], Step [1200/13958], Loss: 9.7188\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [37/100], Step [100/13958], Loss: 5.8750\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [38/100], Step [100/13958], Loss: 7.7500\n",
      "Epoch [38/100], Step [200/13958], Loss: 5.4375\n",
      "Epoch [38/100], Step [300/13958], Loss: 6.6251\n",
      "Epoch [38/100], Step [400/13958], Loss: 9.8125\n",
      "Epoch [38/100], Step [500/13958], Loss: 9.4845\n",
      "Epoch [38/100], Step [600/13958], Loss: 6.6719\n",
      "Epoch [38/100], Step [700/13958], Loss: 7.0469\n",
      "Epoch [38/100], Step [800/13958], Loss: 3.4532\n",
      "Epoch [38/100], Step [900/13958], Loss: 8.5469\n",
      "Epoch [38/100], Step [1000/13958], Loss: 3.4688\n",
      "Epoch [38/100], Step [1100/13958], Loss: 6.3438\n",
      "Epoch [38/100], Step [1200/13958], Loss: 7.1252\n",
      "Epoch [38/100], Step [1300/13958], Loss: 5.7031\n",
      "Epoch [38/100], Step [1400/13958], Loss: 6.3282\n",
      "Epoch [38/100], Step [1500/13958], Loss: 8.3439\n",
      "Epoch [38/100], Step [1600/13958], Loss: 8.8595\n",
      "Epoch [38/100], Step [1700/13958], Loss: 9.7501\n",
      "Epoch [38/100], Step [1800/13958], Loss: 9.9376\n",
      "Epoch [38/100], Step [1900/13958], Loss: 6.7344\n",
      "Epoch [38/100], Step [2000/13958], Loss: 4.2969\n",
      "Epoch [38/100], Step [2100/13958], Loss: 7.0313\n",
      "Epoch [38/100], Step [2200/13958], Loss: 7.4532\n",
      "Epoch [38/100], Step [2300/13958], Loss: 8.8438\n",
      "Epoch [38/100], Step [2400/13958], Loss: 6.3125\n",
      "Epoch [38/100], Step [2500/13958], Loss: 11.0938\n",
      "Epoch [38/100], Step [2600/13958], Loss: 7.1719\n",
      "Epoch [38/100], Step [2700/13958], Loss: 10.1563\n",
      "Epoch [38/100], Step [2800/13958], Loss: 8.9688\n",
      "Epoch [38/100], Step [2900/13958], Loss: 8.7500\n",
      "Epoch [38/100], Step [3000/13958], Loss: 8.7032\n",
      "Epoch [38/100], Step [3100/13958], Loss: 9.0625\n",
      "Epoch [38/100], Step [3200/13958], Loss: 11.5782\n",
      "Epoch [38/100], Step [3300/13958], Loss: 6.8438\n",
      "Epoch [38/100], Step [3400/13958], Loss: 9.2188\n",
      "Epoch [38/100], Step [3500/13958], Loss: 6.9844\n",
      "Epoch [38/100], Step [3600/13958], Loss: 8.7813\n",
      "Epoch [38/100], Step [3700/13958], Loss: 6.1250\n",
      "Epoch [38/100], Step [3800/13958], Loss: 4.3125\n",
      "Epoch [38/100], Step [3900/13958], Loss: 7.6406\n",
      "Epoch [38/100], Step [4000/13958], Loss: 7.8594\n",
      "Epoch [38/100], Step [4100/13958], Loss: 8.0000\n",
      "Epoch [38/100], Step [4200/13958], Loss: 9.3438\n",
      "Epoch [38/100], Step [4300/13958], Loss: 9.0626\n",
      "Epoch [38/100], Step [4400/13958], Loss: 7.3438\n",
      "Epoch [38/100], Step [4500/13958], Loss: 11.3282\n",
      "Epoch [38/100], Step [4600/13958], Loss: 6.3438\n",
      "Epoch [38/100], Step [4700/13958], Loss: 10.9063\n",
      "Epoch [38/100], Step [4800/13958], Loss: 7.0158\n",
      "Epoch [38/100], Step [4900/13958], Loss: 8.3282\n",
      "Epoch [38/100], Step [5000/13958], Loss: 12.8438\n",
      "Epoch [38/100], Step [5100/13958], Loss: 7.6563\n",
      "Epoch [38/100], Step [5200/13958], Loss: 7.6719\n",
      "Epoch [38/100], Step [5300/13958], Loss: 5.8281\n",
      "Epoch [38/100], Step [5400/13958], Loss: 6.5000\n",
      "Epoch [38/100], Step [5500/13958], Loss: 10.0313\n",
      "Epoch [38/100], Step [5600/13958], Loss: 5.3751\n",
      "Epoch [38/100], Step [5700/13958], Loss: 11.0781\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [39/100], Step [100/13958], Loss: 7.6406\n",
      "Epoch [39/100], Step [200/13958], Loss: 9.3282\n",
      "Epoch [39/100], Step [300/13958], Loss: 7.5314\n",
      "Epoch [39/100], Step [400/13958], Loss: 10.5782\n",
      "Epoch [39/100], Step [500/13958], Loss: 8.4219\n",
      "Epoch [39/100], Step [600/13958], Loss: 5.9845\n",
      "Epoch [39/100], Step [700/13958], Loss: 5.8438\n",
      "Epoch [39/100], Step [800/13958], Loss: 9.6719\n",
      "Epoch [39/100], Step [900/13958], Loss: 5.3750\n",
      "Epoch [39/100], Step [1000/13958], Loss: 6.6876\n",
      "Epoch [39/100], Step [1100/13958], Loss: 6.1406\n",
      "Epoch [39/100], Step [1200/13958], Loss: 8.0626\n",
      "Epoch [39/100], Step [1300/13958], Loss: 3.7032\n",
      "Epoch [39/100], Step [1400/13958], Loss: 7.7501\n",
      "Epoch [39/100], Step [1500/13958], Loss: 7.8438\n",
      "Epoch [39/100], Step [1600/13958], Loss: 4.1719\n",
      "Epoch [39/100], Step [1700/13958], Loss: 6.0782\n",
      "Epoch [39/100], Step [1800/13958], Loss: 8.4531\n",
      "Epoch [39/100], Step [1900/13958], Loss: 10.4844\n",
      "Epoch [39/100], Step [2000/13958], Loss: 7.8125\n",
      "Epoch [39/100], Step [2100/13958], Loss: 9.0625\n",
      "Epoch [39/100], Step [2200/13958], Loss: 8.1563\n",
      "Epoch [39/100], Step [2300/13958], Loss: 7.0938\n",
      "Epoch [39/100], Step [2400/13958], Loss: 10.0313\n",
      "Epoch [39/100], Step [2500/13958], Loss: 6.1720\n",
      "Epoch [39/100], Step [2600/13958], Loss: 5.6250\n",
      "Epoch [39/100], Step [2700/13958], Loss: 7.7501\n",
      "Epoch [39/100], Step [2800/13958], Loss: 7.7188\n",
      "Epoch [39/100], Step [2900/13958], Loss: 6.5157\n",
      "Epoch [39/100], Step [3000/13958], Loss: 4.6250\n",
      "Epoch [39/100], Step [3100/13958], Loss: 8.1250\n",
      "Epoch [39/100], Step [3200/13958], Loss: 7.1094\n",
      "Epoch [39/100], Step [3300/13958], Loss: 4.9688\n",
      "Epoch [39/100], Step [3400/13958], Loss: 5.6719\n",
      "Epoch [39/100], Step [3500/13958], Loss: 6.1876\n",
      "Epoch [39/100], Step [3600/13958], Loss: 8.7969\n",
      "Epoch [39/100], Step [3700/13958], Loss: 6.9532\n",
      "Epoch [39/100], Step [3800/13958], Loss: 10.1563\n",
      "Epoch [39/100], Step [3900/13958], Loss: 7.0781\n",
      "Epoch [39/100], Step [4000/13958], Loss: 4.4219\n",
      "Epoch [39/100], Step [4100/13958], Loss: 6.2345\n",
      "Epoch [39/100], Step [4200/13958], Loss: 9.7970\n",
      "Epoch [39/100], Step [4300/13958], Loss: 10.0000\n",
      "Epoch [39/100], Step [4400/13958], Loss: 8.5781\n",
      "Epoch [39/100], Step [4500/13958], Loss: 7.8125\n",
      "Epoch [39/100], Step [4600/13958], Loss: 5.0157\n",
      "Epoch [39/100], Step [4700/13958], Loss: 7.1406\n",
      "Epoch [39/100], Step [4800/13958], Loss: 8.6095\n",
      "Epoch [39/100], Step [4900/13958], Loss: 9.8125\n",
      "Epoch [39/100], Step [5000/13958], Loss: 7.3281\n",
      "Epoch [39/100], Step [5100/13958], Loss: 8.2501\n",
      "Epoch [39/100], Step [5200/13958], Loss: 4.5002\n",
      "Epoch [39/100], Step [5300/13958], Loss: 7.1720\n",
      "Epoch [39/100], Step [5400/13958], Loss: 9.6095\n",
      "Epoch [39/100], Step [5500/13958], Loss: 6.5625\n",
      "Epoch [39/100], Step [5600/13958], Loss: 9.1563\n",
      "Epoch [39/100], Step [5700/13958], Loss: 5.4845\n",
      "Epoch [39/100], Step [5800/13958], Loss: 8.3127\n",
      "Epoch [39/100], Step [5900/13958], Loss: 8.7344\n",
      "Epoch [39/100], Step [6000/13958], Loss: 12.5469\n",
      "Epoch [39/100], Step [6100/13958], Loss: 7.1251\n",
      "Epoch [39/100], Step [6200/13958], Loss: 6.5782\n",
      "Epoch [39/100], Step [6300/13958], Loss: 6.5782\n",
      "Epoch [39/100], Step [6400/13958], Loss: 9.4063\n",
      "Epoch [39/100], Step [6500/13958], Loss: 4.9690\n",
      "Epoch [39/100], Step [6600/13958], Loss: 6.4532\n",
      "Epoch [39/100], Step [6700/13958], Loss: 6.9688\n",
      "Epoch [39/100], Step [6800/13958], Loss: 7.0000\n",
      "Epoch [39/100], Step [6900/13958], Loss: 10.4688\n",
      "Epoch [39/100], Step [7000/13958], Loss: 8.0470\n",
      "Epoch [39/100], Step [7100/13958], Loss: 6.6406\n",
      "Epoch [39/100], Step [7200/13958], Loss: 4.4688\n",
      "Epoch [39/100], Step [7300/13958], Loss: 7.8282\n",
      "Epoch [39/100], Step [7400/13958], Loss: 8.0938\n",
      "Epoch [39/100], Step [7500/13958], Loss: 8.5156\n",
      "Epoch [39/100], Step [7600/13958], Loss: 6.1406\n",
      "Epoch [39/100], Step [7700/13958], Loss: 9.2031\n",
      "Epoch [39/100], Step [7800/13958], Loss: 10.2656\n",
      "Epoch [39/100], Step [7900/13958], Loss: 4.4063\n",
      "Epoch [39/100], Step [8000/13958], Loss: 8.3594\n",
      "Epoch [39/100], Step [8100/13958], Loss: 7.3439\n",
      "Epoch [39/100], Step [8200/13958], Loss: 7.1407\n",
      "Epoch [39/100], Step [8300/13958], Loss: 5.9063\n",
      "Epoch [39/100], Step [8400/13958], Loss: 9.4688\n",
      "Epoch [39/100], Step [8500/13958], Loss: 11.0000\n",
      "Epoch [39/100], Step [8600/13958], Loss: 8.5315\n",
      "Epoch [39/100], Step [8700/13958], Loss: 5.9532\n",
      "Epoch [39/100], Step [8800/13958], Loss: 7.9688\n",
      "Epoch [39/100], Step [8900/13958], Loss: 5.6876\n",
      "Epoch [39/100], Step [9000/13958], Loss: 9.2813\n",
      "Epoch [39/100], Step [9100/13958], Loss: 4.1563\n",
      "Epoch [39/100], Step [9200/13958], Loss: 8.0313\n",
      "Epoch [39/100], Step [9300/13958], Loss: 6.7970\n",
      "Epoch [39/100], Step [9400/13958], Loss: 5.0782\n",
      "Epoch [39/100], Step [9500/13958], Loss: 4.7344\n",
      "Epoch [39/100], Step [9600/13958], Loss: 5.6251\n",
      "Epoch [39/100], Step [9700/13958], Loss: 7.2969\n",
      "Epoch [39/100], Step [9800/13958], Loss: 8.7813\n",
      "Epoch [39/100], Step [9900/13958], Loss: 6.1719\n",
      "Epoch [39/100], Step [10000/13958], Loss: 8.8594\n",
      "Epoch [39/100], Step [10100/13958], Loss: 8.4688\n",
      "Epoch [39/100], Step [10200/13958], Loss: 6.1875\n",
      "Epoch [39/100], Step [10300/13958], Loss: 5.8751\n",
      "Epoch [39/100], Step [10400/13958], Loss: 10.0000\n",
      "Epoch [39/100], Step [10500/13958], Loss: 7.0000\n",
      "Epoch [39/100], Step [10600/13958], Loss: 7.2813\n",
      "Epoch [39/100], Step [10700/13958], Loss: 9.1250\n",
      "Epoch [39/100], Step [10800/13958], Loss: 7.7813\n",
      "Epoch [39/100], Step [10900/13958], Loss: 5.7032\n",
      "Epoch [39/100], Step [11000/13958], Loss: 7.4064\n",
      "Epoch [39/100], Step [11100/13958], Loss: 9.5157\n",
      "Epoch [39/100], Step [11200/13958], Loss: 7.2813\n",
      "Epoch [39/100], Step [11300/13958], Loss: 7.7344\n",
      "Epoch [39/100], Step [11400/13958], Loss: 8.8125\n",
      "Epoch [39/100], Step [11500/13958], Loss: 6.7656\n",
      "Epoch [39/100], Step [11600/13958], Loss: 7.1721\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [40/100], Step [100/13958], Loss: 6.5781\n",
      "Epoch [40/100], Step [200/13958], Loss: 5.0157\n",
      "Epoch [40/100], Step [300/13958], Loss: 8.4688\n",
      "Epoch [40/100], Step [400/13958], Loss: 7.1406\n",
      "Epoch [40/100], Step [500/13958], Loss: 9.4532\n",
      "Epoch [40/100], Step [600/13958], Loss: 8.6094\n",
      "Epoch [40/100], Step [700/13958], Loss: 7.0938\n",
      "Epoch [40/100], Step [800/13958], Loss: 7.7189\n",
      "Epoch [40/100], Step [900/13958], Loss: 6.2656\n",
      "Epoch [40/100], Step [1000/13958], Loss: 6.6094\n",
      "Epoch [40/100], Step [1100/13958], Loss: 6.6719\n",
      "Epoch [40/100], Step [1200/13958], Loss: 6.7189\n",
      "Epoch [40/100], Step [1300/13958], Loss: 7.6876\n",
      "Epoch [40/100], Step [1400/13958], Loss: 6.3127\n",
      "Epoch [40/100], Step [1500/13958], Loss: 9.3907\n",
      "Epoch [40/100], Step [1600/13958], Loss: 8.3439\n",
      "Epoch [40/100], Step [1700/13958], Loss: 9.8750\n",
      "Epoch [40/100], Step [1800/13958], Loss: 5.1876\n",
      "Epoch [40/100], Step [1900/13958], Loss: 8.7969\n",
      "Epoch [40/100], Step [2000/13958], Loss: 7.1563\n",
      "Epoch [40/100], Step [2100/13958], Loss: 6.7815\n",
      "Epoch [40/100], Step [2200/13958], Loss: 6.0938\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [41/100], Step [100/13958], Loss: 8.0000\n",
      "Epoch [41/100], Step [200/13958], Loss: 6.0313\n",
      "Epoch [41/100], Step [300/13958], Loss: 5.5626\n",
      "Epoch [41/100], Step [400/13958], Loss: 5.5470\n",
      "Epoch [41/100], Step [500/13958], Loss: 4.7969\n",
      "Epoch [41/100], Step [600/13958], Loss: 7.5783\n",
      "Epoch [41/100], Step [700/13958], Loss: 6.2656\n",
      "Epoch [41/100], Step [800/13958], Loss: 6.8750\n",
      "Epoch [41/100], Step [900/13958], Loss: 8.5156\n",
      "Epoch [41/100], Step [1000/13958], Loss: 8.5781\n",
      "Epoch [41/100], Step [1100/13958], Loss: 6.8594\n",
      "Epoch [41/100], Step [1200/13958], Loss: 8.4063\n",
      "Epoch [41/100], Step [1300/13958], Loss: 8.4844\n",
      "Epoch [41/100], Step [1400/13958], Loss: 6.2970\n",
      "Epoch [41/100], Step [1500/13958], Loss: 6.9687\n",
      "Epoch [41/100], Step [1600/13958], Loss: 6.8125\n",
      "Epoch [41/100], Step [1700/13958], Loss: 6.1094\n",
      "Epoch [41/100], Step [1800/13958], Loss: 5.6407\n",
      "Epoch [41/100], Step [1900/13958], Loss: 9.1094\n",
      "Epoch [41/100], Step [2000/13958], Loss: 6.3126\n",
      "Epoch [41/100], Step [2100/13958], Loss: 11.0625\n",
      "Epoch [41/100], Step [2200/13958], Loss: 8.7813\n",
      "Epoch [41/100], Step [2300/13958], Loss: 8.5625\n",
      "Epoch [41/100], Step [2400/13958], Loss: 9.4063\n",
      "Epoch [41/100], Step [2500/13958], Loss: 6.7033\n",
      "Epoch [41/100], Step [2600/13958], Loss: 6.7500\n",
      "Epoch [41/100], Step [2700/13958], Loss: 9.3282\n",
      "Epoch [41/100], Step [2800/13958], Loss: 7.9221\n",
      "Epoch [41/100], Step [2900/13958], Loss: 7.4375\n",
      "Epoch [41/100], Step [3000/13958], Loss: 8.9220\n",
      "Epoch [41/100], Step [3100/13958], Loss: 7.1719\n",
      "Epoch [41/100], Step [3200/13958], Loss: 5.5157\n",
      "Epoch [41/100], Step [3300/13958], Loss: 12.0625\n",
      "Epoch [41/100], Step [3400/13958], Loss: 7.3281\n",
      "Epoch [41/100], Step [3500/13958], Loss: 8.1094\n",
      "Epoch [41/100], Step [3600/13958], Loss: 3.7032\n",
      "Epoch [41/100], Step [3700/13958], Loss: 8.9531\n",
      "Epoch [41/100], Step [3800/13958], Loss: 8.9531\n",
      "Epoch [41/100], Step [3900/13958], Loss: 8.2500\n",
      "Epoch [41/100], Step [4000/13958], Loss: 9.1719\n",
      "Epoch [41/100], Step [4100/13958], Loss: 8.6719\n",
      "Epoch [41/100], Step [4200/13958], Loss: 5.5000\n",
      "Epoch [41/100], Step [4300/13958], Loss: 6.1094\n",
      "Epoch [41/100], Step [4400/13958], Loss: 7.3907\n",
      "Epoch [41/100], Step [4500/13958], Loss: 10.5781\n",
      "Epoch [41/100], Step [4600/13958], Loss: 7.0626\n",
      "Epoch [41/100], Step [4700/13958], Loss: 9.7969\n",
      "Epoch [41/100], Step [4800/13958], Loss: 8.9377\n",
      "Epoch [41/100], Step [4900/13958], Loss: 5.8907\n",
      "Epoch [41/100], Step [5000/13958], Loss: 8.2969\n",
      "Epoch [41/100], Step [5100/13958], Loss: 8.7969\n",
      "Epoch [41/100], Step [5200/13958], Loss: 8.0625\n",
      "Epoch [41/100], Step [5300/13958], Loss: 12.5313\n",
      "Epoch [41/100], Step [5400/13958], Loss: 8.5313\n",
      "Epoch [41/100], Step [5500/13958], Loss: 3.2660\n",
      "Epoch [41/100], Step [5600/13958], Loss: 7.6719\n",
      "Epoch [41/100], Step [5700/13958], Loss: 8.5625\n",
      "Epoch [41/100], Step [5800/13958], Loss: 8.6875\n",
      "Epoch [41/100], Step [5900/13958], Loss: 6.7657\n",
      "Epoch [41/100], Step [6000/13958], Loss: 8.5626\n",
      "Epoch [41/100], Step [6100/13958], Loss: 8.2657\n",
      "Epoch [41/100], Step [6200/13958], Loss: 8.1563\n",
      "Epoch [41/100], Step [6300/13958], Loss: 6.8594\n",
      "Epoch [41/100], Step [6400/13958], Loss: 4.7188\n",
      "Epoch [41/100], Step [6500/13958], Loss: 7.0782\n",
      "Epoch [41/100], Step [6600/13958], Loss: 8.5781\n",
      "Epoch [41/100], Step [6700/13958], Loss: 6.8594\n",
      "Epoch [41/100], Step [6800/13958], Loss: 7.7970\n",
      "Epoch [41/100], Step [6900/13958], Loss: 8.0157\n",
      "Epoch [41/100], Step [7000/13958], Loss: 7.5625\n",
      "Epoch [41/100], Step [7100/13958], Loss: 8.0626\n",
      "Epoch [41/100], Step [7200/13958], Loss: 9.1875\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [42/100], Step [100/13958], Loss: 6.7501\n",
      "Epoch [42/100], Step [200/13958], Loss: 6.4688\n",
      "Epoch [42/100], Step [300/13958], Loss: 9.0000\n",
      "Epoch [42/100], Step [400/13958], Loss: 6.4376\n",
      "Epoch [42/100], Step [500/13958], Loss: 10.8595\n",
      "Epoch [42/100], Step [600/13958], Loss: 11.6719\n",
      "Epoch [42/100], Step [700/13958], Loss: 6.0000\n",
      "Epoch [42/100], Step [800/13958], Loss: 9.4376\n",
      "Epoch [42/100], Step [900/13958], Loss: 7.8907\n",
      "Epoch [42/100], Step [1000/13958], Loss: 10.5782\n",
      "Epoch [42/100], Step [1100/13958], Loss: 6.8907\n",
      "Epoch [42/100], Step [1200/13958], Loss: 9.0313\n",
      "Epoch [42/100], Step [1300/13958], Loss: 7.6719\n",
      "Epoch [42/100], Step [1400/13958], Loss: 6.4844\n",
      "Epoch [42/100], Step [1500/13958], Loss: 3.5157\n",
      "Epoch [42/100], Step [1600/13958], Loss: 10.2969\n",
      "Epoch [42/100], Step [1700/13958], Loss: 7.7969\n",
      "Epoch [42/100], Step [1800/13958], Loss: 8.0782\n",
      "Epoch [42/100], Step [1900/13958], Loss: 8.6564\n",
      "Epoch [42/100], Step [2000/13958], Loss: 8.1876\n",
      "Epoch [42/100], Step [2100/13958], Loss: 7.4375\n",
      "Epoch [42/100], Step [2200/13958], Loss: 5.6407\n",
      "Epoch [42/100], Step [2300/13958], Loss: 7.1563\n",
      "Epoch [42/100], Step [2400/13958], Loss: 6.3126\n",
      "Epoch [42/100], Step [2500/13958], Loss: 8.9219\n",
      "Epoch [42/100], Step [2600/13958], Loss: 6.1721\n",
      "Epoch [42/100], Step [2700/13958], Loss: 7.4533\n",
      "Epoch [42/100], Step [2800/13958], Loss: 5.7189\n",
      "Epoch [42/100], Step [2900/13958], Loss: 6.2657\n",
      "Epoch [42/100], Step [3000/13958], Loss: 6.7344\n",
      "Epoch [42/100], Step [3100/13958], Loss: 9.8909\n",
      "Epoch [42/100], Step [3200/13958], Loss: 7.3750\n",
      "Epoch [42/100], Step [3300/13958], Loss: 7.1250\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [43/100], Step [100/13958], Loss: 11.3438\n",
      "Epoch [43/100], Step [200/13958], Loss: 6.8125\n",
      "Epoch [43/100], Step [300/13958], Loss: 9.1407\n",
      "Epoch [43/100], Step [400/13958], Loss: 6.7970\n",
      "Epoch [43/100], Step [500/13958], Loss: 6.9688\n",
      "Epoch [43/100], Step [600/13958], Loss: 7.8282\n",
      "Epoch [43/100], Step [700/13958], Loss: 7.0782\n",
      "Epoch [43/100], Step [800/13958], Loss: 6.7813\n",
      "Epoch [43/100], Step [900/13958], Loss: 6.2813\n",
      "Epoch [43/100], Step [1000/13958], Loss: 7.9532\n",
      "Epoch [43/100], Step [1100/13958], Loss: 6.9532\n",
      "Epoch [43/100], Step [1200/13958], Loss: 6.7656\n",
      "Epoch [43/100], Step [1300/13958], Loss: 7.0000\n",
      "Epoch [43/100], Step [1400/13958], Loss: 6.2031\n",
      "Epoch [43/100], Step [1500/13958], Loss: 6.5001\n",
      "Epoch [43/100], Step [1600/13958], Loss: 10.3125\n",
      "Epoch [43/100], Step [1700/13958], Loss: 7.6875\n",
      "Epoch [43/100], Step [1800/13958], Loss: 8.6250\n",
      "Epoch [43/100], Step [1900/13958], Loss: 8.2344\n",
      "Epoch [43/100], Step [2000/13958], Loss: 6.0938\n",
      "Epoch [43/100], Step [2100/13958], Loss: 9.0781\n",
      "Epoch [43/100], Step [2200/13958], Loss: 5.5469\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [44/100], Step [100/13958], Loss: 6.2344\n",
      "Epoch [44/100], Step [200/13958], Loss: 6.4375\n",
      "Epoch [44/100], Step [300/13958], Loss: 7.5156\n",
      "Epoch [44/100], Step [400/13958], Loss: 11.7344\n",
      "Epoch [44/100], Step [500/13958], Loss: 6.5781\n",
      "Epoch [44/100], Step [600/13958], Loss: 7.6406\n",
      "Epoch [44/100], Step [700/13958], Loss: 5.6407\n",
      "Epoch [44/100], Step [800/13958], Loss: 7.5157\n",
      "Epoch [44/100], Step [900/13958], Loss: 5.7813\n",
      "Epoch [44/100], Step [1000/13958], Loss: 7.9375\n",
      "Epoch [44/100], Step [1100/13958], Loss: 10.6720\n",
      "Epoch [44/100], Step [1200/13958], Loss: 4.4844\n",
      "Epoch [44/100], Step [1300/13958], Loss: 7.7500\n",
      "Epoch [44/100], Step [1400/13958], Loss: 7.2814\n",
      "Epoch [44/100], Step [1500/13958], Loss: 5.8594\n",
      "Epoch [44/100], Step [1600/13958], Loss: 9.3282\n",
      "Epoch [44/100], Step [1700/13958], Loss: 7.1407\n",
      "Epoch [44/100], Step [1800/13958], Loss: 9.6094\n",
      "Epoch [44/100], Step [1900/13958], Loss: 8.3595\n",
      "Epoch [44/100], Step [2000/13958], Loss: 7.4219\n",
      "Epoch [44/100], Step [2100/13958], Loss: 7.4063\n",
      "Epoch [44/100], Step [2200/13958], Loss: 8.2657\n",
      "Epoch [44/100], Step [2300/13958], Loss: 6.8750\n",
      "Epoch [44/100], Step [2400/13958], Loss: 5.5315\n",
      "Epoch [44/100], Step [2500/13958], Loss: 9.2969\n",
      "Epoch [44/100], Step [2600/13958], Loss: 6.4219\n",
      "Epoch [44/100], Step [2700/13958], Loss: 6.2032\n",
      "Epoch [44/100], Step [2800/13958], Loss: 6.3750\n",
      "Epoch [44/100], Step [2900/13958], Loss: 6.8438\n",
      "Epoch [44/100], Step [3000/13958], Loss: 6.7656\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [45/100], Step [100/13958], Loss: 5.3907\n",
      "Epoch [45/100], Step [200/13958], Loss: 7.3907\n",
      "Epoch [45/100], Step [300/13958], Loss: 7.7813\n",
      "Epoch [45/100], Step [400/13958], Loss: 15.0000\n",
      "Epoch [45/100], Step [500/13958], Loss: 7.3907\n",
      "Epoch [45/100], Step [600/13958], Loss: 4.6564\n",
      "Epoch [45/100], Step [700/13958], Loss: 7.3594\n",
      "Epoch [45/100], Step [800/13958], Loss: 7.6563\n",
      "Epoch [45/100], Step [900/13958], Loss: 5.3595\n",
      "Epoch [45/100], Step [1000/13958], Loss: 9.3281\n",
      "Epoch [45/100], Step [1100/13958], Loss: 7.2031\n",
      "Epoch [45/100], Step [1200/13958], Loss: 8.7813\n",
      "Epoch [45/100], Step [1300/13958], Loss: 10.0156\n",
      "Epoch [45/100], Step [1400/13958], Loss: 10.3907\n",
      "Epoch [45/100], Step [1500/13958], Loss: 6.9375\n",
      "Epoch [45/100], Step [1600/13958], Loss: 6.5781\n",
      "Epoch [45/100], Step [1700/13958], Loss: 7.5158\n",
      "Epoch [45/100], Step [1800/13958], Loss: 5.1719\n",
      "Epoch [45/100], Step [1900/13958], Loss: 10.3750\n",
      "Epoch [45/100], Step [2000/13958], Loss: 7.4219\n",
      "Epoch [45/100], Step [2100/13958], Loss: 12.9688\n",
      "Epoch [45/100], Step [2200/13958], Loss: 8.0000\n",
      "Epoch [45/100], Step [2300/13958], Loss: 7.0314\n",
      "Epoch [45/100], Step [2400/13958], Loss: 8.0313\n",
      "Epoch [45/100], Step [2500/13958], Loss: 9.1251\n",
      "Epoch [45/100], Step [2600/13958], Loss: 8.8125\n",
      "Epoch [45/100], Step [2700/13958], Loss: 8.7344\n",
      "Epoch [45/100], Step [2800/13958], Loss: 6.9219\n",
      "Epoch [45/100], Step [2900/13958], Loss: 7.8438\n",
      "Epoch [45/100], Step [3000/13958], Loss: 7.4532\n",
      "Epoch [45/100], Step [3100/13958], Loss: 8.3281\n",
      "Epoch [45/100], Step [3200/13958], Loss: 7.0626\n",
      "Epoch [45/100], Step [3300/13958], Loss: 6.4845\n",
      "Epoch [45/100], Step [3400/13958], Loss: 7.8750\n",
      "Epoch [45/100], Step [3500/13958], Loss: 9.6875\n",
      "Epoch [45/100], Step [3600/13958], Loss: 5.8594\n",
      "Epoch [45/100], Step [3700/13958], Loss: 8.8126\n",
      "Epoch [45/100], Step [3800/13958], Loss: 7.4063\n",
      "Epoch [45/100], Step [3900/13958], Loss: 7.9375\n",
      "Epoch [45/100], Step [4000/13958], Loss: 9.5001\n",
      "Epoch [45/100], Step [4100/13958], Loss: 7.9219\n",
      "Epoch [45/100], Step [4200/13958], Loss: 9.2031\n",
      "Epoch [45/100], Step [4300/13958], Loss: 4.7188\n",
      "Epoch [45/100], Step [4400/13958], Loss: 6.6564\n",
      "Epoch [45/100], Step [4500/13958], Loss: 9.8595\n",
      "Epoch [45/100], Step [4600/13958], Loss: 7.7970\n",
      "Epoch [45/100], Step [4700/13958], Loss: 6.5938\n",
      "Epoch [45/100], Step [4800/13958], Loss: 7.7657\n",
      "Epoch [45/100], Step [4900/13958], Loss: 7.2657\n",
      "Epoch [45/100], Step [5000/13958], Loss: 8.2813\n",
      "Epoch [45/100], Step [5100/13958], Loss: 5.4532\n",
      "Epoch [45/100], Step [5200/13958], Loss: 7.0313\n",
      "Epoch [45/100], Step [5300/13958], Loss: 7.8594\n",
      "Epoch [45/100], Step [5400/13958], Loss: 5.3750\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [46/100], Step [100/13958], Loss: 6.9844\n",
      "Epoch [46/100], Step [200/13958], Loss: 7.3438\n",
      "Epoch [46/100], Step [300/13958], Loss: 10.9688\n",
      "Epoch [46/100], Step [400/13958], Loss: 7.6251\n",
      "Epoch [46/100], Step [500/13958], Loss: 6.5000\n",
      "Epoch [46/100], Step [600/13958], Loss: 8.5312\n",
      "Epoch [46/100], Step [700/13958], Loss: 10.1406\n",
      "Epoch [46/100], Step [800/13958], Loss: 9.0938\n",
      "Epoch [46/100], Step [900/13958], Loss: 10.7031\n",
      "Epoch [46/100], Step [1000/13958], Loss: 7.6719\n",
      "Epoch [46/100], Step [1100/13958], Loss: 7.0469\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [47/100], Step [100/13958], Loss: 8.0157\n",
      "Epoch [47/100], Step [200/13958], Loss: 11.5156\n",
      "Epoch [47/100], Step [300/13958], Loss: 7.4375\n",
      "Epoch [47/100], Step [400/13958], Loss: 10.5469\n",
      "Epoch [47/100], Step [500/13958], Loss: 8.7813\n",
      "Epoch [47/100], Step [600/13958], Loss: 7.0625\n",
      "Epoch [47/100], Step [700/13958], Loss: 7.9375\n",
      "Epoch [47/100], Step [800/13958], Loss: 6.7188\n",
      "Epoch [47/100], Step [900/13958], Loss: 7.4219\n",
      "Epoch [47/100], Step [1000/13958], Loss: 7.0469\n",
      "Epoch [47/100], Step [1100/13958], Loss: 8.5938\n",
      "Epoch [47/100], Step [1200/13958], Loss: 10.5313\n",
      "Epoch [47/100], Step [1300/13958], Loss: 8.9219\n",
      "Epoch [47/100], Step [1400/13958], Loss: 8.8594\n",
      "Epoch [47/100], Step [1500/13958], Loss: 7.7188\n",
      "Epoch [47/100], Step [1600/13958], Loss: 6.3438\n",
      "Epoch [47/100], Step [1700/13958], Loss: 6.8438\n",
      "Epoch [47/100], Step [1800/13958], Loss: 9.4688\n",
      "Epoch [47/100], Step [1900/13958], Loss: 4.8438\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [48/100], Step [100/13958], Loss: 4.9844\n",
      "Epoch [48/100], Step [200/13958], Loss: 5.5469\n",
      "Epoch [48/100], Step [300/13958], Loss: 8.6094\n",
      "Epoch [48/100], Step [400/13958], Loss: 9.7345\n",
      "Epoch [48/100], Step [500/13958], Loss: 9.7344\n",
      "Epoch [48/100], Step [600/13958], Loss: 6.7031\n",
      "Epoch [48/100], Step [700/13958], Loss: 7.1407\n",
      "Epoch [48/100], Step [800/13958], Loss: 5.6094\n",
      "Epoch [48/100], Step [900/13958], Loss: 7.5625\n",
      "Epoch [48/100], Step [1000/13958], Loss: 7.2969\n",
      "Epoch [48/100], Step [1100/13958], Loss: 5.5313\n",
      "Epoch [48/100], Step [1200/13958], Loss: 11.3594\n",
      "Epoch [48/100], Step [1300/13958], Loss: 10.0939\n",
      "Epoch [48/100], Step [1400/13958], Loss: 6.7969\n",
      "Epoch [48/100], Step [1500/13958], Loss: 5.6875\n",
      "Epoch [48/100], Step [1600/13958], Loss: 7.4845\n",
      "Epoch [48/100], Step [1700/13958], Loss: 10.2500\n",
      "Epoch [48/100], Step [1800/13958], Loss: 6.0782\n",
      "Epoch [48/100], Step [1900/13958], Loss: 7.5628\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [49/100], Step [100/13958], Loss: 11.6094\n",
      "Epoch [49/100], Step [200/13958], Loss: 6.3751\n",
      "Epoch [49/100], Step [300/13958], Loss: 10.8281\n",
      "Epoch [49/100], Step [400/13958], Loss: 6.2969\n",
      "Epoch [49/100], Step [500/13958], Loss: 5.7813\n",
      "Epoch [49/100], Step [600/13958], Loss: 10.7189\n",
      "Epoch [49/100], Step [700/13958], Loss: 6.8125\n",
      "Epoch [49/100], Step [800/13958], Loss: 9.5000\n",
      "Epoch [49/100], Step [900/13958], Loss: 6.5156\n",
      "Epoch [49/100], Step [1000/13958], Loss: 7.8751\n",
      "Epoch [49/100], Step [1100/13958], Loss: 5.4377\n",
      "Epoch [49/100], Step [1200/13958], Loss: 6.0782\n",
      "Epoch [49/100], Step [1300/13958], Loss: 9.2970\n",
      "Epoch [49/100], Step [1400/13958], Loss: 6.1720\n",
      "Epoch [49/100], Step [1500/13958], Loss: 5.7188\n",
      "Epoch [49/100], Step [1600/13958], Loss: 7.7657\n",
      "Epoch [49/100], Step [1700/13958], Loss: 8.3907\n",
      "Epoch [49/100], Step [1800/13958], Loss: 7.0939\n",
      "Epoch [49/100], Step [1900/13958], Loss: 14.3594\n",
      "Epoch [49/100], Step [2000/13958], Loss: 7.5469\n",
      "Epoch [49/100], Step [2100/13958], Loss: 8.6875\n",
      "Epoch [49/100], Step [2200/13958], Loss: 7.3125\n",
      "Epoch [49/100], Step [2300/13958], Loss: 10.0938\n",
      "Epoch [49/100], Step [2400/13958], Loss: 9.0313\n",
      "Epoch [49/100], Step [2500/13958], Loss: 4.5000\n",
      "Epoch [49/100], Step [2600/13958], Loss: 7.7344\n",
      "Epoch [49/100], Step [2700/13958], Loss: 6.2344\n",
      "Epoch [49/100], Step [2800/13958], Loss: 8.7970\n",
      "Epoch [49/100], Step [2900/13958], Loss: 9.2656\n",
      "Epoch [49/100], Step [3000/13958], Loss: 5.8751\n",
      "Epoch [49/100], Step [3100/13958], Loss: 10.7657\n",
      "Epoch [49/100], Step [3200/13958], Loss: 5.5625\n",
      "Epoch [49/100], Step [3300/13958], Loss: 9.4689\n",
      "Epoch [49/100], Step [3400/13958], Loss: 5.4532\n",
      "Epoch [49/100], Step [3500/13958], Loss: 8.4375\n",
      "Epoch [49/100], Step [3600/13958], Loss: 6.0938\n",
      "Epoch [49/100], Step [3700/13958], Loss: 7.6094\n",
      "Epoch [49/100], Step [3800/13958], Loss: 8.6875\n",
      "Epoch [49/100], Step [3900/13958], Loss: 4.8594\n",
      "Epoch [49/100], Step [4000/13958], Loss: 10.5001\n",
      "Epoch [49/100], Step [4100/13958], Loss: 5.0469\n",
      "Epoch [49/100], Step [4200/13958], Loss: 9.0002\n",
      "Epoch [49/100], Step [4300/13958], Loss: 7.6406\n",
      "Epoch [49/100], Step [4400/13958], Loss: 6.7189\n",
      "Epoch [49/100], Step [4500/13958], Loss: 5.9063\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [50/100], Step [100/13958], Loss: 7.8281\n",
      "Epoch [50/100], Step [200/13958], Loss: 7.6094\n",
      "Epoch [50/100], Step [300/13958], Loss: 7.2658\n",
      "Epoch [50/100], Step [400/13958], Loss: 5.2501\n",
      "Epoch [50/100], Step [500/13958], Loss: 5.6563\n",
      "Epoch [50/100], Step [600/13958], Loss: 8.9220\n",
      "Epoch [50/100], Step [700/13958], Loss: 5.6875\n",
      "Epoch [50/100], Step [800/13958], Loss: 7.4531\n",
      "Epoch [50/100], Step [900/13958], Loss: 8.5000\n",
      "Epoch [50/100], Step [1000/13958], Loss: 8.5625\n",
      "Epoch [50/100], Step [1100/13958], Loss: 10.8125\n",
      "Epoch [50/100], Step [1200/13958], Loss: 6.0000\n",
      "Epoch [50/100], Step [1300/13958], Loss: 7.7344\n",
      "Epoch [50/100], Step [1400/13958], Loss: 6.5000\n",
      "Epoch [50/100], Step [1500/13958], Loss: 9.4376\n",
      "Epoch [50/100], Step [1600/13958], Loss: 8.0782\n",
      "Epoch [50/100], Step [1700/13958], Loss: 7.8126\n",
      "Epoch [50/100], Step [1800/13958], Loss: 8.8906\n",
      "Epoch [50/100], Step [1900/13958], Loss: 6.2500\n",
      "Epoch [50/100], Step [2000/13958], Loss: 5.3126\n",
      "Epoch [50/100], Step [2100/13958], Loss: 8.3750\n",
      "Epoch [50/100], Step [2200/13958], Loss: 7.9845\n",
      "Epoch [50/100], Step [2300/13958], Loss: 7.7501\n",
      "Epoch [50/100], Step [2400/13958], Loss: 6.2500\n",
      "Epoch [50/100], Step [2500/13958], Loss: 6.2345\n",
      "Epoch [50/100], Step [2600/13958], Loss: 8.7344\n",
      "Epoch [50/100], Step [2700/13958], Loss: 7.6563\n",
      "Epoch [50/100], Step [2800/13958], Loss: 6.8438\n",
      "Epoch [50/100], Step [2900/13958], Loss: 7.6406\n",
      "Epoch [50/100], Step [3000/13958], Loss: 5.1251\n",
      "Epoch [50/100], Step [3100/13958], Loss: 9.4531\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [51/100], Step [100/13958], Loss: 8.1094\n",
      "Epoch [51/100], Step [200/13958], Loss: 10.2344\n",
      "Epoch [51/100], Step [300/13958], Loss: 6.3906\n",
      "Epoch [51/100], Step [400/13958], Loss: 9.5000\n",
      "Epoch [51/100], Step [500/13958], Loss: 9.7500\n",
      "Epoch [51/100], Step [600/13958], Loss: 10.6250\n",
      "Epoch [51/100], Step [700/13958], Loss: 7.3907\n",
      "Epoch [51/100], Step [800/13958], Loss: 7.2188\n",
      "Epoch [51/100], Step [900/13958], Loss: 7.7344\n",
      "Epoch [51/100], Step [1000/13958], Loss: 8.6251\n",
      "Epoch [51/100], Step [1100/13958], Loss: 6.4375\n",
      "Epoch [51/100], Step [1200/13958], Loss: 8.7813\n",
      "Epoch [51/100], Step [1300/13958], Loss: 6.5000\n",
      "Epoch [51/100], Step [1400/13958], Loss: 9.5001\n",
      "Epoch [51/100], Step [1500/13958], Loss: 9.1563\n",
      "Epoch [51/100], Step [1600/13958], Loss: 6.9532\n",
      "Epoch [51/100], Step [1700/13958], Loss: 8.5938\n",
      "Epoch [51/100], Step [1800/13958], Loss: 6.6094\n",
      "Epoch [51/100], Step [1900/13958], Loss: 6.7969\n",
      "Epoch [51/100], Step [2000/13958], Loss: 9.3438\n",
      "Epoch [51/100], Step [2100/13958], Loss: 7.5781\n",
      "Epoch [51/100], Step [2200/13958], Loss: 5.2969\n",
      "Epoch [51/100], Step [2300/13958], Loss: 8.2970\n",
      "Epoch [51/100], Step [2400/13958], Loss: 6.0156\n",
      "Epoch [51/100], Step [2500/13958], Loss: 7.5157\n",
      "Epoch [51/100], Step [2600/13958], Loss: 7.4063\n",
      "Epoch [51/100], Step [2700/13958], Loss: 6.5469\n",
      "Epoch [51/100], Step [2800/13958], Loss: 7.7344\n",
      "Epoch [51/100], Step [2900/13958], Loss: 8.0156\n",
      "Epoch [51/100], Step [3000/13958], Loss: 7.7813\n",
      "Epoch [51/100], Step [3100/13958], Loss: 5.5313\n",
      "Epoch [51/100], Step [3200/13958], Loss: 6.5781\n",
      "Epoch [51/100], Step [3300/13958], Loss: 7.9375\n",
      "Epoch [51/100], Step [3400/13958], Loss: 9.5470\n",
      "Epoch [51/100], Step [3500/13958], Loss: 6.4845\n",
      "Epoch [51/100], Step [3600/13958], Loss: 9.8750\n",
      "Epoch [51/100], Step [3700/13958], Loss: 6.1250\n",
      "Epoch [51/100], Step [3800/13958], Loss: 7.0001\n",
      "Epoch [51/100], Step [3900/13958], Loss: 8.3438\n",
      "Epoch [51/100], Step [4000/13958], Loss: 9.6719\n",
      "Epoch [51/100], Step [4100/13958], Loss: 10.2500\n",
      "Epoch [51/100], Step [4200/13958], Loss: 7.8438\n",
      "Epoch [51/100], Step [4300/13958], Loss: 7.5626\n",
      "Epoch [51/100], Step [4400/13958], Loss: 6.1406\n",
      "Epoch [51/100], Step [4500/13958], Loss: 5.9532\n",
      "Epoch [51/100], Step [4600/13958], Loss: 9.4375\n",
      "Epoch [51/100], Step [4700/13958], Loss: 7.5001\n",
      "Epoch [51/100], Step [4800/13958], Loss: 6.9063\n",
      "Epoch [51/100], Step [4900/13958], Loss: 8.7032\n",
      "Epoch [51/100], Step [5000/13958], Loss: 7.3438\n",
      "Epoch [51/100], Step [5100/13958], Loss: 6.8908\n",
      "Epoch [51/100], Step [5200/13958], Loss: 8.2969\n",
      "Epoch [51/100], Step [5300/13958], Loss: 9.0937\n",
      "Epoch [51/100], Step [5400/13958], Loss: 7.7500\n",
      "Epoch [51/100], Step [5500/13958], Loss: 10.3750\n",
      "Epoch [51/100], Step [5600/13958], Loss: 5.3438\n",
      "Epoch [51/100], Step [5700/13958], Loss: 7.7501\n",
      "Epoch [51/100], Step [5800/13958], Loss: 13.1563\n",
      "Epoch [51/100], Step [5900/13958], Loss: 7.0157\n",
      "Epoch [51/100], Step [6000/13958], Loss: 10.7657\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [52/100], Step [100/13958], Loss: 8.5313\n",
      "Epoch [52/100], Step [200/13958], Loss: 6.5938\n",
      "Epoch [52/100], Step [300/13958], Loss: 9.0469\n",
      "Epoch [52/100], Step [400/13958], Loss: 5.7501\n",
      "Epoch [52/100], Step [500/13958], Loss: 12.1250\n",
      "Epoch [52/100], Step [600/13958], Loss: 9.2344\n",
      "Epoch [52/100], Step [700/13958], Loss: 6.8125\n",
      "Epoch [52/100], Step [800/13958], Loss: 11.8438\n",
      "Epoch [52/100], Step [900/13958], Loss: 9.3126\n",
      "Epoch [52/100], Step [1000/13958], Loss: 8.3438\n",
      "Epoch [52/100], Step [1100/13958], Loss: 7.5002\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [53/100], Step [100/13958], Loss: 6.5782\n",
      "Epoch [53/100], Step [200/13958], Loss: 9.2657\n",
      "Epoch [53/100], Step [300/13958], Loss: 7.3750\n",
      "Epoch [53/100], Step [400/13958], Loss: 7.7188\n",
      "Epoch [53/100], Step [500/13958], Loss: 8.0000\n",
      "Epoch [53/100], Step [600/13958], Loss: 6.1719\n",
      "Epoch [53/100], Step [700/13958], Loss: 11.3907\n",
      "Epoch [53/100], Step [800/13958], Loss: 8.4531\n",
      "Epoch [53/100], Step [900/13958], Loss: 8.3906\n",
      "Epoch [53/100], Step [1000/13958], Loss: 6.1251\n",
      "Epoch [53/100], Step [1100/13958], Loss: 8.1720\n",
      "Epoch [53/100], Step [1200/13958], Loss: 7.6875\n",
      "Epoch [53/100], Step [1300/13958], Loss: 8.1250\n",
      "Epoch [53/100], Step [1400/13958], Loss: 5.2033\n",
      "Epoch [53/100], Step [1500/13958], Loss: 9.9844\n",
      "Epoch [53/100], Step [1600/13958], Loss: 8.2500\n",
      "Epoch [53/100], Step [1700/13958], Loss: 6.7188\n",
      "Epoch [53/100], Step [1800/13958], Loss: 13.5469\n",
      "Epoch [53/100], Step [1900/13958], Loss: 10.5625\n",
      "Epoch [53/100], Step [2000/13958], Loss: 6.8750\n",
      "Epoch [53/100], Step [2100/13958], Loss: 7.5783\n",
      "Epoch [53/100], Step [2200/13958], Loss: 8.1251\n",
      "Epoch [53/100], Step [2300/13958], Loss: 8.3594\n",
      "Epoch [53/100], Step [2400/13958], Loss: 8.8906\n",
      "Epoch [53/100], Step [2500/13958], Loss: 5.3438\n",
      "Epoch [53/100], Step [2600/13958], Loss: 9.4844\n",
      "Epoch [53/100], Step [2700/13958], Loss: 8.0157\n",
      "Epoch [53/100], Step [2800/13958], Loss: 10.8906\n",
      "Epoch [53/100], Step [2900/13958], Loss: 6.0313\n",
      "Epoch [53/100], Step [3000/13958], Loss: 7.7969\n",
      "Epoch [53/100], Step [3100/13958], Loss: 5.9375\n",
      "Epoch [53/100], Step [3200/13958], Loss: 6.8126\n",
      "Epoch [53/100], Step [3300/13958], Loss: 7.8438\n",
      "Epoch [53/100], Step [3400/13958], Loss: 8.7188\n",
      "Epoch [53/100], Step [3500/13958], Loss: 6.8594\n",
      "Epoch [53/100], Step [3600/13958], Loss: 9.4375\n",
      "Epoch [53/100], Step [3700/13958], Loss: 10.8594\n",
      "Epoch [53/100], Step [3800/13958], Loss: 9.6875\n",
      "Epoch [53/100], Step [3900/13958], Loss: 5.7031\n",
      "Epoch [53/100], Step [4000/13958], Loss: 8.6094\n",
      "Epoch [53/100], Step [4100/13958], Loss: 9.3126\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [54/100], Step [100/13958], Loss: 7.4688\n",
      "Epoch [54/100], Step [200/13958], Loss: 9.0781\n",
      "Epoch [54/100], Step [300/13958], Loss: 9.2188\n",
      "Epoch [54/100], Step [400/13958], Loss: 6.9063\n",
      "Epoch [54/100], Step [500/13958], Loss: 8.2501\n",
      "Epoch [54/100], Step [600/13958], Loss: 10.9687\n",
      "Epoch [54/100], Step [700/13958], Loss: 9.1250\n",
      "Epoch [54/100], Step [800/13958], Loss: 7.2969\n",
      "Epoch [54/100], Step [900/13958], Loss: 8.8282\n",
      "Epoch [54/100], Step [1000/13958], Loss: 8.7345\n",
      "Epoch [54/100], Step [1100/13958], Loss: 7.3750\n",
      "Epoch [54/100], Step [1200/13958], Loss: 7.2345\n",
      "Epoch [54/100], Step [1300/13958], Loss: 8.1719\n",
      "Epoch [54/100], Step [1400/13958], Loss: 5.9845\n",
      "Epoch [54/100], Step [1500/13958], Loss: 5.3282\n",
      "Epoch [54/100], Step [1600/13958], Loss: 9.1875\n",
      "Epoch [54/100], Step [1700/13958], Loss: 7.0938\n",
      "Epoch [54/100], Step [1800/13958], Loss: 6.7188\n",
      "Epoch [54/100], Step [1900/13958], Loss: 9.4844\n",
      "Epoch [54/100], Step [2000/13958], Loss: 6.1563\n",
      "Epoch [54/100], Step [2100/13958], Loss: 5.0470\n",
      "Epoch [54/100], Step [2200/13958], Loss: 6.8438\n",
      "Epoch [54/100], Step [2300/13958], Loss: 10.6250\n",
      "Epoch [54/100], Step [2400/13958], Loss: 10.5782\n",
      "Epoch [54/100], Step [2500/13958], Loss: 7.4843\n",
      "Epoch [54/100], Step [2600/13958], Loss: 11.1406\n",
      "Epoch [54/100], Step [2700/13958], Loss: 7.9844\n",
      "Epoch [54/100], Step [2800/13958], Loss: 10.1250\n",
      "Epoch [54/100], Step [2900/13958], Loss: 7.5000\n",
      "Epoch [54/100], Step [3000/13958], Loss: 8.1407\n",
      "Epoch [54/100], Step [3100/13958], Loss: 6.1250\n",
      "Epoch [54/100], Step [3200/13958], Loss: 7.2344\n",
      "Epoch [54/100], Step [3300/13958], Loss: 10.2813\n",
      "Epoch [54/100], Step [3400/13958], Loss: 8.7031\n",
      "Epoch [54/100], Step [3500/13958], Loss: 9.4375\n",
      "Epoch [54/100], Step [3600/13958], Loss: 9.2501\n",
      "Epoch [54/100], Step [3700/13958], Loss: 7.0156\n",
      "Epoch [54/100], Step [3800/13958], Loss: 6.6250\n",
      "Epoch [54/100], Step [3900/13958], Loss: 4.8594\n",
      "Epoch [54/100], Step [4000/13958], Loss: 9.2969\n",
      "Epoch [54/100], Step [4100/13958], Loss: 6.4219\n",
      "Epoch [54/100], Step [4200/13958], Loss: 5.8439\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [55/100], Step [100/13958], Loss: 8.6094\n",
      "Epoch [55/100], Step [200/13958], Loss: 9.9219\n",
      "Epoch [55/100], Step [300/13958], Loss: 5.7032\n",
      "Epoch [55/100], Step [400/13958], Loss: 7.6407\n",
      "Epoch [55/100], Step [500/13958], Loss: 11.4844\n",
      "Epoch [55/100], Step [600/13958], Loss: 7.6875\n",
      "Epoch [55/100], Step [700/13958], Loss: 8.1250\n",
      "Epoch [55/100], Step [800/13958], Loss: 7.5625\n",
      "Epoch [55/100], Step [900/13958], Loss: 6.3282\n",
      "Epoch [55/100], Step [1000/13958], Loss: 9.2500\n",
      "Epoch [55/100], Step [1100/13958], Loss: 5.6094\n",
      "Epoch [55/100], Step [1200/13958], Loss: 7.0156\n",
      "Epoch [55/100], Step [1300/13958], Loss: 5.2188\n",
      "Epoch [55/100], Step [1400/13958], Loss: 8.8126\n",
      "Epoch [55/100], Step [1500/13958], Loss: 6.7188\n",
      "Epoch [55/100], Step [1600/13958], Loss: 7.3596\n",
      "Epoch [55/100], Step [1700/13958], Loss: 9.6097\n",
      "Epoch [55/100], Step [1800/13958], Loss: 11.1406\n",
      "Epoch [55/100], Step [1900/13958], Loss: 5.2032\n",
      "Epoch [55/100], Step [2000/13958], Loss: 6.3440\n",
      "Epoch [55/100], Step [2100/13958], Loss: 9.1719\n",
      "Epoch [55/100], Step [2200/13958], Loss: 10.5001\n",
      "Epoch [55/100], Step [2300/13958], Loss: 7.9220\n",
      "Epoch [55/100], Step [2400/13958], Loss: 8.1250\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [56/100], Step [100/13958], Loss: 6.8282\n",
      "Epoch [56/100], Step [200/13958], Loss: 8.6406\n",
      "Epoch [56/100], Step [300/13958], Loss: 4.7345\n",
      "Epoch [56/100], Step [400/13958], Loss: 6.1875\n",
      "Epoch [56/100], Step [500/13958], Loss: 7.2188\n",
      "Epoch [56/100], Step [600/13958], Loss: 10.2031\n",
      "Epoch [56/100], Step [700/13958], Loss: 8.6406\n",
      "Epoch [56/100], Step [800/13958], Loss: 7.7501\n",
      "Epoch [56/100], Step [900/13958], Loss: 6.4064\n",
      "Epoch [56/100], Step [1000/13958], Loss: 5.5469\n",
      "Epoch [56/100], Step [1100/13958], Loss: 6.6563\n",
      "Epoch [56/100], Step [1200/13958], Loss: 9.0156\n",
      "Epoch [56/100], Step [1300/13958], Loss: 5.7657\n",
      "Epoch [56/100], Step [1400/13958], Loss: 6.5313\n",
      "Epoch [56/100], Step [1500/13958], Loss: 5.5313\n",
      "Epoch [56/100], Step [1600/13958], Loss: 7.7814\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [57/100], Step [100/13958], Loss: 6.9376\n",
      "Epoch [57/100], Step [200/13958], Loss: 8.2032\n",
      "Epoch [57/100], Step [300/13958], Loss: 5.7969\n",
      "Epoch [57/100], Step [400/13958], Loss: 9.4375\n",
      "Epoch [57/100], Step [500/13958], Loss: 5.4532\n",
      "Epoch [57/100], Step [600/13958], Loss: 4.6094\n",
      "Epoch [57/100], Step [700/13958], Loss: 6.7969\n",
      "Epoch [57/100], Step [800/13958], Loss: 9.9533\n",
      "Epoch [57/100], Step [900/13958], Loss: 5.7971\n",
      "Epoch [57/100], Step [1000/13958], Loss: 5.6095\n",
      "Epoch [57/100], Step [1100/13958], Loss: 9.3125\n",
      "Epoch [57/100], Step [1200/13958], Loss: 5.9219\n",
      "Epoch [57/100], Step [1300/13958], Loss: 7.7500\n",
      "Epoch [57/100], Step [1400/13958], Loss: 5.5781\n",
      "Epoch [57/100], Step [1500/13958], Loss: 8.1720\n",
      "Epoch [57/100], Step [1600/13958], Loss: 8.5001\n",
      "Epoch [57/100], Step [1700/13958], Loss: 6.7031\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [58/100], Step [100/13958], Loss: 7.6407\n",
      "Epoch [58/100], Step [200/13958], Loss: 12.5781\n",
      "Epoch [58/100], Step [300/13958], Loss: 8.0938\n",
      "Epoch [58/100], Step [400/13958], Loss: 6.6094\n",
      "Epoch [58/100], Step [500/13958], Loss: 9.1250\n",
      "Epoch [58/100], Step [600/13958], Loss: 8.6563\n",
      "Epoch [58/100], Step [700/13958], Loss: 8.9532\n",
      "Epoch [58/100], Step [800/13958], Loss: 9.0313\n",
      "Epoch [58/100], Step [900/13958], Loss: 8.6408\n",
      "Epoch [58/100], Step [1000/13958], Loss: 10.0626\n",
      "Epoch [58/100], Step [1100/13958], Loss: 7.9062\n",
      "Epoch [58/100], Step [1200/13958], Loss: 3.1094\n",
      "Epoch [58/100], Step [1300/13958], Loss: 6.7502\n",
      "Epoch [58/100], Step [1400/13958], Loss: 7.1563\n",
      "Epoch [58/100], Step [1500/13958], Loss: 6.7969\n",
      "Epoch [58/100], Step [1600/13958], Loss: 8.6719\n",
      "Epoch [58/100], Step [1700/13958], Loss: 8.2969\n",
      "Epoch [58/100], Step [1800/13958], Loss: 6.9531\n",
      "Epoch [58/100], Step [1900/13958], Loss: 8.3281\n",
      "Epoch [58/100], Step [2000/13958], Loss: 7.8906\n",
      "Epoch [58/100], Step [2100/13958], Loss: 7.0938\n",
      "Epoch [58/100], Step [2200/13958], Loss: 10.1250\n",
      "Epoch [58/100], Step [2300/13958], Loss: 5.2969\n",
      "Epoch [58/100], Step [2400/13958], Loss: 9.7813\n",
      "Epoch [58/100], Step [2500/13958], Loss: 8.0156\n",
      "Epoch [58/100], Step [2600/13958], Loss: 5.7657\n",
      "Epoch [58/100], Step [2700/13958], Loss: 9.4844\n",
      "Epoch [58/100], Step [2800/13958], Loss: 6.9220\n",
      "Epoch [58/100], Step [2900/13958], Loss: 4.6564\n",
      "Epoch [58/100], Step [3000/13958], Loss: 8.6563\n",
      "Epoch [58/100], Step [3100/13958], Loss: 7.6250\n",
      "Epoch [58/100], Step [3200/13958], Loss: 7.6407\n",
      "Epoch [58/100], Step [3300/13958], Loss: 8.7032\n",
      "Epoch [58/100], Step [3400/13958], Loss: 9.0782\n",
      "Epoch [58/100], Step [3500/13958], Loss: 7.3125\n",
      "Epoch [58/100], Step [3600/13958], Loss: 3.7188\n",
      "Epoch [58/100], Step [3700/13958], Loss: 7.7657\n",
      "Epoch [58/100], Step [3800/13958], Loss: 9.9688\n",
      "Epoch [58/100], Step [3900/13958], Loss: 5.5938\n",
      "Epoch [58/100], Step [4000/13958], Loss: 6.8595\n",
      "Epoch [58/100], Step [4100/13958], Loss: 7.3126\n",
      "Epoch [58/100], Step [4200/13958], Loss: 5.6563\n",
      "Epoch [58/100], Step [4300/13958], Loss: 6.8438\n",
      "Epoch [58/100], Step [4400/13958], Loss: 8.3438\n",
      "Epoch [58/100], Step [4500/13958], Loss: 10.6563\n",
      "Epoch [58/100], Step [4600/13958], Loss: 9.3750\n",
      "Epoch [58/100], Step [4700/13958], Loss: 3.3751\n",
      "Epoch [58/100], Step [4800/13958], Loss: 7.7657\n",
      "Epoch [58/100], Step [4900/13958], Loss: 7.4220\n",
      "Epoch [58/100], Step [5000/13958], Loss: 9.0626\n",
      "Epoch [58/100], Step [5100/13958], Loss: 8.1250\n",
      "Epoch [58/100], Step [5200/13958], Loss: 7.9219\n",
      "Epoch [58/100], Step [5300/13958], Loss: 9.4532\n",
      "Epoch [58/100], Step [5400/13958], Loss: 6.6094\n",
      "Epoch [58/100], Step [5500/13958], Loss: 7.9219\n",
      "Epoch [58/100], Step [5600/13958], Loss: 8.1563\n",
      "Epoch [58/100], Step [5700/13958], Loss: 8.3281\n",
      "Epoch [58/100], Step [5800/13958], Loss: 7.1251\n",
      "Epoch [58/100], Step [5900/13958], Loss: 6.2814\n",
      "Epoch [58/100], Step [6000/13958], Loss: 5.7813\n",
      "Epoch [58/100], Step [6100/13958], Loss: 4.2969\n",
      "Epoch [58/100], Step [6200/13958], Loss: 6.2188\n",
      "Epoch [58/100], Step [6300/13958], Loss: 5.4532\n",
      "Epoch [58/100], Step [6400/13958], Loss: 8.4063\n",
      "Epoch [58/100], Step [6500/13958], Loss: 11.2657\n",
      "Epoch [58/100], Step [6600/13958], Loss: 6.7500\n",
      "Epoch [58/100], Step [6700/13958], Loss: 9.8126\n",
      "Epoch [58/100], Step [6800/13958], Loss: 4.8907\n",
      "Epoch [58/100], Step [6900/13958], Loss: 5.9220\n",
      "Epoch [58/100], Step [7000/13958], Loss: 7.2188\n",
      "Epoch [58/100], Step [7100/13958], Loss: 7.7188\n",
      "Epoch [58/100], Step [7200/13958], Loss: 10.9531\n",
      "Epoch [58/100], Step [7300/13958], Loss: 10.2813\n",
      "Epoch [58/100], Step [7400/13958], Loss: 6.4533\n",
      "Epoch [58/100], Step [7500/13958], Loss: 8.3907\n",
      "Epoch [58/100], Step [7600/13958], Loss: 5.9844\n",
      "Epoch [58/100], Step [7700/13958], Loss: 7.8907\n",
      "Epoch [58/100], Step [7800/13958], Loss: 7.4532\n",
      "Epoch [58/100], Step [7900/13958], Loss: 6.8125\n",
      "Epoch [58/100], Step [8000/13958], Loss: 5.4845\n",
      "Epoch [58/100], Step [8100/13958], Loss: 7.2032\n",
      "Epoch [58/100], Step [8200/13958], Loss: 5.9688\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [59/100], Step [100/13958], Loss: 5.6252\n",
      "Epoch [59/100], Step [200/13958], Loss: 9.9844\n",
      "Epoch [59/100], Step [300/13958], Loss: 6.5157\n",
      "Epoch [59/100], Step [400/13958], Loss: 7.1407\n",
      "Epoch [59/100], Step [500/13958], Loss: 8.9220\n",
      "Epoch [59/100], Step [600/13958], Loss: 10.0313\n",
      "Epoch [59/100], Step [700/13958], Loss: 12.2969\n",
      "Epoch [59/100], Step [800/13958], Loss: 6.9219\n",
      "Epoch [59/100], Step [900/13958], Loss: 9.0781\n",
      "Epoch [59/100], Step [1000/13958], Loss: 4.1251\n",
      "Epoch [59/100], Step [1100/13958], Loss: 7.4532\n",
      "Epoch [59/100], Step [1200/13958], Loss: 15.8906\n",
      "Epoch [59/100], Step [1300/13958], Loss: 11.2656\n",
      "Epoch [59/100], Step [1400/13958], Loss: 4.2344\n",
      "Epoch [59/100], Step [1500/13958], Loss: 8.6875\n",
      "Epoch [59/100], Step [1600/13958], Loss: 6.6876\n",
      "Epoch [59/100], Step [1700/13958], Loss: 6.9531\n",
      "Epoch [59/100], Step [1800/13958], Loss: 5.7032\n",
      "Epoch [59/100], Step [1900/13958], Loss: 5.2032\n",
      "Epoch [59/100], Step [2000/13958], Loss: 8.1407\n",
      "Epoch [59/100], Step [2100/13958], Loss: 8.8906\n",
      "Epoch [59/100], Step [2200/13958], Loss: 8.4844\n",
      "Epoch [59/100], Step [2300/13958], Loss: 6.2032\n",
      "Epoch [59/100], Step [2400/13958], Loss: 5.6094\n",
      "Epoch [59/100], Step [2500/13958], Loss: 9.7189\n",
      "Epoch [59/100], Step [2600/13958], Loss: 8.6407\n",
      "Epoch [59/100], Step [2700/13958], Loss: 4.1095\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [60/100], Step [100/13958], Loss: 11.2188\n",
      "Epoch [60/100], Step [200/13958], Loss: 9.4219\n",
      "Epoch [60/100], Step [300/13958], Loss: 6.0938\n",
      "Epoch [60/100], Step [400/13958], Loss: 9.7501\n",
      "Epoch [60/100], Step [500/13958], Loss: 8.0938\n",
      "Epoch [60/100], Step [600/13958], Loss: 4.6096\n",
      "Epoch [60/100], Step [700/13958], Loss: 7.9375\n",
      "Epoch [60/100], Step [800/13958], Loss: 8.7656\n",
      "Epoch [60/100], Step [900/13958], Loss: 6.3750\n",
      "Epoch [60/100], Step [1000/13958], Loss: 7.2657\n",
      "Epoch [60/100], Step [1100/13958], Loss: 11.2656\n",
      "Epoch [60/100], Step [1200/13958], Loss: 8.1875\n",
      "Epoch [60/100], Step [1300/13958], Loss: 6.8438\n",
      "Epoch [60/100], Step [1400/13958], Loss: 4.8594\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [61/100], Step [100/13958], Loss: 7.6875\n",
      "Epoch [61/100], Step [200/13958], Loss: 8.7813\n",
      "Epoch [61/100], Step [300/13958], Loss: 8.0313\n",
      "Epoch [61/100], Step [400/13958], Loss: 7.0781\n",
      "Epoch [61/100], Step [500/13958], Loss: 6.7031\n",
      "Epoch [61/100], Step [600/13958], Loss: 8.5000\n",
      "Epoch [61/100], Step [700/13958], Loss: 7.1720\n",
      "Epoch [61/100], Step [800/13958], Loss: 6.1094\n",
      "Epoch [61/100], Step [900/13958], Loss: 6.5938\n",
      "Epoch [61/100], Step [1000/13958], Loss: 7.8594\n",
      "Epoch [61/100], Step [1100/13958], Loss: 5.7344\n",
      "Epoch [61/100], Step [1200/13958], Loss: 7.5313\n",
      "Epoch [61/100], Step [1300/13958], Loss: 9.7969\n",
      "Epoch [61/100], Step [1400/13958], Loss: 7.6250\n",
      "Epoch [61/100], Step [1500/13958], Loss: 11.1719\n",
      "Epoch [61/100], Step [1600/13958], Loss: 4.5469\n",
      "Epoch [61/100], Step [1700/13958], Loss: 7.2500\n",
      "Epoch [61/100], Step [1800/13958], Loss: 9.4375\n",
      "Epoch [61/100], Step [1900/13958], Loss: 7.7657\n",
      "Epoch [61/100], Step [2000/13958], Loss: 6.2502\n",
      "Epoch [61/100], Step [2100/13958], Loss: 6.2969\n",
      "Epoch [61/100], Step [2200/13958], Loss: 6.6094\n",
      "Epoch [61/100], Step [2300/13958], Loss: 6.7971\n",
      "Epoch [61/100], Step [2400/13958], Loss: 10.7656\n",
      "Epoch [61/100], Step [2500/13958], Loss: 9.3440\n",
      "Epoch [61/100], Step [2600/13958], Loss: 8.4533\n",
      "Epoch [61/100], Step [2700/13958], Loss: 8.7813\n",
      "Epoch [61/100], Step [2800/13958], Loss: 6.3595\n",
      "Epoch [61/100], Step [2900/13958], Loss: 6.8125\n",
      "Epoch [61/100], Step [3000/13958], Loss: 8.5626\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [62/100], Step [100/13958], Loss: 7.8125\n",
      "Epoch [62/100], Step [200/13958], Loss: 5.4688\n",
      "Epoch [62/100], Step [300/13958], Loss: 6.4219\n",
      "Epoch [62/100], Step [400/13958], Loss: 10.1719\n",
      "Epoch [62/100], Step [500/13958], Loss: 6.5625\n",
      "Epoch [62/100], Step [600/13958], Loss: 5.5001\n",
      "Epoch [62/100], Step [700/13958], Loss: 8.7032\n",
      "Epoch [62/100], Step [800/13958], Loss: 5.3907\n",
      "Epoch [62/100], Step [900/13958], Loss: 9.3125\n",
      "Epoch [62/100], Step [1000/13958], Loss: 6.2813\n",
      "Epoch [62/100], Step [1100/13958], Loss: 7.2656\n",
      "Epoch [62/100], Step [1200/13958], Loss: 5.2032\n",
      "Epoch [62/100], Step [1300/13958], Loss: 5.8438\n",
      "Epoch [62/100], Step [1400/13958], Loss: 8.3439\n",
      "Epoch [62/100], Step [1500/13958], Loss: 6.4064\n",
      "Epoch [62/100], Step [1600/13958], Loss: 10.7813\n",
      "Epoch [62/100], Step [1700/13958], Loss: 7.8126\n",
      "Epoch [62/100], Step [1800/13958], Loss: 5.3907\n",
      "Epoch [62/100], Step [1900/13958], Loss: 8.9375\n",
      "Epoch [62/100], Step [2000/13958], Loss: 10.8750\n",
      "Epoch [62/100], Step [2100/13958], Loss: 4.6876\n",
      "Epoch [62/100], Step [2200/13958], Loss: 7.2188\n",
      "Epoch [62/100], Step [2300/13958], Loss: 10.0313\n",
      "Epoch [62/100], Step [2400/13958], Loss: 7.1406\n",
      "Epoch [62/100], Step [2500/13958], Loss: 7.4688\n",
      "Epoch [62/100], Step [2600/13958], Loss: 8.6720\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [63/100], Step [100/13958], Loss: 7.6720\n",
      "Epoch [63/100], Step [200/13958], Loss: 7.1250\n",
      "Epoch [63/100], Step [300/13958], Loss: 3.9688\n",
      "Epoch [63/100], Step [400/13958], Loss: 7.7969\n",
      "Epoch [63/100], Step [500/13958], Loss: 8.4220\n",
      "Epoch [63/100], Step [600/13958], Loss: 6.0938\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [64/100], Step [100/13958], Loss: 5.5781\n",
      "Epoch [64/100], Step [200/13958], Loss: 9.9063\n",
      "Epoch [64/100], Step [300/13958], Loss: 8.5625\n",
      "Epoch [64/100], Step [400/13958], Loss: 5.1876\n",
      "Epoch [64/100], Step [500/13958], Loss: 7.3125\n",
      "Epoch [64/100], Step [600/13958], Loss: 8.5313\n",
      "Epoch [64/100], Step [700/13958], Loss: 7.0156\n",
      "Epoch [64/100], Step [800/13958], Loss: 4.5313\n",
      "Epoch [64/100], Step [900/13958], Loss: 9.0938\n",
      "Epoch [64/100], Step [1000/13958], Loss: 10.9531\n",
      "Epoch [64/100], Step [1100/13958], Loss: 7.1720\n",
      "Epoch [64/100], Step [1200/13958], Loss: 6.0626\n",
      "Epoch [64/100], Step [1300/13958], Loss: 8.9219\n",
      "Epoch [64/100], Step [1400/13958], Loss: 7.9844\n",
      "Epoch [64/100], Step [1500/13958], Loss: 6.7657\n",
      "Epoch [64/100], Step [1600/13958], Loss: 10.7344\n",
      "Epoch [64/100], Step [1700/13958], Loss: 8.9844\n",
      "Epoch [64/100], Step [1800/13958], Loss: 4.7344\n",
      "Epoch [64/100], Step [1900/13958], Loss: 8.2032\n",
      "Epoch [64/100], Step [2000/13958], Loss: 8.0938\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [65/100], Step [100/13958], Loss: 4.8125\n",
      "Epoch [65/100], Step [200/13958], Loss: 7.9375\n",
      "Epoch [65/100], Step [300/13958], Loss: 4.8283\n",
      "Epoch [65/100], Step [400/13958], Loss: 4.7657\n",
      "Epoch [65/100], Step [500/13958], Loss: 10.6250\n",
      "Epoch [65/100], Step [600/13958], Loss: 9.1875\n",
      "Epoch [65/100], Step [700/13958], Loss: 8.6875\n",
      "Epoch [65/100], Step [800/13958], Loss: 5.9376\n",
      "Epoch [65/100], Step [900/13958], Loss: 5.1878\n",
      "Epoch [65/100], Step [1000/13958], Loss: 8.3126\n",
      "Epoch [65/100], Step [1100/13958], Loss: 7.2657\n",
      "Epoch [65/100], Step [1200/13958], Loss: 7.1409\n",
      "Epoch [65/100], Step [1300/13958], Loss: 10.4062\n",
      "Epoch [65/100], Step [1400/13958], Loss: 4.1406\n",
      "Epoch [65/100], Step [1500/13958], Loss: 5.9063\n",
      "Epoch [65/100], Step [1600/13958], Loss: 5.8594\n",
      "Epoch [65/100], Step [1700/13958], Loss: 6.5468\n",
      "Epoch [65/100], Step [1800/13958], Loss: 8.5625\n",
      "Epoch [65/100], Step [1900/13958], Loss: 5.3594\n",
      "Epoch [65/100], Step [2000/13958], Loss: 9.9219\n",
      "Epoch [65/100], Step [2100/13958], Loss: 7.2188\n",
      "Epoch [65/100], Step [2200/13958], Loss: 5.7500\n",
      "Epoch [65/100], Step [2300/13958], Loss: 7.8125\n",
      "Epoch [65/100], Step [2400/13958], Loss: 7.4063\n",
      "Epoch [65/100], Step [2500/13958], Loss: 12.7657\n",
      "Epoch [65/100], Step [2600/13958], Loss: 12.3594\n",
      "Epoch [65/100], Step [2700/13958], Loss: 7.2501\n",
      "Epoch [65/100], Step [2800/13958], Loss: 4.6407\n",
      "Epoch [65/100], Step [2900/13958], Loss: 7.2657\n",
      "Epoch [65/100], Step [3000/13958], Loss: 10.4063\n",
      "Epoch [65/100], Step [3100/13958], Loss: 10.0625\n",
      "Epoch [65/100], Step [3200/13958], Loss: 7.8595\n",
      "Epoch [65/100], Step [3300/13958], Loss: 9.7188\n",
      "Epoch [65/100], Step [3400/13958], Loss: 8.5781\n",
      "Epoch [65/100], Step [3500/13958], Loss: 5.6406\n",
      "Epoch [65/100], Step [3600/13958], Loss: 9.3438\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [66/100], Step [100/13958], Loss: 5.9532\n",
      "Epoch [66/100], Step [200/13958], Loss: 11.8750\n",
      "Epoch [66/100], Step [300/13958], Loss: 13.0469\n",
      "Epoch [66/100], Step [400/13958], Loss: 9.2656\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [67/100], Step [100/13958], Loss: 6.0156\n",
      "Epoch [67/100], Step [200/13958], Loss: 9.2032\n",
      "Epoch [67/100], Step [300/13958], Loss: 5.5313\n",
      "Epoch [67/100], Step [400/13958], Loss: 6.1252\n",
      "Epoch [67/100], Step [500/13958], Loss: 9.1720\n",
      "Epoch [67/100], Step [600/13958], Loss: 10.2969\n",
      "Epoch [67/100], Step [700/13958], Loss: 5.0001\n",
      "Epoch [67/100], Step [800/13958], Loss: 8.3594\n",
      "Epoch [67/100], Step [900/13958], Loss: 12.0782\n",
      "Epoch [67/100], Step [1000/13958], Loss: 8.5469\n",
      "Epoch [67/100], Step [1100/13958], Loss: 9.5625\n",
      "Epoch [67/100], Step [1200/13958], Loss: 7.5938\n",
      "Epoch [67/100], Step [1300/13958], Loss: 7.7032\n",
      "Epoch [67/100], Step [1400/13958], Loss: 5.2501\n",
      "Epoch [67/100], Step [1500/13958], Loss: 10.5781\n",
      "Epoch [67/100], Step [1600/13958], Loss: 10.7344\n",
      "Epoch [67/100], Step [1700/13958], Loss: 7.2345\n",
      "Epoch [67/100], Step [1800/13958], Loss: 8.2657\n",
      "Epoch [67/100], Step [1900/13958], Loss: 9.7813\n",
      "Epoch [67/100], Step [2000/13958], Loss: 8.2188\n",
      "Epoch [67/100], Step [2100/13958], Loss: 6.9688\n",
      "Epoch [67/100], Step [2200/13958], Loss: 9.1563\n",
      "Epoch [67/100], Step [2300/13958], Loss: 7.7813\n",
      "Epoch [67/100], Step [2400/13958], Loss: 7.7500\n",
      "Epoch [67/100], Step [2500/13958], Loss: 9.2969\n",
      "Epoch [67/100], Step [2600/13958], Loss: 7.5626\n",
      "Epoch [67/100], Step [2700/13958], Loss: 8.4376\n",
      "Epoch [67/100], Step [2800/13958], Loss: 7.5157\n",
      "Epoch [67/100], Step [2900/13958], Loss: 9.2969\n",
      "Epoch [67/100], Step [3000/13958], Loss: 5.2344\n",
      "Epoch [67/100], Step [3100/13958], Loss: 7.8906\n",
      "Epoch [67/100], Step [3200/13958], Loss: 6.0158\n",
      "Epoch [67/100], Step [3300/13958], Loss: 8.1096\n",
      "Epoch [67/100], Step [3400/13958], Loss: 9.1719\n",
      "Epoch [67/100], Step [3500/13958], Loss: 5.4532\n",
      "Epoch [67/100], Step [3600/13958], Loss: 10.3438\n",
      "Epoch [67/100], Step [3700/13958], Loss: 9.2970\n",
      "Epoch [67/100], Step [3800/13958], Loss: 8.6094\n",
      "Epoch [67/100], Step [3900/13958], Loss: 8.4375\n",
      "Epoch [67/100], Step [4000/13958], Loss: 5.1563\n",
      "Epoch [67/100], Step [4100/13958], Loss: 10.8907\n",
      "Epoch [67/100], Step [4200/13958], Loss: 10.3281\n",
      "Epoch [67/100], Step [4300/13958], Loss: 7.3281\n",
      "Epoch [67/100], Step [4400/13958], Loss: 7.5313\n",
      "Epoch [67/100], Step [4500/13958], Loss: 8.1406\n",
      "Epoch [67/100], Step [4600/13958], Loss: 8.6407\n",
      "Epoch [67/100], Step [4700/13958], Loss: 6.9219\n",
      "Epoch [67/100], Step [4800/13958], Loss: 8.8750\n",
      "Epoch [67/100], Step [4900/13958], Loss: 7.0470\n",
      "Epoch [67/100], Step [5000/13958], Loss: 9.8750\n",
      "Epoch [67/100], Step [5100/13958], Loss: 4.5782\n",
      "Epoch [67/100], Step [5200/13958], Loss: 5.6094\n",
      "Epoch [67/100], Step [5300/13958], Loss: 7.7500\n",
      "Epoch [67/100], Step [5400/13958], Loss: 8.0469\n",
      "Epoch [67/100], Step [5500/13958], Loss: 7.9064\n",
      "Epoch [67/100], Step [5600/13958], Loss: 7.8906\n",
      "Epoch [67/100], Step [5700/13958], Loss: 7.7500\n",
      "Epoch [67/100], Step [5800/13958], Loss: 7.8438\n",
      "Epoch [67/100], Step [5900/13958], Loss: 5.1563\n",
      "Epoch [67/100], Step [6000/13958], Loss: 8.8595\n",
      "Epoch [67/100], Step [6100/13958], Loss: 8.3907\n",
      "Epoch [67/100], Step [6200/13958], Loss: 12.6875\n",
      "Epoch [67/100], Step [6300/13958], Loss: 5.2188\n",
      "Epoch [67/100], Step [6400/13958], Loss: 4.4844\n",
      "Epoch [67/100], Step [6500/13958], Loss: 5.8750\n",
      "Epoch [67/100], Step [6600/13958], Loss: 6.8125\n",
      "Epoch [67/100], Step [6700/13958], Loss: 5.6719\n",
      "Epoch [67/100], Step [6800/13958], Loss: 6.1407\n",
      "Epoch [67/100], Step [6900/13958], Loss: 8.9375\n",
      "Epoch [67/100], Step [7000/13958], Loss: 6.6408\n",
      "Epoch [67/100], Step [7100/13958], Loss: 5.1563\n",
      "Epoch [67/100], Step [7200/13958], Loss: 7.1095\n",
      "Epoch [67/100], Step [7300/13958], Loss: 5.7189\n",
      "Epoch [67/100], Step [7400/13958], Loss: 6.1876\n",
      "Epoch [67/100], Step [7500/13958], Loss: 6.2970\n",
      "Epoch [67/100], Step [7600/13958], Loss: 7.3750\n",
      "Epoch [67/100], Step [7700/13958], Loss: 9.3907\n",
      "Epoch [67/100], Step [7800/13958], Loss: 6.1250\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [68/100], Step [100/13958], Loss: 7.7813\n",
      "Epoch [68/100], Step [200/13958], Loss: 9.2813\n",
      "Epoch [68/100], Step [300/13958], Loss: 8.3595\n",
      "Epoch [68/100], Step [400/13958], Loss: 10.6094\n",
      "Epoch [68/100], Step [500/13958], Loss: 7.3906\n",
      "Epoch [68/100], Step [600/13958], Loss: 6.8595\n",
      "Epoch [68/100], Step [700/13958], Loss: 10.9689\n",
      "Epoch [68/100], Step [800/13958], Loss: 6.2969\n",
      "Epoch [68/100], Step [900/13958], Loss: 7.7970\n",
      "Epoch [68/100], Step [1000/13958], Loss: 8.1407\n",
      "Epoch [68/100], Step [1100/13958], Loss: 10.1250\n",
      "Epoch [68/100], Step [1200/13958], Loss: 8.5157\n",
      "Epoch [68/100], Step [1300/13958], Loss: 9.8438\n",
      "Epoch [68/100], Step [1400/13958], Loss: 7.2188\n",
      "Epoch [68/100], Step [1500/13958], Loss: 8.9063\n",
      "Epoch [68/100], Step [1600/13958], Loss: 9.4845\n",
      "Epoch [68/100], Step [1700/13958], Loss: 13.6875\n",
      "Epoch [68/100], Step [1800/13958], Loss: 8.8595\n",
      "Epoch [68/100], Step [1900/13958], Loss: 6.1094\n",
      "Epoch [68/100], Step [2000/13958], Loss: 4.5313\n",
      "Epoch [68/100], Step [2100/13958], Loss: 8.9845\n",
      "Epoch [68/100], Step [2200/13958], Loss: 7.4845\n",
      "Epoch [68/100], Step [2300/13958], Loss: 7.2813\n",
      "Epoch [68/100], Step [2400/13958], Loss: 8.5782\n",
      "Epoch [68/100], Step [2500/13958], Loss: 6.2032\n",
      "Epoch [68/100], Step [2600/13958], Loss: 4.6876\n",
      "Epoch [68/100], Step [2700/13958], Loss: 8.2969\n",
      "Epoch [68/100], Step [2800/13958], Loss: 7.7345\n",
      "Epoch [68/100], Step [2900/13958], Loss: 7.6251\n",
      "Epoch [68/100], Step [3000/13958], Loss: 5.7969\n",
      "Epoch [68/100], Step [3100/13958], Loss: 7.6719\n",
      "Epoch [68/100], Step [3200/13958], Loss: 6.7969\n",
      "Epoch [68/100], Step [3300/13958], Loss: 9.6250\n",
      "Epoch [68/100], Step [3400/13958], Loss: 9.9531\n",
      "Epoch [68/100], Step [3500/13958], Loss: 6.6251\n",
      "Epoch [68/100], Step [3600/13958], Loss: 10.3128\n",
      "Epoch [68/100], Step [3700/13958], Loss: 8.5156\n",
      "Epoch [68/100], Step [3800/13958], Loss: 6.1719\n",
      "Epoch [68/100], Step [3900/13958], Loss: 7.3282\n",
      "Epoch [68/100], Step [4000/13958], Loss: 9.9531\n",
      "Epoch [68/100], Step [4100/13958], Loss: 6.6250\n",
      "Epoch [68/100], Step [4200/13958], Loss: 5.5157\n",
      "Epoch [68/100], Step [4300/13958], Loss: 13.4844\n",
      "Epoch [68/100], Step [4400/13958], Loss: 9.5625\n",
      "Epoch [68/100], Step [4500/13958], Loss: 7.2971\n",
      "Epoch [68/100], Step [4600/13958], Loss: 6.5156\n",
      "Epoch [68/100], Step [4700/13958], Loss: 6.9063\n",
      "Epoch [68/100], Step [4800/13958], Loss: 9.2345\n",
      "Epoch [68/100], Step [4900/13958], Loss: 6.3282\n",
      "Epoch [68/100], Step [5000/13958], Loss: 5.0626\n",
      "Epoch [68/100], Step [5100/13958], Loss: 10.7813\n",
      "Epoch [68/100], Step [5200/13958], Loss: 9.6251\n",
      "Epoch [68/100], Step [5300/13958], Loss: 11.7500\n",
      "Epoch [68/100], Step [5400/13958], Loss: 8.0000\n",
      "Epoch [68/100], Step [5500/13958], Loss: 9.3594\n",
      "Epoch [68/100], Step [5600/13958], Loss: 5.9688\n",
      "Epoch [68/100], Step [5700/13958], Loss: 6.6563\n",
      "Epoch [68/100], Step [5800/13958], Loss: 10.1876\n",
      "Epoch [68/100], Step [5900/13958], Loss: 7.6719\n",
      "Epoch [68/100], Step [6000/13958], Loss: 10.6407\n",
      "Epoch [68/100], Step [6100/13958], Loss: 4.8125\n",
      "Epoch [68/100], Step [6200/13958], Loss: 6.0938\n",
      "Epoch [68/100], Step [6300/13958], Loss: 6.7657\n",
      "Epoch [68/100], Step [6400/13958], Loss: 5.2813\n",
      "Epoch [68/100], Step [6500/13958], Loss: 8.2344\n",
      "Epoch [68/100], Step [6600/13958], Loss: 3.5938\n",
      "Epoch [68/100], Step [6700/13958], Loss: 6.6251\n",
      "Epoch [68/100], Step [6800/13958], Loss: 6.1876\n",
      "Epoch [68/100], Step [6900/13958], Loss: 5.2656\n",
      "Epoch [68/100], Step [7000/13958], Loss: 6.1876\n",
      "Epoch [68/100], Step [7100/13958], Loss: 9.0938\n",
      "Epoch [68/100], Step [7200/13958], Loss: 7.6406\n",
      "Epoch [68/100], Step [7300/13958], Loss: 6.7657\n",
      "Epoch [68/100], Step [7400/13958], Loss: 5.3127\n",
      "Epoch [68/100], Step [7500/13958], Loss: 8.4062\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [69/100], Step [100/13958], Loss: 8.9531\n",
      "Epoch [69/100], Step [200/13958], Loss: 10.6719\n",
      "Epoch [69/100], Step [300/13958], Loss: 6.4532\n",
      "Epoch [69/100], Step [400/13958], Loss: 11.0157\n",
      "Epoch [69/100], Step [500/13958], Loss: 7.0781\n",
      "Epoch [69/100], Step [600/13958], Loss: 8.3594\n",
      "Epoch [69/100], Step [700/13958], Loss: 7.0939\n",
      "Epoch [69/100], Step [800/13958], Loss: 7.0625\n",
      "Epoch [69/100], Step [900/13958], Loss: 11.5156\n",
      "Epoch [69/100], Step [1000/13958], Loss: 6.0313\n",
      "Epoch [69/100], Step [1100/13958], Loss: 13.9375\n",
      "Epoch [69/100], Step [1200/13958], Loss: 4.9219\n",
      "Epoch [69/100], Step [1300/13958], Loss: 13.0625\n",
      "Epoch [69/100], Step [1400/13958], Loss: 8.3282\n",
      "Epoch [69/100], Step [1500/13958], Loss: 6.0626\n",
      "Epoch [69/100], Step [1600/13958], Loss: 8.5469\n",
      "Epoch [69/100], Step [1700/13958], Loss: 7.8281\n",
      "Epoch [69/100], Step [1800/13958], Loss: 6.1251\n",
      "Epoch [69/100], Step [1900/13958], Loss: 7.0469\n",
      "Epoch [69/100], Step [2000/13958], Loss: 6.7813\n",
      "Epoch [69/100], Step [2100/13958], Loss: 8.9063\n",
      "Epoch [69/100], Step [2200/13958], Loss: 12.5782\n",
      "Epoch [69/100], Step [2300/13958], Loss: 5.9063\n",
      "Epoch [69/100], Step [2400/13958], Loss: 5.4531\n",
      "Epoch [69/100], Step [2500/13958], Loss: 12.8437\n",
      "Epoch [69/100], Step [2600/13958], Loss: 6.0469\n",
      "Epoch [69/100], Step [2700/13958], Loss: 9.6720\n",
      "Epoch [69/100], Step [2800/13958], Loss: 9.9689\n",
      "Epoch [69/100], Step [2900/13958], Loss: 7.7813\n",
      "Epoch [69/100], Step [3000/13958], Loss: 8.6094\n",
      "Epoch [69/100], Step [3100/13958], Loss: 5.4063\n",
      "Epoch [69/100], Step [3200/13958], Loss: 8.7188\n",
      "Epoch [69/100], Step [3300/13958], Loss: 8.1875\n",
      "Epoch [69/100], Step [3400/13958], Loss: 7.1719\n",
      "Epoch [69/100], Step [3500/13958], Loss: 9.6563\n",
      "Epoch [69/100], Step [3600/13958], Loss: 6.6563\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [70/100], Step [100/13958], Loss: 7.3125\n",
      "Epoch [70/100], Step [200/13958], Loss: 6.6563\n",
      "Epoch [70/100], Step [300/13958], Loss: 5.1719\n",
      "Epoch [70/100], Step [400/13958], Loss: 10.2656\n",
      "Epoch [70/100], Step [500/13958], Loss: 8.0002\n",
      "Epoch [70/100], Step [600/13958], Loss: 6.8908\n",
      "Epoch [70/100], Step [700/13958], Loss: 6.8906\n",
      "Epoch [70/100], Step [800/13958], Loss: 6.4532\n",
      "Epoch [70/100], Step [900/13958], Loss: 5.0627\n",
      "Epoch [70/100], Step [1000/13958], Loss: 9.6250\n",
      "Epoch [70/100], Step [1100/13958], Loss: 6.5782\n",
      "Epoch [70/100], Step [1200/13958], Loss: 6.8438\n",
      "Epoch [70/100], Step [1300/13958], Loss: 7.4690\n",
      "Epoch [70/100], Step [1400/13958], Loss: 7.3282\n",
      "Epoch [70/100], Step [1500/13958], Loss: 8.2813\n",
      "Epoch [70/100], Step [1600/13958], Loss: 6.4690\n",
      "Epoch [70/100], Step [1700/13958], Loss: 6.3438\n",
      "Epoch [70/100], Step [1800/13958], Loss: 9.5625\n",
      "Epoch [70/100], Step [1900/13958], Loss: 7.8751\n",
      "Epoch [70/100], Step [2000/13958], Loss: 7.8908\n",
      "Epoch [70/100], Step [2100/13958], Loss: 6.0939\n",
      "Epoch [70/100], Step [2200/13958], Loss: 12.3594\n",
      "Epoch [70/100], Step [2300/13958], Loss: 5.9064\n",
      "Epoch [70/100], Step [2400/13958], Loss: 7.8750\n",
      "Epoch [70/100], Step [2500/13958], Loss: 10.0000\n",
      "Epoch [70/100], Step [2600/13958], Loss: 7.0783\n",
      "Epoch [70/100], Step [2700/13958], Loss: 9.1407\n",
      "Epoch [70/100], Step [2800/13958], Loss: 6.3594\n",
      "Epoch [70/100], Step [2900/13958], Loss: 6.1407\n",
      "Epoch [70/100], Step [3000/13958], Loss: 8.5158\n",
      "Epoch [70/100], Step [3100/13958], Loss: 7.2345\n",
      "Epoch [70/100], Step [3200/13958], Loss: 7.6720\n",
      "Epoch [70/100], Step [3300/13958], Loss: 6.7813\n",
      "Epoch [70/100], Step [3400/13958], Loss: 8.9063\n",
      "Epoch [70/100], Step [3500/13958], Loss: 7.7188\n",
      "Epoch [70/100], Step [3600/13958], Loss: 10.2188\n",
      "Epoch [70/100], Step [3700/13958], Loss: 4.7500\n",
      "Epoch [70/100], Step [3800/13958], Loss: 9.4375\n",
      "Epoch [70/100], Step [3900/13958], Loss: 7.0781\n",
      "Epoch [70/100], Step [4000/13958], Loss: 9.8594\n",
      "Epoch [70/100], Step [4100/13958], Loss: 5.7657\n",
      "Epoch [70/100], Step [4200/13958], Loss: 10.0625\n",
      "Epoch [70/100], Step [4300/13958], Loss: 8.1095\n",
      "Epoch [70/100], Step [4400/13958], Loss: 8.7032\n",
      "Epoch [70/100], Step [4500/13958], Loss: 6.2969\n",
      "Epoch [70/100], Step [4600/13958], Loss: 6.3438\n",
      "Epoch [70/100], Step [4700/13958], Loss: 10.2031\n",
      "Epoch [70/100], Step [4800/13958], Loss: 7.1875\n",
      "Epoch [70/100], Step [4900/13958], Loss: 5.8439\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [71/100], Step [100/13958], Loss: 9.0000\n",
      "Epoch [71/100], Step [200/13958], Loss: 7.9063\n",
      "Epoch [71/100], Step [300/13958], Loss: 8.9688\n",
      "Epoch [71/100], Step [400/13958], Loss: 9.0157\n",
      "Epoch [71/100], Step [500/13958], Loss: 8.1094\n",
      "Epoch [71/100], Step [600/13958], Loss: 11.3281\n",
      "Epoch [71/100], Step [700/13958], Loss: 5.2657\n",
      "Epoch [71/100], Step [800/13958], Loss: 9.4844\n",
      "Epoch [71/100], Step [900/13958], Loss: 5.3596\n",
      "Epoch [71/100], Step [1000/13958], Loss: 7.5628\n",
      "Epoch [71/100], Step [1100/13958], Loss: 5.7032\n",
      "Epoch [71/100], Step [1200/13958], Loss: 10.1251\n",
      "Epoch [71/100], Step [1300/13958], Loss: 5.1563\n",
      "Epoch [71/100], Step [1400/13958], Loss: 6.0781\n",
      "Epoch [71/100], Step [1500/13958], Loss: 5.3751\n",
      "Epoch [71/100], Step [1600/13958], Loss: 9.1094\n",
      "Epoch [71/100], Step [1700/13958], Loss: 9.5626\n",
      "Epoch [71/100], Step [1800/13958], Loss: 7.1876\n",
      "Epoch [71/100], Step [1900/13958], Loss: 5.5625\n",
      "Epoch [71/100], Step [2000/13958], Loss: 6.2344\n",
      "Epoch [71/100], Step [2100/13958], Loss: 4.0626\n",
      "Epoch [71/100], Step [2200/13958], Loss: 4.9688\n",
      "Epoch [71/100], Step [2300/13958], Loss: 7.4688\n",
      "Epoch [71/100], Step [2400/13958], Loss: 6.8594\n",
      "Epoch [71/100], Step [2500/13958], Loss: 8.5157\n",
      "Epoch [71/100], Step [2600/13958], Loss: 6.3595\n",
      "Epoch [71/100], Step [2700/13958], Loss: 8.3907\n",
      "Epoch [71/100], Step [2800/13958], Loss: 7.5782\n",
      "Epoch [71/100], Step [2900/13958], Loss: 5.6250\n",
      "Epoch [71/100], Step [3000/13958], Loss: 4.5470\n",
      "Epoch [71/100], Step [3100/13958], Loss: 7.4064\n",
      "Epoch [71/100], Step [3200/13958], Loss: 9.6094\n",
      "Epoch [71/100], Step [3300/13958], Loss: 6.7501\n",
      "Epoch [71/100], Step [3400/13958], Loss: 4.8438\n",
      "Epoch [71/100], Step [3500/13958], Loss: 8.9219\n",
      "Epoch [71/100], Step [3600/13958], Loss: 4.6094\n",
      "Epoch [71/100], Step [3700/13958], Loss: 8.5000\n",
      "Epoch [71/100], Step [3800/13958], Loss: 5.8906\n",
      "Epoch [71/100], Step [3900/13958], Loss: 8.9063\n",
      "Epoch [71/100], Step [4000/13958], Loss: 7.9219\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [72/100], Step [100/13958], Loss: 6.5313\n",
      "Epoch [72/100], Step [200/13958], Loss: 7.9219\n",
      "Epoch [72/100], Step [300/13958], Loss: 7.3125\n",
      "Epoch [72/100], Step [400/13958], Loss: 6.4063\n",
      "Epoch [72/100], Step [500/13958], Loss: 6.5781\n",
      "Epoch [72/100], Step [600/13958], Loss: 6.7031\n",
      "Epoch [72/100], Step [700/13958], Loss: 6.1563\n",
      "Epoch [72/100], Step [800/13958], Loss: 6.9688\n",
      "Epoch [72/100], Step [900/13958], Loss: 7.0625\n",
      "Epoch [72/100], Step [1000/13958], Loss: 5.8908\n",
      "Epoch [72/100], Step [1100/13958], Loss: 4.3128\n",
      "Epoch [72/100], Step [1200/13958], Loss: 7.8594\n",
      "Epoch [72/100], Step [1300/13958], Loss: 10.3125\n",
      "Epoch [72/100], Step [1400/13958], Loss: 5.2969\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [73/100], Step [100/13958], Loss: 8.4844\n",
      "Epoch [73/100], Step [200/13958], Loss: 8.4532\n",
      "Epoch [73/100], Step [300/13958], Loss: 7.5939\n",
      "Epoch [73/100], Step [400/13958], Loss: 6.4845\n",
      "Epoch [73/100], Step [500/13958], Loss: 7.9688\n",
      "Epoch [73/100], Step [600/13958], Loss: 7.4376\n",
      "Epoch [73/100], Step [700/13958], Loss: 6.1094\n",
      "Epoch [73/100], Step [800/13958], Loss: 8.7031\n",
      "Epoch [73/100], Step [900/13958], Loss: 6.0469\n",
      "Epoch [73/100], Step [1000/13958], Loss: 5.7969\n",
      "Epoch [73/100], Step [1100/13958], Loss: 7.2188\n",
      "Epoch [73/100], Step [1200/13958], Loss: 7.1563\n",
      "Epoch [73/100], Step [1300/13958], Loss: 7.4688\n",
      "Epoch [73/100], Step [1400/13958], Loss: 8.7501\n",
      "Epoch [73/100], Step [1500/13958], Loss: 6.9064\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [74/100], Step [100/13958], Loss: 7.9376\n",
      "Epoch [74/100], Step [200/13958], Loss: 9.3440\n",
      "Epoch [74/100], Step [300/13958], Loss: 7.9531\n",
      "Epoch [74/100], Step [400/13958], Loss: 7.6876\n",
      "Epoch [74/100], Step [500/13958], Loss: 9.6876\n",
      "Epoch [74/100], Step [600/13958], Loss: 9.0001\n",
      "Epoch [74/100], Step [700/13958], Loss: 7.9219\n",
      "Epoch [74/100], Step [800/13958], Loss: 9.8438\n",
      "Epoch [74/100], Step [900/13958], Loss: 4.1719\n",
      "Epoch [74/100], Step [1000/13958], Loss: 6.7969\n",
      "Epoch [74/100], Step [1100/13958], Loss: 8.4063\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [75/100], Step [100/13958], Loss: 4.7188\n",
      "Epoch [75/100], Step [200/13958], Loss: 6.9531\n",
      "Epoch [75/100], Step [300/13958], Loss: 4.3750\n",
      "Epoch [75/100], Step [400/13958], Loss: 11.5781\n",
      "Epoch [75/100], Step [500/13958], Loss: 8.5782\n",
      "Epoch [75/100], Step [600/13958], Loss: 11.7188\n",
      "Epoch [75/100], Step [700/13958], Loss: 7.3907\n",
      "Epoch [75/100], Step [800/13958], Loss: 7.2969\n",
      "Epoch [75/100], Step [900/13958], Loss: 5.7501\n",
      "Epoch [75/100], Step [1000/13958], Loss: 5.3282\n",
      "Epoch [75/100], Step [1100/13958], Loss: 10.2501\n",
      "Epoch [75/100], Step [1200/13958], Loss: 5.9688\n",
      "Epoch [75/100], Step [1300/13958], Loss: 4.5939\n",
      "Epoch [75/100], Step [1400/13958], Loss: 6.6875\n",
      "Epoch [75/100], Step [1500/13958], Loss: 5.2188\n",
      "Epoch [75/100], Step [1600/13958], Loss: 9.3594\n",
      "Epoch [75/100], Step [1700/13958], Loss: 11.0000\n",
      "Epoch [75/100], Step [1800/13958], Loss: 6.4221\n",
      "Epoch [75/100], Step [1900/13958], Loss: 6.7345\n",
      "Epoch [75/100], Step [2000/13958], Loss: 8.7189\n",
      "Epoch [75/100], Step [2100/13958], Loss: 5.5628\n",
      "Epoch [75/100], Step [2200/13958], Loss: 6.6094\n",
      "Epoch [75/100], Step [2300/13958], Loss: 9.1096\n",
      "Epoch [75/100], Step [2400/13958], Loss: 7.9532\n",
      "Epoch [75/100], Step [2500/13958], Loss: 10.4376\n",
      "Epoch [75/100], Step [2600/13958], Loss: 9.2033\n",
      "Epoch [75/100], Step [2700/13958], Loss: 5.7345\n",
      "Epoch [75/100], Step [2800/13958], Loss: 6.9219\n",
      "Epoch [75/100], Step [2900/13958], Loss: 8.0625\n",
      "Epoch [75/100], Step [3000/13958], Loss: 6.0156\n",
      "Epoch [75/100], Step [3100/13958], Loss: 9.5938\n",
      "Epoch [75/100], Step [3200/13958], Loss: 7.6094\n",
      "Epoch [75/100], Step [3300/13958], Loss: 5.7032\n",
      "Epoch [75/100], Step [3400/13958], Loss: 4.3281\n",
      "Epoch [75/100], Step [3500/13958], Loss: 9.8751\n",
      "Epoch [75/100], Step [3600/13958], Loss: 5.3595\n",
      "Epoch [75/100], Step [3700/13958], Loss: 6.2188\n",
      "Epoch [75/100], Step [3800/13958], Loss: 8.9844\n",
      "Epoch [75/100], Step [3900/13958], Loss: 6.3438\n",
      "Epoch [75/100], Step [4000/13958], Loss: 8.7969\n",
      "Epoch [75/100], Step [4100/13958], Loss: 7.1876\n",
      "Epoch [75/100], Step [4200/13958], Loss: 10.1407\n",
      "Epoch [75/100], Step [4300/13958], Loss: 7.6094\n",
      "Epoch [75/100], Step [4400/13958], Loss: 7.8126\n",
      "Epoch [75/100], Step [4500/13958], Loss: 10.5156\n",
      "Epoch [75/100], Step [4600/13958], Loss: 6.6095\n",
      "Epoch [75/100], Step [4700/13958], Loss: 4.8750\n",
      "Epoch [75/100], Step [4800/13958], Loss: 5.0938\n",
      "Epoch [75/100], Step [4900/13958], Loss: 8.2031\n",
      "Epoch [75/100], Step [5000/13958], Loss: 8.9688\n",
      "Epoch [75/100], Step [5100/13958], Loss: 7.5783\n",
      "Epoch [75/100], Step [5200/13958], Loss: 6.7501\n",
      "Epoch [75/100], Step [5300/13958], Loss: 7.6094\n",
      "Epoch [75/100], Step [5400/13958], Loss: 8.5000\n",
      "Epoch [75/100], Step [5500/13958], Loss: 6.4688\n",
      "Epoch [75/100], Step [5600/13958], Loss: 8.6875\n",
      "Epoch [75/100], Step [5700/13958], Loss: 10.0470\n",
      "Epoch [75/100], Step [5800/13958], Loss: 7.2031\n",
      "Epoch [75/100], Step [5900/13958], Loss: 6.4219\n",
      "Epoch [75/100], Step [6000/13958], Loss: 7.1406\n",
      "Epoch [75/100], Step [6100/13958], Loss: 10.0313\n",
      "Epoch [75/100], Step [6200/13958], Loss: 6.3441\n",
      "Epoch [75/100], Step [6300/13958], Loss: 6.5316\n",
      "Epoch [75/100], Step [6400/13958], Loss: 6.9376\n",
      "Epoch [75/100], Step [6500/13958], Loss: 7.0157\n",
      "Epoch [75/100], Step [6600/13958], Loss: 5.7344\n",
      "Epoch [75/100], Step [6700/13958], Loss: 9.9375\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [76/100], Step [100/13958], Loss: 7.6563\n",
      "Epoch [76/100], Step [200/13958], Loss: 7.1251\n",
      "Epoch [76/100], Step [300/13958], Loss: 7.0625\n",
      "Epoch [76/100], Step [400/13958], Loss: 7.4688\n",
      "Epoch [76/100], Step [500/13958], Loss: 5.4221\n",
      "Epoch [76/100], Step [600/13958], Loss: 7.3906\n",
      "Epoch [76/100], Step [700/13958], Loss: 6.7813\n",
      "Epoch [76/100], Step [800/13958], Loss: 5.9532\n",
      "Epoch [76/100], Step [900/13958], Loss: 9.9531\n",
      "Epoch [76/100], Step [1000/13958], Loss: 4.4532\n",
      "Epoch [76/100], Step [1100/13958], Loss: 7.6407\n",
      "Epoch [76/100], Step [1200/13958], Loss: 5.0000\n",
      "Epoch [76/100], Step [1300/13958], Loss: 10.0625\n",
      "Epoch [76/100], Step [1400/13958], Loss: 9.8281\n",
      "Epoch [76/100], Step [1500/13958], Loss: 5.5156\n",
      "Epoch [76/100], Step [1600/13958], Loss: 8.5781\n",
      "Epoch [76/100], Step [1700/13958], Loss: 8.2813\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [77/100], Step [100/13958], Loss: 7.4376\n",
      "Epoch [77/100], Step [200/13958], Loss: 5.7969\n",
      "Epoch [77/100], Step [300/13958], Loss: 9.9531\n",
      "Epoch [77/100], Step [400/13958], Loss: 8.2500\n",
      "Epoch [77/100], Step [500/13958], Loss: 10.2188\n",
      "Epoch [77/100], Step [600/13958], Loss: 8.8750\n",
      "Epoch [77/100], Step [700/13958], Loss: 6.9063\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [78/100], Step [100/13958], Loss: 6.3126\n",
      "Epoch [78/100], Step [200/13958], Loss: 7.3907\n",
      "Epoch [78/100], Step [300/13958], Loss: 8.7969\n",
      "Epoch [78/100], Step [400/13958], Loss: 11.4063\n",
      "Epoch [78/100], Step [500/13958], Loss: 5.8751\n",
      "Epoch [78/100], Step [600/13958], Loss: 7.7969\n",
      "Epoch [78/100], Step [700/13958], Loss: 8.5626\n",
      "Epoch [78/100], Step [800/13958], Loss: 7.4063\n",
      "Epoch [78/100], Step [900/13958], Loss: 9.3751\n",
      "Epoch [78/100], Step [1000/13958], Loss: 8.1407\n",
      "Epoch [78/100], Step [1100/13958], Loss: 6.6876\n",
      "Epoch [78/100], Step [1200/13958], Loss: 11.9220\n",
      "Epoch [78/100], Step [1300/13958], Loss: 6.6876\n",
      "Epoch [78/100], Step [1400/13958], Loss: 9.2657\n",
      "Epoch [78/100], Step [1500/13958], Loss: 11.1719\n",
      "Epoch [78/100], Step [1600/13958], Loss: 7.5626\n",
      "Epoch [78/100], Step [1700/13958], Loss: 8.1720\n",
      "Epoch [78/100], Step [1800/13958], Loss: 9.4531\n",
      "Epoch [78/100], Step [1900/13958], Loss: 6.6250\n",
      "Epoch [78/100], Step [2000/13958], Loss: 10.3907\n",
      "Epoch [78/100], Step [2100/13958], Loss: 9.1095\n",
      "Epoch [78/100], Step [2200/13958], Loss: 7.4532\n",
      "Epoch [78/100], Step [2300/13958], Loss: 6.7969\n",
      "Epoch [78/100], Step [2400/13958], Loss: 5.7656\n",
      "Epoch [78/100], Step [2500/13958], Loss: 5.3750\n",
      "Epoch [78/100], Step [2600/13958], Loss: 10.8906\n",
      "Epoch [78/100], Step [2700/13958], Loss: 7.4532\n",
      "Epoch [78/100], Step [2800/13958], Loss: 8.6408\n",
      "Epoch [78/100], Step [2900/13958], Loss: 7.2188\n",
      "Epoch [78/100], Step [3000/13958], Loss: 7.6406\n",
      "Epoch [78/100], Step [3100/13958], Loss: 8.2032\n",
      "Epoch [78/100], Step [3200/13958], Loss: 11.0156\n",
      "Epoch [78/100], Step [3300/13958], Loss: 5.5313\n",
      "Epoch [78/100], Step [3400/13958], Loss: 8.7500\n",
      "Epoch [78/100], Step [3500/13958], Loss: 8.0001\n",
      "Epoch [78/100], Step [3600/13958], Loss: 5.9688\n",
      "Epoch [78/100], Step [3700/13958], Loss: 5.2500\n",
      "Epoch [78/100], Step [3800/13958], Loss: 6.7344\n",
      "Epoch [78/100], Step [3900/13958], Loss: 6.8281\n",
      "Epoch [78/100], Step [4000/13958], Loss: 5.5000\n",
      "Epoch [78/100], Step [4100/13958], Loss: 6.9063\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [79/100], Step [100/13958], Loss: 9.0781\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [80/100], Step [100/13958], Loss: 7.8907\n",
      "Epoch [80/100], Step [200/13958], Loss: 6.7813\n",
      "Epoch [80/100], Step [300/13958], Loss: 5.7813\n",
      "Epoch [80/100], Step [400/13958], Loss: 5.7812\n",
      "Epoch [80/100], Step [500/13958], Loss: 10.9533\n",
      "Epoch [80/100], Step [600/13958], Loss: 13.0156\n",
      "Epoch [80/100], Step [700/13958], Loss: 9.2813\n",
      "Epoch [80/100], Step [800/13958], Loss: 7.8750\n",
      "Epoch [80/100], Step [900/13958], Loss: 5.0000\n",
      "Epoch [80/100], Step [1000/13958], Loss: 8.3282\n",
      "Epoch [80/100], Step [1100/13958], Loss: 9.3282\n",
      "Epoch [80/100], Step [1200/13958], Loss: 10.4844\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [81/100], Step [100/13958], Loss: 9.0001\n",
      "Epoch [81/100], Step [200/13958], Loss: 9.2500\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [82/100], Step [100/13958], Loss: 9.3438\n",
      "Epoch [82/100], Step [200/13958], Loss: 6.3751\n",
      "Epoch [82/100], Step [300/13958], Loss: 6.2032\n",
      "Epoch [82/100], Step [400/13958], Loss: 6.9531\n",
      "Epoch [82/100], Step [500/13958], Loss: 9.2344\n",
      "Epoch [82/100], Step [600/13958], Loss: 7.8438\n",
      "Epoch [82/100], Step [700/13958], Loss: 9.4532\n",
      "Epoch [82/100], Step [800/13958], Loss: 8.2344\n",
      "Epoch [82/100], Step [900/13958], Loss: 7.2344\n",
      "Epoch [82/100], Step [1000/13958], Loss: 9.2344\n",
      "Epoch [82/100], Step [1100/13958], Loss: 7.6407\n",
      "Epoch [82/100], Step [1200/13958], Loss: 9.1409\n",
      "Epoch [82/100], Step [1300/13958], Loss: 6.9531\n",
      "Epoch [82/100], Step [1400/13958], Loss: 5.6563\n",
      "Epoch [82/100], Step [1500/13958], Loss: 8.9532\n",
      "Epoch [82/100], Step [1600/13958], Loss: 5.8438\n",
      "Epoch [82/100], Step [1700/13958], Loss: 9.5625\n",
      "Epoch [82/100], Step [1800/13958], Loss: 7.0156\n",
      "Epoch [82/100], Step [1900/13958], Loss: 7.2969\n",
      "Epoch [82/100], Step [2000/13958], Loss: 8.3126\n",
      "Epoch [82/100], Step [2100/13958], Loss: 6.9219\n",
      "Epoch [82/100], Step [2200/13958], Loss: 4.8125\n",
      "Epoch [82/100], Step [2300/13958], Loss: 7.0001\n",
      "Epoch [82/100], Step [2400/13958], Loss: 7.7813\n",
      "Epoch [82/100], Step [2500/13958], Loss: 6.6719\n",
      "Epoch [82/100], Step [2600/13958], Loss: 6.7657\n",
      "Epoch [82/100], Step [2700/13958], Loss: 8.2500\n",
      "Epoch [82/100], Step [2800/13958], Loss: 11.0469\n",
      "Epoch [82/100], Step [2900/13958], Loss: 7.2813\n",
      "Epoch [82/100], Step [3000/13958], Loss: 6.3438\n",
      "Epoch [82/100], Step [3100/13958], Loss: 6.8907\n",
      "Epoch [82/100], Step [3200/13958], Loss: 6.5469\n",
      "Epoch [82/100], Step [3300/13958], Loss: 7.1877\n",
      "Epoch [82/100], Step [3400/13958], Loss: 6.2189\n",
      "Epoch [82/100], Step [3500/13958], Loss: 7.7344\n",
      "Epoch [82/100], Step [3600/13958], Loss: 9.0159\n",
      "Epoch [82/100], Step [3700/13958], Loss: 6.3907\n",
      "Epoch [82/100], Step [3800/13958], Loss: 6.1875\n",
      "Epoch [82/100], Step [3900/13958], Loss: 8.3438\n",
      "Epoch [82/100], Step [4000/13958], Loss: 6.5781\n",
      "Epoch [82/100], Step [4100/13958], Loss: 5.4688\n",
      "Epoch [82/100], Step [4200/13958], Loss: 9.4688\n",
      "Epoch [82/100], Step [4300/13958], Loss: 6.7500\n",
      "Epoch [82/100], Step [4400/13958], Loss: 8.3907\n",
      "Epoch [82/100], Step [4500/13958], Loss: 8.0470\n",
      "Epoch [82/100], Step [4600/13958], Loss: 7.5625\n",
      "Epoch [82/100], Step [4700/13958], Loss: 8.5625\n",
      "Epoch [82/100], Step [4800/13958], Loss: 5.8438\n",
      "Epoch [82/100], Step [4900/13958], Loss: 11.2345\n",
      "Epoch [82/100], Step [5000/13958], Loss: 8.3907\n",
      "Epoch [82/100], Step [5100/13958], Loss: 7.9063\n",
      "Epoch [82/100], Step [5200/13958], Loss: 5.8282\n",
      "Epoch [82/100], Step [5300/13958], Loss: 8.6095\n",
      "Epoch [82/100], Step [5400/13958], Loss: 8.9532\n",
      "Epoch [82/100], Step [5500/13958], Loss: 8.2813\n",
      "Epoch [82/100], Step [5600/13958], Loss: 6.5470\n",
      "Epoch [82/100], Step [5700/13958], Loss: 7.2813\n",
      "Epoch [82/100], Step [5800/13958], Loss: 12.1876\n",
      "Epoch [82/100], Step [5900/13958], Loss: 6.8908\n",
      "Epoch [82/100], Step [6000/13958], Loss: 8.8594\n",
      "Epoch [82/100], Step [6100/13958], Loss: 8.0625\n",
      "Epoch [82/100], Step [6200/13958], Loss: 5.1875\n",
      "Epoch [82/100], Step [6300/13958], Loss: 4.5469\n",
      "Epoch [82/100], Step [6400/13958], Loss: 8.5469\n",
      "Epoch [82/100], Step [6500/13958], Loss: 11.0939\n",
      "Epoch [82/100], Step [6600/13958], Loss: 4.7347\n",
      "Epoch [82/100], Step [6700/13958], Loss: 7.0000\n",
      "Epoch [82/100], Step [6800/13958], Loss: 8.7344\n",
      "Epoch [82/100], Step [6900/13958], Loss: 9.0156\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [83/100], Step [100/13958], Loss: 8.1719\n",
      "Epoch [83/100], Step [200/13958], Loss: 4.6563\n",
      "Epoch [83/100], Step [300/13958], Loss: 5.3907\n",
      "Epoch [83/100], Step [400/13958], Loss: 10.7344\n",
      "Epoch [83/100], Step [500/13958], Loss: 8.2188\n",
      "Epoch [83/100], Step [600/13958], Loss: 7.0938\n",
      "Epoch [83/100], Step [700/13958], Loss: 7.1719\n",
      "Epoch [83/100], Step [800/13958], Loss: 7.1094\n",
      "Epoch [83/100], Step [900/13958], Loss: 6.3126\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [84/100], Step [100/13958], Loss: 5.8126\n",
      "Epoch [84/100], Step [200/13958], Loss: 8.5469\n",
      "Epoch [84/100], Step [300/13958], Loss: 9.8595\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [85/100], Step [100/13958], Loss: 7.7814\n",
      "Epoch [85/100], Step [200/13958], Loss: 6.5313\n",
      "Epoch [85/100], Step [300/13958], Loss: 5.2501\n",
      "Epoch [85/100], Step [400/13958], Loss: 9.3595\n",
      "Epoch [85/100], Step [500/13958], Loss: 5.7344\n",
      "Epoch [85/100], Step [600/13958], Loss: 8.9532\n",
      "Epoch [85/100], Step [700/13958], Loss: 7.2657\n",
      "Epoch [85/100], Step [800/13958], Loss: 6.5157\n",
      "Epoch [85/100], Step [900/13958], Loss: 6.1251\n",
      "Epoch [85/100], Step [1000/13958], Loss: 8.0157\n",
      "Epoch [85/100], Step [1100/13958], Loss: 4.5313\n",
      "Epoch [85/100], Step [1200/13958], Loss: 7.0781\n",
      "Epoch [85/100], Step [1300/13958], Loss: 11.0938\n",
      "Epoch [85/100], Step [1400/13958], Loss: 7.5157\n",
      "Epoch [85/100], Step [1500/13958], Loss: 12.4533\n",
      "Epoch [85/100], Step [1600/13958], Loss: 6.2032\n",
      "Epoch [85/100], Step [1700/13958], Loss: 6.3438\n",
      "Epoch [85/100], Step [1800/13958], Loss: 5.5938\n",
      "Epoch [85/100], Step [1900/13958], Loss: 9.3281\n",
      "Epoch [85/100], Step [2000/13958], Loss: 6.1720\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model in batches\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN loss encountered. Exiting training loop.\")\n",
    "            break\n",
    "\n",
    "        # loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/13958], Loss: 4929602887348371720746214921076736.0000\n",
      "Epoch [1/100], Step [200/13958], Loss: 4536415799710784950114404479795200.0000\n",
      "Epoch [1/100], Step [300/13958], Loss: 4842755822862325510250804310704128.0000\n",
      "Epoch [1/100], Step [400/13958], Loss: 4948193342403330097679443794329600.0000\n",
      "Epoch [1/100], Step [500/13958], Loss: 6089998981542935417915795453247488.0000\n",
      "Epoch [1/100], Step [600/13958], Loss: 6957215593143601432651428744134656.0000\n",
      "Epoch [1/100], Step [700/13958], Loss: 4160193132886555209424742912098304.0000\n",
      "Epoch [1/100], Step [800/13958], Loss: 5430571843476359767702255062482944.0000\n",
      "Epoch [1/100], Step [900/13958], Loss: 5139074721395850917061895731019776.0000\n",
      "Epoch [1/100], Step [1000/13958], Loss: 4718732181056500044719898100760576.0000\n",
      "Epoch [1/100], Step [1100/13958], Loss: 5512288265469587719648346252509184.0000\n",
      "Epoch [1/100], Step [1200/13958], Loss: 4783713201143638762074696755183616.0000\n",
      "Epoch [1/100], Step [1300/13958], Loss: 3356536076722918474143255356243968.0000\n",
      "Epoch [1/100], Step [1400/13958], Loss: 5049847717184278562987991554326528.0000\n",
      "Epoch [1/100], Step [1500/13958], Loss: 4345229268498580284494189908983808.0000\n",
      "Epoch [1/100], Step [1600/13958], Loss: 5922817655117523382551218525044736.0000\n",
      "Epoch [1/100], Step [1700/13958], Loss: 4344290290978782323555678923259904.0000\n",
      "Epoch [1/100], Step [1800/13958], Loss: 5645041241462434044187703636918272.0000\n",
      "Epoch [1/100], Step [1900/13958], Loss: 5118570101555147520878604086935552.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [1/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [2/100], Step [100/13958], Loss: 4550655205012665036726431656181760.0000\n",
      "Epoch [2/100], Step [200/13958], Loss: 3952187948300619367389748839579648.0000\n",
      "Epoch [2/100], Step [300/13958], Loss: 5208495303948876829427551369691136.0000\n",
      "Epoch [2/100], Step [400/13958], Loss: 4010471712775224177457343231950848.0000\n",
      "Epoch [2/100], Step [500/13958], Loss: 5365796630920752244818158387462144.0000\n",
      "Epoch [2/100], Step [600/13958], Loss: 6110285104966704944480567401906176.0000\n",
      "Epoch [2/100], Step [700/13958], Loss: 4153723348756239990763051364122624.0000\n",
      "Epoch [2/100], Step [800/13958], Loss: 4254395108631005685478397944266752.0000\n",
      "Epoch [2/100], Step [900/13958], Loss: 4944780651200030125606615633625088.0000\n",
      "Epoch [2/100], Step [1000/13958], Loss: 3823772095235409372643500536889344.0000\n",
      "Epoch [2/100], Step [1100/13958], Loss: 4420353969267623407821511987298304.0000\n",
      "Epoch [2/100], Step [1200/13958], Loss: 3239408379905932219433674717790208.0000\n",
      "Epoch [2/100], Step [1300/13958], Loss: 6246295720172930747213321831710720.0000\n",
      "Epoch [2/100], Step [1400/13958], Loss: 5488625041618647675693649492967424.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [2/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [3/100], Step [100/13958], Loss: 4020062034259568018446986747314176.0000\n",
      "Epoch [3/100], Step [200/13958], Loss: 3458547597205200771661111513579520.0000\n",
      "Epoch [3/100], Step [300/13958], Loss: 5224917816510036684154363151646720.0000\n",
      "Epoch [3/100], Step [400/13958], Loss: 4178413443869767257655776947208192.0000\n",
      "Epoch [3/100], Step [500/13958], Loss: 4399004764835117560945670415712256.0000\n",
      "Epoch [3/100], Step [600/13958], Loss: 4188530508840827027952390039928832.0000\n",
      "Epoch [3/100], Step [700/13958], Loss: 4046183497543519006282564442783744.0000\n",
      "Epoch [3/100], Step [800/13958], Loss: 4948849141139141527880071605387264.0000\n",
      "Epoch [3/100], Step [900/13958], Loss: 5639094177513707077347088244146176.0000\n",
      "Epoch [3/100], Step [1000/13958], Loss: 5289446678997826416363754900946944.0000\n",
      "Epoch [3/100], Step [1100/13958], Loss: 4926125513778019087554023281131520.0000\n",
      "Epoch [3/100], Step [1200/13958], Loss: 3487910606482020527746512565829632.0000\n",
      "Epoch [3/100], Step [1300/13958], Loss: 5928700346184207509617539163357184.0000\n",
      "Epoch [3/100], Step [1400/13958], Loss: 4349413815316374691168417673641984.0000\n",
      "Epoch [3/100], Step [1500/13958], Loss: 4271136081267271704280927525928960.0000\n",
      "Epoch [3/100], Step [1600/13958], Loss: 5583094102926573972541614011187200.0000\n",
      "Epoch [3/100], Step [1700/13958], Loss: 4600800442183997935521770481123328.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [3/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [4/100], Step [100/13958], Loss: 4683767183616904121545509786615808.0000\n",
      "Epoch [4/100], Step [200/13958], Loss: 5358455646487789939788006580813824.0000\n",
      "Epoch [4/100], Step [300/13958], Loss: 4792228681188873071640659105939456.0000\n",
      "Epoch [4/100], Step [400/13958], Loss: 4176510111059365985483119543713792.0000\n",
      "Epoch [4/100], Step [500/13958], Loss: 2827500853509262106939770995212288.0000\n",
      "Epoch [4/100], Step [600/13958], Loss: 6423634963590698970423583523733504.0000\n",
      "Epoch [4/100], Step [700/13958], Loss: 4762955113079891684280119515414528.0000\n",
      "Epoch [4/100], Step [800/13958], Loss: 4207642446137334371326420169261056.0000\n",
      "Epoch [4/100], Step [900/13958], Loss: 6058209300304106495146523392737280.0000\n",
      "Epoch [4/100], Step [1000/13958], Loss: 5559237141459485766573895538704384.0000\n",
      "Epoch [4/100], Step [1100/13958], Loss: 5461488158017472854687585790853120.0000\n",
      "Epoch [4/100], Step [1200/13958], Loss: 5913918104175100783754968720998400.0000\n",
      "Epoch [4/100], Step [1300/13958], Loss: 3492808825732462956149219675602944.0000\n",
      "Epoch [4/100], Step [1400/13958], Loss: 5938318521319435271663367909015552.0000\n",
      "Epoch [4/100], Step [1500/13958], Loss: 4992856981080707332617529297207296.0000\n",
      "Epoch [4/100], Step [1600/13958], Loss: 4937053429474810781930695300218880.0000\n",
      "Epoch [4/100], Step [1700/13958], Loss: 3461906438016791829691981562380288.0000\n",
      "Epoch [4/100], Step [1800/13958], Loss: 4401491476889032068572874031497216.0000\n",
      "Epoch [4/100], Step [1900/13958], Loss: 7203412156396520720202266973306880.0000\n",
      "Epoch [4/100], Step [2000/13958], Loss: 4322873618814135423454855349403648.0000\n",
      "Epoch [4/100], Step [2100/13958], Loss: 4698005969948764565467399513440256.0000\n",
      "Epoch [4/100], Step [2200/13958], Loss: 4873363580848646716202616432361472.0000\n",
      "Epoch [4/100], Step [2300/13958], Loss: 5035988359474459088120358409076736.0000\n",
      "Epoch [4/100], Step [2400/13958], Loss: 3211567727392423659741330863554560.0000\n",
      "Epoch [4/100], Step [2500/13958], Loss: 3637584984871858715263466079256576.0000\n",
      "Epoch [4/100], Step [2600/13958], Loss: 3952381066946747886712633102958592.0000\n",
      "Epoch [4/100], Step [2700/13958], Loss: 5731612864289739257571949192151040.0000\n",
      "Epoch [4/100], Step [2800/13958], Loss: 4666336678378756145929861392760832.0000\n",
      "Epoch [4/100], Step [2900/13958], Loss: 4749579480440422971754903202955264.0000\n",
      "Epoch [4/100], Step [3000/13958], Loss: 3972216580196217534857341770399744.0000\n",
      "Epoch [4/100], Step [3100/13958], Loss: 4273354779302680927078615481319424.0000\n",
      "Epoch [4/100], Step [3200/13958], Loss: 4578363397941970060729361304125440.0000\n",
      "Epoch [4/100], Step [3300/13958], Loss: 4701932715753377791699379535478784.0000\n",
      "Epoch [4/100], Step [3400/13958], Loss: 5323546975319981501416126176821248.0000\n",
      "Epoch [4/100], Step [3500/13958], Loss: 3765757892204358954130765428817920.0000\n",
      "Epoch [4/100], Step [3600/13958], Loss: 3961799314765631059844065640054784.0000\n",
      "Epoch [4/100], Step [3700/13958], Loss: 5571684628554500265238006232776704.0000\n",
      "Epoch [4/100], Step [3800/13958], Loss: 4833712051905326164652528758685696.0000\n",
      "Epoch [4/100], Step [3900/13958], Loss: 4599370930923633142649330717425664.0000\n",
      "Epoch [4/100], Step [4000/13958], Loss: 4350421498508352990712185560760320.0000\n",
      "Epoch [4/100], Step [4100/13958], Loss: 5148177913474735960913366441000960.0000\n",
      "Epoch [4/100], Step [4200/13958], Loss: 2906443670844471164379950697414656.0000\n",
      "Epoch [4/100], Step [4300/13958], Loss: 4678889699862119723262407237173248.0000\n",
      "Epoch [4/100], Step [4400/13958], Loss: 4137815819251422854230597617844224.0000\n",
      "Epoch [4/100], Step [4500/13958], Loss: 4548013440968830035219796925087744.0000\n",
      "Epoch [4/100], Step [4600/13958], Loss: 5344469399923943713442196275331072.0000\n",
      "Epoch [4/100], Step [4700/13958], Loss: 4660868387740222799910563236282368.0000\n",
      "Epoch [4/100], Step [4800/13958], Loss: 5058556315875641391876838168461312.0000\n",
      "Epoch [4/100], Step [4900/13958], Loss: 6234740168876221365037275956641792.0000\n",
      "Epoch [4/100], Step [5000/13958], Loss: 5951354648903129968648193136656384.0000\n",
      "Epoch [4/100], Step [5100/13958], Loss: 5159056621054966061424111219900416.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [4/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [5/100], Step [100/13958], Loss: 4577474866478772979037052457713664.0000\n",
      "Epoch [5/100], Step [200/13958], Loss: 4017070242669625075667624288845824.0000\n",
      "Epoch [5/100], Step [300/13958], Loss: 4663235329095336446996170361798656.0000\n",
      "Epoch [5/100], Step [400/13958], Loss: 5754216411467051016011332301357056.0000\n",
      "Epoch [5/100], Step [500/13958], Loss: 3646260778152180481575027866599424.0000\n",
      "Epoch [5/100], Step [600/13958], Loss: 3868580572897382997073749237301248.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [5/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [6/100], Step [100/13958], Loss: 6068634612344948325131586367389696.0000\n",
      "Epoch [6/100], Step [200/13958], Loss: 6214732581961291049034356310933504.0000\n",
      "Epoch [6/100], Step [300/13958], Loss: 5037757685275607717878257982373888.0000\n",
      "Epoch [6/100], Step [400/13958], Loss: 3519988418264993122774767397502976.0000\n",
      "Epoch [6/100], Step [500/13958], Loss: 3231299563163603157288016729341952.0000\n",
      "Epoch [6/100], Step [600/13958], Loss: 4618009356155113828068211931742208.0000\n",
      "Epoch [6/100], Step [700/13958], Loss: 5067740593027099628136314771079168.0000\n",
      "Epoch [6/100], Step [800/13958], Loss: 5929405972006600176374231664164864.0000\n",
      "Epoch [6/100], Step [900/13958], Loss: 6492200748546598327709421317128192.0000\n",
      "Epoch [6/100], Step [1000/13958], Loss: 5227880207024046599152196755914752.0000\n",
      "Epoch [6/100], Step [1100/13958], Loss: 3761218985050319107352847789850624.0000\n",
      "Epoch [6/100], Step [1200/13958], Loss: 5500095794022666009320864778027008.0000\n",
      "Epoch [6/100], Step [1300/13958], Loss: 4393752494733439513784342156410880.0000\n",
      "Epoch [6/100], Step [1400/13958], Loss: 5133317681243154256093477353816064.0000\n",
      "Epoch [6/100], Step [1500/13958], Loss: 5714468632685676026144871220772864.0000\n",
      "Epoch [6/100], Step [1600/13958], Loss: 6064636066018056546843662196146176.0000\n",
      "Epoch [6/100], Step [1700/13958], Loss: 4330939417140099318635960593285120.0000\n",
      "Epoch [6/100], Step [1800/13958], Loss: 3308843508254419735017560421171200.0000\n",
      "Epoch [6/100], Step [1900/13958], Loss: 5917697535115039049734235747254272.0000\n",
      "Epoch [6/100], Step [2000/13958], Loss: 5043454375851389216558275027271680.0000\n",
      "Epoch [6/100], Step [2100/13958], Loss: 3999368319077863780426704262004736.0000\n",
      "Epoch [6/100], Step [2200/13958], Loss: 4437223687667975106172631077879808.0000\n",
      "Epoch [6/100], Step [2300/13958], Loss: 4798379695759072304881564129427456.0000\n",
      "Epoch [6/100], Step [2400/13958], Loss: 4406233406209514717715875126837248.0000\n",
      "Epoch [6/100], Step [2500/13958], Loss: 4279565524479775679917784387551232.0000\n",
      "Epoch [6/100], Step [2600/13958], Loss: 5750776795067896586917525084700672.0000\n",
      "Epoch [6/100], Step [2700/13958], Loss: 5968925350850727013579973856329728.0000\n",
      "Epoch [6/100], Step [2800/13958], Loss: 5109954967336750738200512356679680.0000\n",
      "Epoch [6/100], Step [2900/13958], Loss: 4158660872602929729989486521090048.0000\n",
      "Epoch [6/100], Step [3000/13958], Loss: 5514050473115510458469665155842048.0000\n",
      "Epoch [6/100], Step [3100/13958], Loss: 4911778407692721172858079880937472.0000\n",
      "Epoch [6/100], Step [3200/13958], Loss: 4461037630718698145175796805795840.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [6/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [7/100], Step [100/13958], Loss: 4713749781883386210458497850540032.0000\n",
      "Epoch [7/100], Step [200/13958], Loss: 4039233083192951238729143309828096.0000\n",
      "Epoch [7/100], Step [300/13958], Loss: 4736702118666766624790333787996160.0000\n",
      "Epoch [7/100], Step [400/13958], Loss: 4177114225798537251057270316335104.0000\n",
      "Epoch [7/100], Step [500/13958], Loss: 6142798362158496172020518000525312.0000\n",
      "Epoch [7/100], Step [600/13958], Loss: 4091563284533622833709679089025024.0000\n",
      "Epoch [7/100], Step [700/13958], Loss: 4088268507119064794108035069902848.0000\n",
      "Epoch [7/100], Step [800/13958], Loss: 5826847591511963562119938818703360.0000\n",
      "Epoch [7/100], Step [900/13958], Loss: 6811312599083446156141955412656128.0000\n",
      "Epoch [7/100], Step [1000/13958], Loss: 5049729493910526809171738687963136.0000\n",
      "Epoch [7/100], Step [1100/13958], Loss: 6140589877128411053610097962909696.0000\n",
      "Epoch [7/100], Step [1200/13958], Loss: 4842348231104390798795293774053376.0000\n",
      "Epoch [7/100], Step [1300/13958], Loss: 5710093752586841492253377715765248.0000\n",
      "Epoch [7/100], Step [1400/13958], Loss: 4336033230916748837122101764685824.0000\n",
      "Epoch [7/100], Step [1500/13958], Loss: 5525729818416148378673200943333376.0000\n",
      "Epoch [7/100], Step [1600/13958], Loss: 5042726157623279591611565617446912.0000\n",
      "Epoch [7/100], Step [1700/13958], Loss: 4132678987058408168779903701876736.0000\n",
      "Epoch [7/100], Step [1800/13958], Loss: 5848372273945038111649747341148160.0000\n",
      "Epoch [7/100], Step [1900/13958], Loss: 6850425933594687388617530692075520.0000\n",
      "Epoch [7/100], Step [2000/13958], Loss: 5661240305846502887774896126951424.0000\n",
      "Epoch [7/100], Step [2100/13958], Loss: 5021417495727060340939726992179200.0000\n",
      "Epoch [7/100], Step [2200/13958], Loss: 5523627796229441802966422230401024.0000\n",
      "Epoch [7/100], Step [2300/13958], Loss: 5076440835623197280708305816125440.0000\n",
      "Epoch [7/100], Step [2400/13958], Loss: 4303857312385662875707061177417728.0000\n",
      "Epoch [7/100], Step [2500/13958], Loss: 4254075101130850414677336520654848.0000\n",
      "Epoch [7/100], Step [2600/13958], Loss: 3314190790254112935114987188256768.0000\n",
      "Epoch [7/100], Step [2700/13958], Loss: 4574544971890794305271434955456512.0000\n",
      "Epoch [7/100], Step [2800/13958], Loss: 5376538236641631489463458088353792.0000\n",
      "Epoch [7/100], Step [2900/13958], Loss: 5648332305056874227648522958667776.0000\n",
      "Epoch [7/100], Step [3000/13958], Loss: 4779536081966080067682118385270784.0000\n",
      "Epoch [7/100], Step [3100/13958], Loss: 3045393155703960459885315648126976.0000\n",
      "Epoch [7/100], Step [3200/13958], Loss: 5074704005748079892182622344839168.0000\n",
      "Epoch [7/100], Step [3300/13958], Loss: 6085314616434279538955577167183872.0000\n",
      "Epoch [7/100], Step [3400/13958], Loss: 4501660014137828256206474117644288.0000\n",
      "Epoch [7/100], Step [3500/13958], Loss: 4385234229323116812112761282625536.0000\n",
      "Epoch [7/100], Step [3600/13958], Loss: 4417501136447090248978006955524096.0000\n",
      "Epoch [7/100], Step [3700/13958], Loss: 5765473000244272978850990037925888.0000\n",
      "Epoch [7/100], Step [3800/13958], Loss: 7398669962912925180201556038909952.0000\n",
      "Epoch [7/100], Step [3900/13958], Loss: 4625475063047034135161059825156096.0000\n",
      "Epoch [7/100], Step [4000/13958], Loss: 6490322793507002405832399345680384.0000\n",
      "Epoch [7/100], Step [4100/13958], Loss: 5577932511932773579485422112735232.0000\n",
      "Epoch [7/100], Step [4200/13958], Loss: 3849286658415100702799308936708096.0000\n",
      "Epoch [7/100], Step [4300/13958], Loss: 5903957638619010614063130367492096.0000\n",
      "Epoch [7/100], Step [4400/13958], Loss: 3667821360846394307132808463646720.0000\n",
      "Epoch [7/100], Step [4500/13958], Loss: 4379373821177139821891388828549120.0000\n",
      "Epoch [7/100], Step [4600/13958], Loss: 4036280905684265428118577623334912.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [7/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [8/100], Step [100/13958], Loss: 3895298413795259716856759585865728.0000\n",
      "Epoch [8/100], Step [200/13958], Loss: 6298297248433172073730871893426176.0000\n",
      "Epoch [8/100], Step [300/13958], Loss: 3671827953783541440392519479197696.0000\n",
      "Epoch [8/100], Step [400/13958], Loss: 6281476119179362326555542593470464.0000\n",
      "Epoch [8/100], Step [500/13958], Loss: 5921534530266804085896285582786560.0000\n",
      "Epoch [8/100], Step [600/13958], Loss: 3598393969623152325175640879792128.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [8/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [9/100], Step [100/13958], Loss: 4747105457271911139275517303193600.0000\n",
      "Epoch [9/100], Step [200/13958], Loss: 3909515845661442487968907302797312.0000\n",
      "Epoch [9/100], Step [300/13958], Loss: 5074500055126607625782332714123264.0000\n",
      "Epoch [9/100], Step [400/13958], Loss: 5618150707929077013856344860524544.0000\n",
      "Epoch [9/100], Step [500/13958], Loss: 5246097113672150612587474818433024.0000\n",
      "Epoch [9/100], Step [600/13958], Loss: 4400947092756756322596987141619712.0000\n",
      "Epoch [9/100], Step [700/13958], Loss: 4240727631627275444553374163271680.0000\n",
      "Epoch [9/100], Step [800/13958], Loss: 3712534517125342957281889931493376.0000\n",
      "Epoch [9/100], Step [900/13958], Loss: 3878895708274728428214346189897728.0000\n",
      "Epoch [9/100], Step [1000/13958], Loss: 3990564089518466155911621690523648.0000\n",
      "Epoch [9/100], Step [1100/13958], Loss: 4730214384405881768114656202719232.0000\n",
      "Epoch [9/100], Step [1200/13958], Loss: 4365332486281545396123345512038400.0000\n",
      "Epoch [9/100], Step [1300/13958], Loss: 5425589753788255754785923537043456.0000\n",
      "Epoch [9/100], Step [1400/13958], Loss: 4737608909745543165841697396490240.0000\n",
      "Epoch [9/100], Step [1500/13958], Loss: 6346164675932219872820396329795584.0000\n",
      "Epoch [9/100], Step [1600/13958], Loss: 5703939952651553866906854169247744.0000\n",
      "Epoch [9/100], Step [1700/13958], Loss: 5587818701086506626360766518788096.0000\n",
      "Epoch [9/100], Step [1800/13958], Loss: 4791246066282690301047457926086656.0000\n",
      "Epoch [9/100], Step [1900/13958], Loss: 5103204480302527559561487432286208.0000\n",
      "Epoch [9/100], Step [2000/13958], Loss: 5225388852694984771348962268413952.0000\n",
      "Epoch [9/100], Step [2100/13958], Loss: 5808945121633838035274485133737984.0000\n",
      "Epoch [9/100], Step [2200/13958], Loss: 4096549088041844702766835311837184.0000\n",
      "Epoch [9/100], Step [2300/13958], Loss: 3920742104907701958991830010822656.0000\n",
      "Epoch [9/100], Step [2400/13958], Loss: 3816685197995510391914771775488000.0000\n",
      "Epoch [9/100], Step [2500/13958], Loss: 5672806998603565838373416094138368.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [9/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [10/100], Step [100/13958], Loss: 5046766174941487430138698909351936.0000\n",
      "Epoch [10/100], Step [200/13958], Loss: 4115330185862853028262398650220544.0000\n",
      "Epoch [10/100], Step [300/13958], Loss: 4073589014133218754808281254854656.0000\n",
      "Epoch [10/100], Step [400/13958], Loss: 6153124020026175528893451595677696.0000\n",
      "Epoch [10/100], Step [500/13958], Loss: 4505553026076370955825963138547712.0000\n",
      "Epoch [10/100], Step [600/13958], Loss: 3725928718865400950511229730816000.0000\n",
      "Epoch [10/100], Step [700/13958], Loss: 4855306677950620338167869081649152.0000\n",
      "Epoch [10/100], Step [800/13958], Loss: 5713983979160295799767248213639168.0000\n",
      "Epoch [10/100], Step [900/13958], Loss: 5308353737217832029302289225220096.0000\n",
      "Epoch [10/100], Step [1000/13958], Loss: 3426998385819003034010238607949824.0000\n",
      "Epoch [10/100], Step [1100/13958], Loss: 3042581793874743361281019737014272.0000\n",
      "Epoch [10/100], Step [1200/13958], Loss: 2956067735259264918079556991057920.0000\n",
      "Epoch [10/100], Step [1300/13958], Loss: 4763598532415310260677998335229952.0000\n",
      "Epoch [10/100], Step [1400/13958], Loss: 5239674061778318417031160712396800.0000\n",
      "Epoch [10/100], Step [1500/13958], Loss: 5778926932591226491678593719992320.0000\n",
      "Epoch [10/100], Step [1600/13958], Loss: 4691897664309920677845978509737984.0000\n",
      "Epoch [10/100], Step [1700/13958], Loss: 3642593071300787721165570486304768.0000\n",
      "Epoch [10/100], Step [1800/13958], Loss: 4699244838443079409777504812007424.0000\n",
      "Epoch [10/100], Step [1900/13958], Loss: 4729520519013862312470575243591680.0000\n",
      "Epoch [10/100], Step [2000/13958], Loss: 3466774946706291408968091093172224.0000\n",
      "Epoch [10/100], Step [2100/13958], Loss: 4681366817880729769192480384745472.0000\n",
      "Epoch [10/100], Step [2200/13958], Loss: 4988679242933128995534813477732352.0000\n",
      "Epoch [10/100], Step [2300/13958], Loss: 4431185325641350842536741874696192.0000\n",
      "Epoch [10/100], Step [2400/13958], Loss: 4103524880163217820615891876839424.0000\n",
      "Epoch [10/100], Step [2500/13958], Loss: 4294574928486091273060798819205120.0000\n",
      "Epoch [10/100], Step [2600/13958], Loss: 4771294186669527827156908740968448.0000\n",
      "Epoch [10/100], Step [2700/13958], Loss: 5588580653180686777919966929747968.0000\n",
      "Epoch [10/100], Step [2800/13958], Loss: 4895742441908828178122105350520832.0000\n",
      "Epoch [10/100], Step [2900/13958], Loss: 4527362744203490964164067184345088.0000\n",
      "Epoch [10/100], Step [3000/13958], Loss: 5276254570969181761464292383653888.0000\n",
      "Epoch [10/100], Step [3100/13958], Loss: 4484210011344061535851496062582784.0000\n",
      "Epoch [10/100], Step [3200/13958], Loss: 4669676331119738280566470505136128.0000\n",
      "Epoch [10/100], Step [3300/13958], Loss: 3726185281938542845573202574311424.0000\n",
      "Epoch [10/100], Step [3400/13958], Loss: 4620695376555353281919674306527232.0000\n",
      "Epoch [10/100], Step [3500/13958], Loss: 4256998496533622840196510802509824.0000\n",
      "Epoch [10/100], Step [3600/13958], Loss: 4662802669051606206590093117882368.0000\n",
      "Epoch [10/100], Step [3700/13958], Loss: 4315519017040740979241679652388864.0000\n",
      "Epoch [10/100], Step [3800/13958], Loss: 4818659938967655225890030307246080.0000\n",
      "Epoch [10/100], Step [3900/13958], Loss: 5821139449990818673672378956906496.0000\n",
      "Epoch [10/100], Step [4000/13958], Loss: 6571089715490098471107642907426816.0000\n",
      "Epoch [10/100], Step [4100/13958], Loss: 4757786094445855578942278222217216.0000\n",
      "Epoch [10/100], Step [4200/13958], Loss: 5217051945500417377887654116327424.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [10/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [11/100], Step [100/13958], Loss: 4347665225010884091530122660675584.0000\n",
      "Epoch [11/100], Step [200/13958], Loss: 3352674941740387373065844987789312.0000\n",
      "Epoch [11/100], Step [300/13958], Loss: 3370279068069045123265047983816704.0000\n",
      "Epoch [11/100], Step [400/13958], Loss: 4903603670643300164212783514124288.0000\n",
      "Epoch [11/100], Step [500/13958], Loss: 4196508722909011482479046170771456.0000\n",
      "Epoch [11/100], Step [600/13958], Loss: 3304283556119712036774969497092096.0000\n",
      "Epoch [11/100], Step [700/13958], Loss: 5102335446394949222608508247080960.0000\n",
      "Epoch [11/100], Step [800/13958], Loss: 4095974064893596643629144668635136.0000\n",
      "Epoch [11/100], Step [900/13958], Loss: 3665552216754384205088918368944128.0000\n",
      "Epoch [11/100], Step [1000/13958], Loss: 3900769799283891276326744990154752.0000\n",
      "Epoch [11/100], Step [1100/13958], Loss: 7102141233542740901433869217038336.0000\n",
      "Epoch [11/100], Step [1200/13958], Loss: 3851275409088212666210934379773952.0000\n",
      "Epoch [11/100], Step [1300/13958], Loss: 4655403501436797488687021052395520.0000\n",
      "Epoch [11/100], Step [1400/13958], Loss: 5796260569021300386287731257376768.0000\n",
      "Epoch [11/100], Step [1500/13958], Loss: 5844963606046865817062812602597376.0000\n",
      "Epoch [11/100], Step [1600/13958], Loss: 4752442526266280234985676152504320.0000\n",
      "Epoch [11/100], Step [1700/13958], Loss: 5917149127677635626272455435223040.0000\n",
      "Epoch [11/100], Step [1800/13958], Loss: 4949941932708820697317738807296000.0000\n",
      "Epoch [11/100], Step [1900/13958], Loss: 5015450934222714629359781938200576.0000\n",
      "Epoch [11/100], Step [2000/13958], Loss: 4471574976333095302075738151190528.0000\n",
      "Epoch [11/100], Step [2100/13958], Loss: 3480527841572732341132082913738752.0000\n",
      "Epoch [11/100], Step [2200/13958], Loss: 5147199631358690689151127408082944.0000\n",
      "Epoch [11/100], Step [2300/13958], Loss: 5375347957293858596329142580412416.0000\n",
      "Epoch [11/100], Step [2400/13958], Loss: 5648630029636322361604636198043648.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [11/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [12/100], Step [100/13958], Loss: 5130574096631088032059232169754624.0000\n",
      "Epoch [12/100], Step [200/13958], Loss: 5840081170531924277258610456657920.0000\n",
      "Epoch [12/100], Step [300/13958], Loss: 5679865732707571076700890900463616.0000\n",
      "Epoch [12/100], Step [400/13958], Loss: 5042905349443966150406357265678336.0000\n",
      "Epoch [12/100], Step [500/13958], Loss: 3828549924817031297814473706831872.0000\n",
      "Epoch [12/100], Step [600/13958], Loss: 4301399691922671574516317691052032.0000\n",
      "Epoch [12/100], Step [700/13958], Loss: 5139365018335063338736359575650304.0000\n",
      "Epoch [12/100], Step [800/13958], Loss: 4756367724645844354492312550637568.0000\n",
      "Epoch [12/100], Step [900/13958], Loss: 4329396943851149734813436284502016.0000\n",
      "Epoch [12/100], Step [1000/13958], Loss: 4765993946391327471509928140603392.0000\n",
      "Epoch [12/100], Step [1100/13958], Loss: 4276596944265569338018576287662080.0000\n",
      "Epoch [12/100], Step [1200/13958], Loss: 6350017145334475976235882404380672.0000\n",
      "Epoch [12/100], Step [1300/13958], Loss: 5566903704122780126616345815023616.0000\n",
      "Epoch [12/100], Step [1400/13958], Loss: 2973056605388397834282135122345984.0000\n",
      "Epoch [12/100], Step [1500/13958], Loss: 4165836592040647436752939294654464.0000\n",
      "Epoch [12/100], Step [1600/13958], Loss: 4568188768759083520249965402128384.0000\n",
      "Epoch [12/100], Step [1700/13958], Loss: 4612753372233317924766058975068160.0000\n",
      "Epoch [12/100], Step [1800/13958], Loss: 6112263952119502624849993651978240.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [12/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [13/100], Step [100/13958], Loss: 5914758665461775556961625226346496.0000\n",
      "Epoch [13/100], Step [200/13958], Loss: 4123414243804396382802558656184320.0000\n",
      "Epoch [13/100], Step [300/13958], Loss: 6393776468813155240883154097012736.0000\n",
      "Epoch [13/100], Step [400/13958], Loss: 4625998402198642029672273429921792.0000\n",
      "Epoch [13/100], Step [500/13958], Loss: 4966474621933476950889016611307520.0000\n",
      "Epoch [13/100], Step [600/13958], Loss: 4491607322048811325684155779383296.0000\n",
      "Epoch [13/100], Step [700/13958], Loss: 3620357501800153542012901141774336.0000\n",
      "Epoch [13/100], Step [800/13958], Loss: 4161387435539455780044951842193408.0000\n",
      "Epoch [13/100], Step [900/13958], Loss: 4963385652050450105758074571587584.0000\n",
      "Epoch [13/100], Step [1000/13958], Loss: 5777208671816698383857033735569408.0000\n",
      "Epoch [13/100], Step [1100/13958], Loss: 5310831474206461717922499822354432.0000\n",
      "NaN loss encountered. Exiting training loop.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Performance Tracking\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Validation DataLoader: 'val_loader'\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Reset optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        train_loss = loss_fn(outputs, targets)\n",
    "\n",
    "        if torch.isnan(train_loss):\n",
    "            print(\"NaN loss encountered. Exiting training loop.\")\n",
    "            break\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record training loss\n",
    "        training_losses.append(train_loss.item())  \n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {train_loss.item():.4f}')\n",
    "\n",
    "    # Validation Loop\n",
    "    with torch.no_grad():  # No gradient calculations during validation\n",
    "        val_loss = 0.0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += loss_fn(outputs, targets).item() \n",
    "        val_loss /= len(val_loader)\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualization\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     y_pred = model(X)\n",
    "#     loss = loss_fn(y_pred, y.view(-1, 1))\n",
    "#     print(f'Loss: {loss.item():.4f}')\n",
    "    \n",
    "#     # Print the first 5 predictions\n",
    "#     y_pred = y_pred.cpu()\n",
    "#     y = y.cpu()\n",
    "#     print(y_pred[:5].numpy().flatten())\n",
    "#     print(y[:5].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
