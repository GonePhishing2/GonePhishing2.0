{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML libraries\n",
    "import pycaret\n",
    "import xgboost\n",
    "\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.functions import datediff, to_date, col, expr\n",
    "\n",
    "# Import Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "# from pycaret.classification import setup, compare_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open(\"connection.json\"))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PURCHASE_DOCUMENT_ITEM_ID\"  |\"CREATE_DATE\"  |\"COMPANY_CODE_ID\"  |\"VENDOR_ID\"  |\"POSTAL_CD\"  |\"MATERIAL_ID\"  |\"SUB_COMMODITY_DESC\"                    |\"MRP_TYPE_ID\"  |\"PLANT_ID\"  |\"REQUESTED_DELIVERY_DATE\"  |\"INBOUND_DELIVERY_ID\"  |\"INBOUND_DELIVERY_ITEM_ID\"  |\"PLANNED_DELIVERY_DAYS\"  |\"FIRST_GR_POSTING_DATE\"  |\"TARGET_FEATURE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|10                           |20210210       |US39               |8010000984   |60045-5202   |NULL           |Tools                                   |NULL           |4120        |20210215                   |NULL                   |0                           |0.0                      |20210211                 |-4                |\n",
      "|20                           |20210210       |US10               |8010000984   |60045-5202   |NULL           |Machinery & Equipment                   |NULL           |4014        |20210214                   |185520728              |20                          |1.0                      |20210212                 |-2                |\n",
      "|20                           |20210210       |CA10               |V4138        |19973        |2100032775     |Tolling                                 |1              |4007        |20210216                   |185529650              |10                          |2.0                      |20210216                 |0                 |\n",
      "|10                           |20210210       |CA10               |8010002419   |H9J 4A1      |1100119629     |Valves                                  |1              |4036        |20210224                   |NULL                   |0                           |14.0                     |20210406                 |41                |\n",
      "|30                           |20210210       |CA10               |8010002454   |K7L 4Y5      |3100006053     |Tubes & Cores                           |1              |4036        |20210209                   |NULL                   |0                           |1.0                      |20210209                 |0                 |\n",
      "|10                           |20210210       |US10               |NULL         |NULL         |2100013224     |Additives, Colorants & Catalysts        |1              |4014        |20210211                   |185519500              |900002                      |1.0                      |20210216                 |5                 |\n",
      "|10                           |20210210       |US40               |8010099572   |53821        |NULL           |Transportation, Storage, Mail Services  |NULL           |4138        |20211231                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010005511   |77630-1818   |NULL           |Pumps & Compressors                     |NULL           |4120        |20210210                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010001167   |77705-1129   |NULL           |Maintenance Services                    |NULL           |4120        |20211129                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010004468   |77507        |1100162838     |Piping & Tubing                         |1              |4120        |20210216                   |185562735              |10                          |6.0                      |20210305                 |17                |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PURCHASE_DOCUMENT_ITEM_ID\"  |\"CREATE_DATE\"  |\"COMPANY_CODE_ID\"  |\"VENDOR_ID\"  |\"POSTAL_CD\"  |\"MATERIAL_ID\"  |\"SUB_COMMODITY_DESC\"                    |\"MRP_TYPE_ID\"  |\"PLANT_ID\"  |\"REQUESTED_DELIVERY_DATE\"  |\"INBOUND_DELIVERY_ID\"  |\"INBOUND_DELIVERY_ITEM_ID\"  |\"PLANNED_DELIVERY_DAYS\"  |\"FIRST_GR_POSTING_DATE\"  |\"TARGET_FEATURE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|30                           |20210703       |CA10               |NULL         |NULL         |2100032311     |Tolling                                 |1              |4007        |20210703                   |185821762              |30                          |0.0                      |20210703                 |0                 |\n",
      "|30                           |20210703       |CA10               |NULL         |NULL         |2100032311     |Tolling                                 |1              |4007        |20210705                   |185821762              |30                          |0.0                      |20210703                 |-2                |\n",
      "|20                           |20210703       |GB10               |8010023829   |59229        |1100197903     |Spinning                                |1              |3009        |20210724                   |NULL                   |0                           |30.0                     |20210716                 |-8                |\n",
      "|10                           |20210703       |GB10               |NULL         |NULL         |2100015291     |Resins & Polymers                       |2              |3024        |20210705                   |185822217              |10                          |0.0                      |20210703                 |-2                |\n",
      "|10                           |20210703       |GB10               |8010023829   |59229        |1100197899     |Spinning                                |1              |3009        |20210714                   |NULL                   |0                           |28.0                     |20210716                 |2                 |\n",
      "|50                           |20210703       |CA10               |NULL         |NULL         |2100011410     |Tolling                                 |1              |4007        |20210706                   |185823356              |50                          |0.0                      |20210704                 |-2                |\n",
      "|10                           |20210703       |CA10               |8010001620   |M5W 1P1      |NULL           |Transportation, Storage, Mail Services  |NULL           |4036        |20210703                   |NULL                   |0                           |0.0                      |20210703                 |0                 |\n",
      "|50                           |20210703       |CA10               |NULL         |NULL         |2100021799     |Tolling                                 |1              |4007        |20210705                   |185821762              |50                          |0.0                      |20210703                 |-2                |\n",
      "|10                           |20210703       |GB10               |NULL         |NULL         |2100015294     |Resins & Polymers                       |2              |3024        |20210703                   |185822220              |10                          |0.0                      |20210703                 |0                 |\n",
      "|10                           |20210703       |CA10               |8010001620   |M5W 1P1      |NULL           |Transportation, Storage, Mail Services  |NULL           |4036        |20210703                   |NULL                   |0                           |0.0                      |20210703                 |0                 |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to the correct table\n",
    "tableName = 'PURCHASE_ORDER_HISTORY'\n",
    "dataframe = session.table(tableName)\n",
    "\n",
    "# Calculation to find the lag between Planned Delivery from Actual Delivery\n",
    "dataframe = dataframe.withColumn(\"target_feature\",\n",
    "                                    datediff('day', \n",
    "                                            col(\"DELIVERY_DATE_ML\"), \n",
    "                                            col(\"FIRST_GR_POSTING_DATE_ML\")))\n",
    "\n",
    "\n",
    "# Example: Selecting specific columns\n",
    "# This selects only a subset of columns. Adjust the column names as needed.\n",
    "filtered_dataframe = dataframe.select(\n",
    "    col(\"PURCHASE_DOCUMENT_ITEM_ID\"), # ID for purchase order\n",
    "    col(\"CREATE_DATE\"),            # day purchase order was created\n",
    "    col(\"COMPANY_CODE_ID\"),           # copmany w/in INVISTA making purchase\n",
    "    col(\"VENDOR_ID\"),                 # ID of the vendor \"we\" are purchasing from\n",
    "    col(\"POSTAL_CD\"),                 # postal code associated w company code ID\n",
    "    col(\"MATERIAL_ID\"),               # ID of material being purchase\n",
    "    col(\"SUB_COMMODITY_DESC\"),        # description of sub commodity\n",
    "    col(\"MRP_TYPE_ID\"),               # determined if material is reordered manually or automatically\n",
    "    col(\"PLANT_ID\"),                  # ID of plant making purchase\n",
    "    col(\"REQUESTED_DELIVERY_DATE\"),# delivery date from requisition\n",
    "    col(\"INBOUND_DELIVERY_ID\"),       # ID for delivery\n",
    "    col(\"INBOUND_DELIVERY_ITEM_ID\"),  # ID of item w/in delivery\n",
    "    col(\"PLANNED_DELIVERY_DAYS\"),     # Amount of days expected to take\n",
    "    col(\"FIRST_GR_POSTING_DATE\"),  # expected delivery date        \n",
    "    col(\"target_feature\")             # Lag between Planned Delivery from Actual Delivery \n",
    ")\n",
    "\n",
    "\n",
    "# Print a sample of the filtered dataframe to standard output.\n",
    "filtered_dataframe.show()\n",
    "\n",
    "# Optionally, you might want to filter rows based on some conditions\n",
    "# Example: Filtering out rows where FIRST_GR_POSTING_DATE_ML is NULL\n",
    "filtered_dataframe = filtered_dataframe.filter(col(\"FIRST_GR_POSTING_DATE\").is_not_null())\n",
    "\n",
    "# filtered_dataframe = filtered_dataframe[filtered_dataframe['PLANNED_DELIVERY_DAYS'] < 6]\n",
    "\n",
    "# Show the DataFrame after filtering\n",
    "filtered_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"COMPANY_CODE_ID\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"VENDOR_ID\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"POSTAL_CD\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"SUB_COMMODITY_DESC\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"PLANNED_DELIVERY_DAYS\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'filtered_dataframe' is the DataFrame you've prepared in Snowflake\n",
    "# Convert the Snowpark DataFrame to a Pandas DataFrame with consideration for NULL values\n",
    "\n",
    "# Convert DataFrame to Pandas, handling NULL values by allowing float conversion\n",
    "df = filtered_dataframe.fillna(0).to_pandas()  # This replaces NULL with 0 before conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_DOCUMENT_ITEM_ID</th>\n",
       "      <th>CREATE_DATE</th>\n",
       "      <th>COMPANY_CODE_ID</th>\n",
       "      <th>VENDOR_ID</th>\n",
       "      <th>POSTAL_CD</th>\n",
       "      <th>MATERIAL_ID</th>\n",
       "      <th>SUB_COMMODITY_DESC</th>\n",
       "      <th>MRP_TYPE_ID</th>\n",
       "      <th>PLANT_ID</th>\n",
       "      <th>REQUESTED_DELIVERY_DATE</th>\n",
       "      <th>INBOUND_DELIVERY_ID</th>\n",
       "      <th>INBOUND_DELIVERY_ITEM_ID</th>\n",
       "      <th>PLANNED_DELIVERY_DAYS</th>\n",
       "      <th>FIRST_GR_POSTING_DATE</th>\n",
       "      <th>TARGET_FEATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>20180907</td>\n",
       "      <td>CN15</td>\n",
       "      <td>V4014</td>\n",
       "      <td>29078</td>\n",
       "      <td>2100007708</td>\n",
       "      <td>Custom Manufacturing</td>\n",
       "      <td>1</td>\n",
       "      <td>1016</td>\n",
       "      <td>20181116</td>\n",
       "      <td>183615169</td>\n",
       "      <td>900001</td>\n",
       "      <td>52.0</td>\n",
       "      <td>20181122</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>20180907</td>\n",
       "      <td>CN20</td>\n",
       "      <td>8010095928</td>\n",
       "      <td>201206</td>\n",
       "      <td>0</td>\n",
       "      <td>Tanks and Process Equipment</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20180928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>20180907</td>\n",
       "      <td>CA10</td>\n",
       "      <td>8010003146</td>\n",
       "      <td>L6L 6R2</td>\n",
       "      <td>1100125572</td>\n",
       "      <td>Piping &amp; Tubing</td>\n",
       "      <td>1</td>\n",
       "      <td>4036</td>\n",
       "      <td>20181001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20181205</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180</td>\n",
       "      <td>20180907</td>\n",
       "      <td>CA10</td>\n",
       "      <td>8010005836</td>\n",
       "      <td>N2C 0B7</td>\n",
       "      <td>0</td>\n",
       "      <td>Material Handling</td>\n",
       "      <td>0</td>\n",
       "      <td>4036</td>\n",
       "      <td>20180908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20180918</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>20180907</td>\n",
       "      <td>CA10</td>\n",
       "      <td>8010005836</td>\n",
       "      <td>N2C 0B7</td>\n",
       "      <td>0</td>\n",
       "      <td>Material Handling</td>\n",
       "      <td>0</td>\n",
       "      <td>4036</td>\n",
       "      <td>20180908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20180918</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PURCHASE_DOCUMENT_ITEM_ID  CREATE_DATE COMPANY_CODE_ID   VENDOR_ID  \\\n",
       "0                         10     20180907            CN15       V4014   \n",
       "1                         20     20180907            CN20  8010095928   \n",
       "2                         10     20180907            CA10  8010003146   \n",
       "3                        180     20180907            CA10  8010005836   \n",
       "4                         60     20180907            CA10  8010005836   \n",
       "\n",
       "  POSTAL_CD  MATERIAL_ID           SUB_COMMODITY_DESC  MRP_TYPE_ID  PLANT_ID  \\\n",
       "0     29078   2100007708         Custom Manufacturing            1      1016   \n",
       "1    201206            0  Tanks and Process Equipment            0      1032   \n",
       "2   L6L 6R2   1100125572              Piping & Tubing            1      4036   \n",
       "3   N2C 0B7            0            Material Handling            0      4036   \n",
       "4   N2C 0B7            0            Material Handling            0      4036   \n",
       "\n",
       "   REQUESTED_DELIVERY_DATE  INBOUND_DELIVERY_ID  INBOUND_DELIVERY_ITEM_ID  \\\n",
       "0                 20181116            183615169                    900001   \n",
       "1                 20180928                    0                         0   \n",
       "2                 20181001                    0                         0   \n",
       "3                 20180908                    0                         0   \n",
       "4                 20180908                    0                         0   \n",
       "\n",
       "  PLANNED_DELIVERY_DAYS  FIRST_GR_POSTING_DATE  TARGET_FEATURE  \n",
       "0                  52.0               20181122               6  \n",
       "1                   0.0                      0               0  \n",
       "2                  24.0               20181205              23  \n",
       "3                   0.0               20180918              10  \n",
       "4                   0.0               20180918              10  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           6\n",
       "1           0\n",
       "2          23\n",
       "3          10\n",
       "4          10\n",
       "           ..\n",
       "1139387    -2\n",
       "1139388    -1\n",
       "1139389     0\n",
       "1139390    -1\n",
       "1139391   -27\n",
       "Name: TARGET_FEATURE, Length: 1139392, dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET_FEATURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle missing values\n",
    "# data.fillna(0, inplace=True)  \n",
    "\n",
    "# Impute missing SUB_COMMODITY_DESC\n",
    "df['SUB_COMMODITY_DESC'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Convert categorical columns to one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['SUB_COMMODITY_DESC'])\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encoder = OneHotEncoder(sparse=False)  # Dense output\n",
    "# encoded_data = encoder.fit_transform(df[['SUB_COMMODITY_DESC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_delivery_days(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove leading/trailing whitespace\n",
    "        value = value.strip() \n",
    "\n",
    "        # Check for timestamp format and handle separately\n",
    "        if re.match(r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\", value):  \n",
    "            return \"\"  # Replace timestamps with NA or another placeholder\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "df['PLANNED_DELIVERY_DAYS'] = df['PLANNED_DELIVERY_DAYS'].apply(clean_delivery_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash Alphanumeric columns\n",
    "df['VENDOR_ID'] = df['VENDOR_ID'].apply(hash)\n",
    "df['POSTAL_CD'] = df['POSTAL_CD'].apply(hash)\n",
    "df['COMPANY_CODE_ID'] = df['COMPANY_CODE_ID'].apply(hash)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 <class 'str'> '52'\n",
      "0 <class 'str'> '0'\n",
      "24 <class 'str'> '24'\n",
      "11 <class 'str'> '11'\n",
      "1 <class 'str'> '1'\n",
      "36 <class 'str'> '36'\n",
      "3 <class 'str'> '3'\n",
      "10 <class 'str'> '10'\n",
      "5 <class 'str'> '5'\n",
      "14 <class 'str'> '14'\n",
      "4 <class 'str'> '4'\n",
      "60 <class 'str'> '60'\n",
      "45 <class 'str'> '45'\n",
      "32 <class 'str'> '32'\n",
      "40 <class 'str'> '40'\n",
      "31 <class 'str'> '31'\n",
      "7 <class 'str'> '7'\n",
      "126 <class 'str'> '126'\n",
      "28 <class 'str'> '28'\n",
      "15 <class 'str'> '15'\n",
      "42 <class 'str'> '42'\n",
      "30 <class 'str'> '30'\n",
      "6 <class 'str'> '6'\n",
      "150 <class 'str'> '150'\n",
      "90 <class 'str'> '90'\n",
      "70 <class 'str'> '70'\n",
      "8 <class 'str'> '8'\n",
      "35 <class 'str'> '35'\n",
      "75 <class 'str'> '75'\n",
      "50 <class 'str'> '50'\n",
      "59 <class 'str'> '59'\n",
      "21 <class 'str'> '21'\n",
      "12 <class 'str'> '12'\n",
      "33 <class 'str'> '33'\n",
      "16 <class 'str'> '16'\n",
      "2 <class 'str'> '2'\n",
      "9 <class 'str'> '9'\n",
      "18 <class 'str'> '18'\n",
      "29 <class 'str'> '29'\n",
      "106 <class 'str'> '106'\n",
      "13 <class 'str'> '13'\n",
      "209 <class 'str'> '209'\n",
      "120 <class 'str'> '120'\n",
      "20 <class 'str'> '20'\n",
      "65 <class 'str'> '65'\n",
      "27 <class 'str'> '27'\n",
      "71 <class 'str'> '71'\n",
      "111 <class 'str'> '111'\n",
      "44 <class 'str'> '44'\n",
      "25 <class 'str'> '25'\n",
      "17 <class 'str'> '17'\n",
      "23 <class 'str'> '23'\n",
      "240 <class 'str'> '240'\n",
      "22 <class 'str'> '22'\n",
      "63 <class 'str'> '63'\n",
      "19 <class 'str'> '19'\n",
      "55 <class 'str'> '55'\n",
      "80 <class 'str'> '80'\n",
      "34 <class 'str'> '34'\n",
      "49 <class 'str'> '49'\n",
      "166 <class 'str'> '166'\n",
      "109 <class 'str'> '109'\n",
      "85 <class 'str'> '85'\n",
      "110 <class 'str'> '110'\n",
      "104 <class 'str'> '104'\n",
      "87 <class 'str'> '87'\n",
      "135 <class 'str'> '135'\n",
      "154 <class 'str'> '154'\n",
      "26 <class 'str'> '26'\n",
      "77 <class 'str'> '77'\n",
      "76 <class 'str'> '76'\n",
      "46 <class 'str'> '46'\n",
      "162 <class 'str'> '162'\n",
      "62 <class 'str'> '62'\n",
      "107 <class 'str'> '107'\n",
      "100 <class 'str'> '100'\n",
      "39 <class 'str'> '39'\n",
      "999 <class 'str'> '999'\n",
      "123 <class 'str'> '123'\n",
      "54 <class 'str'> '54'\n",
      "57 <class 'str'> '57'\n",
      "56 <class 'str'> '56'\n",
      "48 <class 'str'> '48'\n",
      "41 <class 'str'> '41'\n",
      "68 <class 'str'> '68'\n",
      "47 <class 'str'> '47'\n",
      "37 <class 'str'> '37'\n",
      "64 <class 'str'> '64'\n",
      "113 <class 'str'> '113'\n",
      "125 <class 'str'> '125'\n",
      "84 <class 'str'> '84'\n",
      "38 <class 'str'> '38'\n",
      "78 <class 'str'> '78'\n",
      "92 <class 'str'> '92'\n",
      "160 <class 'str'> '160'\n",
      "51 <class 'str'> '51'\n",
      "72 <class 'str'> '72'\n",
      "53 <class 'str'> '53'\n",
      "114 <class 'str'> '114'\n",
      "66 <class 'str'> '66'\n",
      "127 <class 'str'> '127'\n",
      "133 <class 'str'> '133'\n",
      "67 <class 'str'> '67'\n",
      "132 <class 'str'> '132'\n",
      "98 <class 'str'> '98'\n",
      "89 <class 'str'> '89'\n",
      "260 <class 'str'> '260'\n",
      "189 <class 'str'> '189'\n",
      "61 <class 'str'> '61'\n",
      "43 <class 'str'> '43'\n",
      "200 <class 'str'> '200'\n",
      "170 <class 'str'> '170'\n",
      "210 <class 'str'> '210'\n",
      "74 <class 'str'> '74'\n",
      "140 <class 'str'> '140'\n",
      "130 <class 'str'> '130'\n",
      "69 <class 'str'> '69'\n",
      "185 <class 'str'> '185'\n",
      "180 <class 'str'> '180'\n",
      "58 <class 'str'> '58'\n",
      "119 <class 'str'> '119'\n",
      "93 <class 'str'> '93'\n",
      "168 <class 'str'> '168'\n",
      "91 <class 'str'> '91'\n",
      "96 <class 'str'> '96'\n",
      "146 <class 'str'> '146'\n",
      "147 <class 'str'> '147'\n",
      "124 <class 'str'> '124'\n",
      "81 <class 'str'> '81'\n",
      "79 <class 'str'> '79'\n",
      "108 <class 'str'> '108'\n",
      "129 <class 'str'> '129'\n",
      "206 <class 'str'> '206'\n",
      "128 <class 'str'> '128'\n",
      "112 <class 'str'> '112'\n",
      "103 <class 'str'> '103'\n",
      "121 <class 'str'> '121'\n",
      "820 <class 'str'> '820'\n",
      "204 <class 'str'> '204'\n",
      "169 <class 'str'> '169'\n",
      "73 <class 'str'> '73'\n",
      "137 <class 'str'> '137'\n",
      "105 <class 'str'> '105'\n",
      "139 <class 'str'> '139'\n",
      "148 <class 'str'> '148'\n",
      "152 <class 'str'> '152'\n",
      "238 <class 'str'> '238'\n",
      "99 <class 'str'> '99'\n",
      "117 <class 'str'> '117'\n",
      "190 <class 'str'> '190'\n",
      "102 <class 'str'> '102'\n",
      "182 <class 'str'> '182'\n",
      "101 <class 'str'> '101'\n",
      "192 <class 'str'> '192'\n",
      "181 <class 'str'> '181'\n",
      "122 <class 'str'> '122'\n",
      "195 <class 'str'> '195'\n",
      "88 <class 'str'> '88'\n",
      "141 <class 'str'> '141'\n",
      "86 <class 'str'> '86'\n",
      "145 <class 'str'> '145'\n",
      "82 <class 'str'> '82'\n",
      "83 <class 'str'> '83'\n",
      "164 <class 'str'> '164'\n",
      "216 <class 'str'> '216'\n",
      "95 <class 'str'> '95'\n",
      "165 <class 'str'> '165'\n",
      "142 <class 'str'> '142'\n",
      "223 <class 'str'> '223'\n",
      "250 <class 'str'> '250'\n",
      "225 <class 'str'> '225'\n",
      "161 <class 'str'> '161'\n",
      "252 <class 'str'> '252'\n",
      "211 <class 'str'> '211'\n",
      "115 <class 'str'> '115'\n",
      "310 <class 'str'> '310'\n",
      "220 <class 'str'> '220'\n",
      "196 <class 'str'> '196'\n",
      "143 <class 'str'> '143'\n",
      "300 <class 'str'> '300'\n",
      "224 <class 'str'> '224'\n",
      "134 <class 'str'> '134'\n",
      "94 <class 'str'> '94'\n",
      "280 <class 'str'> '280'\n",
      "155 <class 'str'> '155'\n",
      "230 <class 'str'> '230'\n",
      "118 <class 'str'> '118'\n",
      "320 <class 'str'> '320'\n",
      "255 <class 'str'> '255'\n",
      "151 <class 'str'> '151'\n",
      "116 <class 'str'> '116'\n",
      "144 <class 'str'> '144'\n",
      "304 <class 'str'> '304'\n",
      "178 <class 'str'> '178'\n",
      "177 <class 'str'> '177'\n",
      "97 <class 'str'> '97'\n",
      "149 <class 'str'> '149'\n",
      "131 <class 'str'> '131'\n",
      "179 <class 'str'> '179'\n",
      "186 <class 'str'> '186'\n",
      "136 <class 'str'> '136'\n",
      "297 <class 'str'> '297'\n",
      "157 <class 'str'> '157'\n",
      "500 <class 'str'> '500'\n",
      "183 <class 'str'> '183'\n",
      "256 <class 'str'> '256'\n",
      "184 <class 'str'> '184'\n",
      "253 <class 'str'> '253'\n",
      "259 <class 'str'> '259'\n",
      "212 <class 'str'> '212'\n",
      "337 <class 'str'> '337'\n",
      "350 <class 'str'> '350'\n",
      "138 <class 'str'> '138'\n",
      "175 <class 'str'> '175'\n",
      "420 <class 'str'> '420'\n",
      "267 <class 'str'> '267'\n",
      "270 <class 'str'> '270'\n",
      "205 <class 'str'> '205'\n",
      "197 <class 'str'> '197'\n",
      "173 <class 'str'> '173'\n",
      "172 <class 'str'> '172'\n",
      "156 <class 'str'> '156'\n",
      "365 <class 'str'> '365'\n",
      "401 <class 'str'> '401'\n",
      "254 <class 'str'> '254'\n",
      "163 <class 'str'> '163'\n",
      "281 <class 'str'> '281'\n",
      "221 <class 'str'> '221'\n",
      " <class 'str'> ''\n",
      "153 <class 'str'> '153'\n",
      "158 <class 'str'> '158'\n",
      "219 <class 'str'> '219'\n",
      "585 <class 'str'> '585'\n",
      "207 <class 'str'> '207'\n",
      "390 <class 'str'> '390'\n",
      "395 <class 'str'> '395'\n",
      "203 <class 'str'> '203'\n",
      "217 <class 'str'> '217'\n",
      "317 <class 'str'> '317'\n",
      "167 <class 'str'> '167'\n",
      "235 <class 'str'> '235'\n",
      "340 <class 'str'> '340'\n",
      "198 <class 'str'> '198'\n",
      "380 <class 'str'> '380'\n",
      "265 <class 'str'> '265'\n",
      "245 <class 'str'> '245'\n",
      "305 <class 'str'> '305'\n",
      "355 <class 'str'> '355'\n",
      "228 <class 'str'> '228'\n",
      "171 <class 'str'> '171'\n",
      "435 <class 'str'> '435'\n",
      "289 <class 'str'> '289'\n",
      "188 <class 'str'> '188'\n",
      "295 <class 'str'> '295'\n",
      "234 <class 'str'> '234'\n",
      "294 <class 'str'> '294'\n",
      "208 <class 'str'> '208'\n",
      "462 <class 'str'> '462'\n",
      "392 <class 'str'> '392'\n",
      "308 <class 'str'> '308'\n"
     ]
    }
   ],
   "source": [
    "def remove_decimal(value):\n",
    "    return value.split('.')[0]  # Split by the decimal and keep the integer part\n",
    "\n",
    "df['PLANNED_DELIVERY_DAYS'] = df['PLANNED_DELIVERY_DAYS'].apply(remove_decimal)\n",
    "\n",
    "for value in df['PLANNED_DELIVERY_DAYS'].unique():\n",
    "    print(value, type(value), repr(value)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(col):\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except ValueError:\n",
    "        print(f\"Error converting column '{col}': Contains non-numeric values\")\n",
    "\n",
    "convert_to_numeric('PURCHASE_DOCUMENT_ITEM_ID')\n",
    "convert_to_numeric('CREATE_DATE')\n",
    "convert_to_numeric('VENDOR_ID')\n",
    "convert_to_numeric('POSTAL_CD')\n",
    "convert_to_numeric('MATERIAL_ID')\n",
    "convert_to_numeric('MRP_TYPE_ID')\n",
    "convert_to_numeric('PLANT_ID')\n",
    "convert_to_numeric('INBOUND_DELIVERY_ID')\n",
    "convert_to_numeric('INBOUND_DELIVERY_ITEM_ID')\n",
    "convert_to_numeric('PLANNED_DELIVERY_DAYS')\n",
    "convert_to_numeric('FIRST_GR_POSTING_DATE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_DOCUMENT_ITEM_ID</th>\n",
       "      <th>CREATE_DATE</th>\n",
       "      <th>COMPANY_CODE_ID</th>\n",
       "      <th>VENDOR_ID</th>\n",
       "      <th>POSTAL_CD</th>\n",
       "      <th>MATERIAL_ID</th>\n",
       "      <th>MRP_TYPE_ID</th>\n",
       "      <th>PLANT_ID</th>\n",
       "      <th>REQUESTED_DELIVERY_DATE</th>\n",
       "      <th>INBOUND_DELIVERY_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>SUB_COMMODITY_DESC_Tools</th>\n",
       "      <th>SUB_COMMODITY_DESC_Transport operations services</th>\n",
       "      <th>SUB_COMMODITY_DESC_Transportation, Storage, Mail Services</th>\n",
       "      <th>SUB_COMMODITY_DESC_Travel Services</th>\n",
       "      <th>SUB_COMMODITY_DESC_Tubes &amp; Cores</th>\n",
       "      <th>SUB_COMMODITY_DESC_Unknown</th>\n",
       "      <th>SUB_COMMODITY_DESC_Valves</th>\n",
       "      <th>SUB_COMMODITY_DESC_Vehicles</th>\n",
       "      <th>SUB_COMMODITY_DESC_Waste Disposal Services</th>\n",
       "      <th>SUB_COMMODITY_DESC_Water Treatment Chemicals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>20180907</td>\n",
       "      <td>194229884258008330</td>\n",
       "      <td>-4589704172822686821</td>\n",
       "      <td>1053147618313030029</td>\n",
       "      <td>2100007708</td>\n",
       "      <td>1</td>\n",
       "      <td>1016</td>\n",
       "      <td>20181116</td>\n",
       "      <td>183615169</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>20180907</td>\n",
       "      <td>-5997776775778932388</td>\n",
       "      <td>1632896595288678667</td>\n",
       "      <td>-5416675489349390164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20180928</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>20180907</td>\n",
       "      <td>-7075448354653650262</td>\n",
       "      <td>-2681110374882210960</td>\n",
       "      <td>-5183451330756665941</td>\n",
       "      <td>1100125572</td>\n",
       "      <td>1</td>\n",
       "      <td>4036</td>\n",
       "      <td>20181001</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180</td>\n",
       "      <td>20180907</td>\n",
       "      <td>-7075448354653650262</td>\n",
       "      <td>-7262751417552565205</td>\n",
       "      <td>-1155049848321441110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4036</td>\n",
       "      <td>20180908</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>20180907</td>\n",
       "      <td>-7075448354653650262</td>\n",
       "      <td>-7262751417552565205</td>\n",
       "      <td>-1155049848321441110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4036</td>\n",
       "      <td>20180908</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PURCHASE_DOCUMENT_ITEM_ID  CREATE_DATE      COMPANY_CODE_ID  \\\n",
       "0                         10     20180907   194229884258008330   \n",
       "1                         20     20180907 -5997776775778932388   \n",
       "2                         10     20180907 -7075448354653650262   \n",
       "3                        180     20180907 -7075448354653650262   \n",
       "4                         60     20180907 -7075448354653650262   \n",
       "\n",
       "             VENDOR_ID            POSTAL_CD  MATERIAL_ID  MRP_TYPE_ID  \\\n",
       "0 -4589704172822686821  1053147618313030029   2100007708            1   \n",
       "1  1632896595288678667 -5416675489349390164            0            0   \n",
       "2 -2681110374882210960 -5183451330756665941   1100125572            1   \n",
       "3 -7262751417552565205 -1155049848321441110            0            0   \n",
       "4 -7262751417552565205 -1155049848321441110            0            0   \n",
       "\n",
       "   PLANT_ID  REQUESTED_DELIVERY_DATE  INBOUND_DELIVERY_ID  ...  \\\n",
       "0      1016                 20181116            183615169  ...   \n",
       "1      1032                 20180928                    0  ...   \n",
       "2      4036                 20181001                    0  ...   \n",
       "3      4036                 20180908                    0  ...   \n",
       "4      4036                 20180908                    0  ...   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Tools  SUB_COMMODITY_DESC_Transport operations services  \\\n",
       "0                         0                                                 0   \n",
       "1                         0                                                 0   \n",
       "2                         0                                                 0   \n",
       "3                         0                                                 0   \n",
       "4                         0                                                 0   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Transportation, Storage, Mail Services  \\\n",
       "0                                                  0           \n",
       "1                                                  0           \n",
       "2                                                  0           \n",
       "3                                                  0           \n",
       "4                                                  0           \n",
       "\n",
       "   SUB_COMMODITY_DESC_Travel Services  SUB_COMMODITY_DESC_Tubes & Cores  \\\n",
       "0                                   0                                 0   \n",
       "1                                   0                                 0   \n",
       "2                                   0                                 0   \n",
       "3                                   0                                 0   \n",
       "4                                   0                                 0   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Unknown  SUB_COMMODITY_DESC_Valves  \\\n",
       "0                           0                          0   \n",
       "1                           0                          0   \n",
       "2                           0                          0   \n",
       "3                           0                          0   \n",
       "4                           0                          0   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Vehicles  SUB_COMMODITY_DESC_Waste Disposal Services  \\\n",
       "0                            0                                           0   \n",
       "1                            0                                           0   \n",
       "2                            0                                           0   \n",
       "3                            0                                           0   \n",
       "4                            0                                           0   \n",
       "\n",
       "   SUB_COMMODITY_DESC_Water Treatment Chemicals  \n",
       "0                                             0  \n",
       "1                                             0  \n",
       "2                                             0  \n",
       "3                                             0  \n",
       "4                                             0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1139392 entries, 0 to 1139391\n",
      "Data columns (total 87 columns):\n",
      " #   Column                                                     Non-Null Count    Dtype  \n",
      "---  ------                                                     --------------    -----  \n",
      " 0   PURCHASE_DOCUMENT_ITEM_ID                                  1139392 non-null  int16  \n",
      " 1   CREATE_DATE                                                1139392 non-null  int32  \n",
      " 2   COMPANY_CODE_ID                                            1139392 non-null  int64  \n",
      " 3   VENDOR_ID                                                  1139392 non-null  int64  \n",
      " 4   POSTAL_CD                                                  1139392 non-null  int64  \n",
      " 5   MATERIAL_ID                                                1139392 non-null  int64  \n",
      " 6   MRP_TYPE_ID                                                1139392 non-null  int8   \n",
      " 7   PLANT_ID                                                   1139392 non-null  int16  \n",
      " 8   REQUESTED_DELIVERY_DATE                                    1139392 non-null  int32  \n",
      " 9   INBOUND_DELIVERY_ID                                        1139392 non-null  int32  \n",
      " 10  INBOUND_DELIVERY_ITEM_ID                                   1139392 non-null  int32  \n",
      " 11  PLANNED_DELIVERY_DAYS                                      1139388 non-null  float64\n",
      " 12  FIRST_GR_POSTING_DATE                                      1139392 non-null  int32  \n",
      " 13  TARGET_FEATURE                                             1139392 non-null  int32  \n",
      " 14  SUB_COMMODITY_DESC_Additives, Colorants & Catalysts        1139392 non-null  uint8  \n",
      " 15  SUB_COMMODITY_DESC_Admin Services                          1139392 non-null  uint8  \n",
      " 16  SUB_COMMODITY_DESC_Blowers & Dryers                        1139392 non-null  uint8  \n",
      " 17  SUB_COMMODITY_DESC_Building Materials                      1139392 non-null  uint8  \n",
      " 18  SUB_COMMODITY_DESC_Building, Construction Services         1139392 non-null  uint8  \n",
      " 19  SUB_COMMODITY_DESC_CONTAINMENTS                            1139392 non-null  uint8  \n",
      " 20  SUB_COMMODITY_DESC_CORRUGATED                              1139392 non-null  uint8  \n",
      " 21  SUB_COMMODITY_DESC_Communications Devices Accessories      1139392 non-null  uint8  \n",
      " 22  SUB_COMMODITY_DESC_Computer Equipment Accessories          1139392 non-null  uint8  \n",
      " 23  SUB_COMMODITY_DESC_Computer services                       1139392 non-null  uint8  \n",
      " 24  SUB_COMMODITY_DESC_Consulting Services                     1139392 non-null  uint8  \n",
      " 25  SUB_COMMODITY_DESC_Corporate Services                      1139392 non-null  uint8  \n",
      " 26  SUB_COMMODITY_DESC_Custom Manufacturing                    1139392 non-null  uint8  \n",
      " 27  SUB_COMMODITY_DESC_Cylinder Gases                          1139392 non-null  uint8  \n",
      " 28  SUB_COMMODITY_DESC_Electric & Electronics                  1139392 non-null  uint8  \n",
      " 29  SUB_COMMODITY_DESC_Elements & Industrial Gases             1139392 non-null  uint8  \n",
      " 30  SUB_COMMODITY_DESC_Engineering Services                    1139392 non-null  uint8  \n",
      " 31  SUB_COMMODITY_DESC_Environmental Services                  1139392 non-null  uint8  \n",
      " 32  SUB_COMMODITY_DESC_Feedstock's                             1139392 non-null  uint8  \n",
      " 33  SUB_COMMODITY_DESC_Film, Strapping, Wrapping               1139392 non-null  uint8  \n",
      " 34  SUB_COMMODITY_DESC_Filtration                              1139392 non-null  uint8  \n",
      " 35  SUB_COMMODITY_DESC_Fuels                                   1139392 non-null  uint8  \n",
      " 36  SUB_COMMODITY_DESC_HR Services                             1139392 non-null  uint8  \n",
      " 37  SUB_COMMODITY_DESC_Health Services                         1139392 non-null  uint8  \n",
      " 38  SUB_COMMODITY_DESC_Industrial Cleaning Services            1139392 non-null  uint8  \n",
      " 39  SUB_COMMODITY_DESC_Infrastructure Services                 1139392 non-null  uint8  \n",
      " 40  SUB_COMMODITY_DESC_Instrumentation & Process Control       1139392 non-null  uint8  \n",
      " 41  SUB_COMMODITY_DESC_Lab Supplies                            1139392 non-null  uint8  \n",
      " 42  SUB_COMMODITY_DESC_Lubricants                              1139392 non-null  uint8  \n",
      " 43  SUB_COMMODITY_DESC_Machinery & Equipment                   1139392 non-null  uint8  \n",
      " 44  SUB_COMMODITY_DESC_Maintenance Services                    1139392 non-null  uint8  \n",
      " 45  SUB_COMMODITY_DESC_Manufacturing Components and Supplies   1139392 non-null  uint8  \n",
      " 46  SUB_COMMODITY_DESC_Manufacturing Services                  1139392 non-null  uint8  \n",
      " 47  SUB_COMMODITY_DESC_Marketing Communications                1139392 non-null  uint8  \n",
      " 48  SUB_COMMODITY_DESC_Marketing Consulting Services           1139392 non-null  uint8  \n",
      " 49  SUB_COMMODITY_DESC_Material Handling                       1139392 non-null  uint8  \n",
      " 50  SUB_COMMODITY_DESC_Mill Supplies                           1139392 non-null  uint8  \n",
      " 51  SUB_COMMODITY_DESC_Motors                                  1139392 non-null  uint8  \n",
      " 52  SUB_COMMODITY_DESC_Office Supplies                         1139392 non-null  uint8  \n",
      " 53  SUB_COMMODITY_DESC_Pads, Trays & Locators                  1139392 non-null  uint8  \n",
      " 54  SUB_COMMODITY_DESC_Pallets                                 1139392 non-null  uint8  \n",
      " 55  SUB_COMMODITY_DESC_Piping & Tubing                         1139392 non-null  uint8  \n",
      " 56  SUB_COMMODITY_DESC_Power & Utilities Services              1139392 non-null  uint8  \n",
      " 57  SUB_COMMODITY_DESC_Power Generation & Distribution         1139392 non-null  uint8  \n",
      " 58  SUB_COMMODITY_DESC_Power Generation Equipment              1139392 non-null  uint8  \n",
      " 59  SUB_COMMODITY_DESC_Power Transmission                      1139392 non-null  uint8  \n",
      " 60  SUB_COMMODITY_DESC_Precious Metals, Alloys and oxides      1139392 non-null  uint8  \n",
      " 61  SUB_COMMODITY_DESC_Process Control Equipment               1139392 non-null  uint8  \n",
      " 62  SUB_COMMODITY_DESC_Promotion & Advertising                 1139392 non-null  uint8  \n",
      " 63  SUB_COMMODITY_DESC_Public Power & Utility Services         1139392 non-null  uint8  \n",
      " 64  SUB_COMMODITY_DESC_Pumps & Compressors                     1139392 non-null  uint8  \n",
      " 65  SUB_COMMODITY_DESC_Real Estate Services                    1139392 non-null  uint8  \n",
      " 66  SUB_COMMODITY_DESC_Rental & Lease services                 1139392 non-null  uint8  \n",
      " 67  SUB_COMMODITY_DESC_Resins & Polymers                       1139392 non-null  uint8  \n",
      " 68  SUB_COMMODITY_DESC_Safety Supplies                         1139392 non-null  uint8  \n",
      " 69  SUB_COMMODITY_DESC_Security Services                       1139392 non-null  uint8  \n",
      " 70  SUB_COMMODITY_DESC_Software                                1139392 non-null  uint8  \n",
      " 71  SUB_COMMODITY_DESC_Spinning                                1139392 non-null  uint8  \n",
      " 72  SUB_COMMODITY_DESC_Storage & Handling Equipment            1139392 non-null  uint8  \n",
      " 73  SUB_COMMODITY_DESC_Tanks and Process Equipment             1139392 non-null  uint8  \n",
      " 74  SUB_COMMODITY_DESC_Telecommunications media services       1139392 non-null  uint8  \n",
      " 75  SUB_COMMODITY_DESC_Textile & Spinning Machinery            1139392 non-null  uint8  \n",
      " 76  SUB_COMMODITY_DESC_Tolling                                 1139392 non-null  uint8  \n",
      " 77  SUB_COMMODITY_DESC_Tools                                   1139392 non-null  uint8  \n",
      " 78  SUB_COMMODITY_DESC_Transport operations services           1139392 non-null  uint8  \n",
      " 79  SUB_COMMODITY_DESC_Transportation, Storage, Mail Services  1139392 non-null  uint8  \n",
      " 80  SUB_COMMODITY_DESC_Travel Services                         1139392 non-null  uint8  \n",
      " 81  SUB_COMMODITY_DESC_Tubes & Cores                           1139392 non-null  uint8  \n",
      " 82  SUB_COMMODITY_DESC_Unknown                                 1139392 non-null  uint8  \n",
      " 83  SUB_COMMODITY_DESC_Valves                                  1139392 non-null  uint8  \n",
      " 84  SUB_COMMODITY_DESC_Vehicles                                1139392 non-null  uint8  \n",
      " 85  SUB_COMMODITY_DESC_Waste Disposal Services                 1139392 non-null  uint8  \n",
      " 86  SUB_COMMODITY_DESC_Water Treatment Chemicals               1139392 non-null  uint8  \n",
      "dtypes: float64(1), int16(2), int32(6), int64(4), int8(1), uint8(73)\n",
      "memory usage: 154.3 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show types of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "df = df[(df['TARGET_FEATURE'] < 100) & (df['TARGET_FEATURE'] > -100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+01,  2.0181e+07,  1.9423e+17,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e+01,  2.0181e+07, -5.9978e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0000e+01,  2.0181e+07, -7.0754e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 1.0000e+01,  2.0211e+07, -8.9874e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0000e+01,  2.0211e+07, -4.4362e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0000e+01,  2.0211e+07, -8.9874e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create feature & target tensors on GPU\n",
    "features = df.drop('TARGET_FEATURE', axis=1)\n",
    "targets = df['TARGET_FEATURE']\n",
    "X = torch.tensor(features.values.astype(np.float32))\n",
    "y = torch.tensor(targets.values.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a model\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(23, 10)  # 23 input features, 10 output features\n",
    "#         self.fc2 = nn.Linear(10, 1)   # 10 input features, 1 output feature\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "    \n",
    "# model = Net()\n",
    "\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Now X_normalized contains the normalized data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  6.,   0.,  23.,  ...,   0.,  -1., -27.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Add training and test data to the device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RegressionNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size1 = 128  \n",
    "        self.hidden_size2 = 64  \n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, self.hidden_size1)\n",
    "        self.linear2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "\n",
    "        # Output layer for regression (no activation)\n",
    "        self.output_layer = nn.Linear(self.hidden_size2, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))  \n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "input_size = X.shape[1]  # Number of features\n",
    "\n",
    "\n",
    "model = RegressionNetwork(input_size) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Mean Absolute Error (MAE) loss\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1Loss()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data Preparation (If not using a DataLoader)\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# port to GPU\n",
    "model.to(device)\n",
    "loss_fn.to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/17448], Loss: 3854778782187520.0000\n",
      "Epoch [1/100], Step [200/17448], Loss: 2593974489448448.0000\n",
      "Epoch [1/100], Step [300/17448], Loss: 1772903175553024.0000\n",
      "Epoch [1/100], Step [400/17448], Loss: 2704452859461632.0000\n",
      "Epoch [1/100], Step [500/17448], Loss: 2016789403795456.0000\n",
      "Epoch [1/100], Step [600/17448], Loss: 1198138303971328.0000\n",
      "Epoch [1/100], Step [700/17448], Loss: 1301882299482112.0000\n",
      "Epoch [1/100], Step [800/17448], Loss: 999340105531392.0000\n",
      "Epoch [1/100], Step [900/17448], Loss: 1123276218695680.0000\n",
      "Epoch [1/100], Step [1000/17448], Loss: 1236591951478784.0000\n",
      "Epoch [1/100], Step [1100/17448], Loss: 1043023815245824.0000\n",
      "Epoch [1/100], Step [1200/17448], Loss: 806894029504512.0000\n",
      "Epoch [1/100], Step [1300/17448], Loss: 346979301326848.0000\n",
      "Epoch [1/100], Step [1400/17448], Loss: 758661412552704.0000\n",
      "Epoch [1/100], Step [1500/17448], Loss: 459858759385088.0000\n",
      "Epoch [1/100], Step [1600/17448], Loss: 369598713036800.0000\n",
      "Epoch [1/100], Step [1700/17448], Loss: 306424273960960.0000\n",
      "Epoch [1/100], Step [1800/17448], Loss: 546182199771136.0000\n",
      "Epoch [1/100], Step [1900/17448], Loss: 352375692853248.0000\n",
      "Epoch [1/100], Step [2000/17448], Loss: 340769147715584.0000\n",
      "Epoch [1/100], Step [2100/17448], Loss: 291745518583808.0000\n",
      "Epoch [1/100], Step [2200/17448], Loss: 143071987630080.0000\n",
      "Epoch [1/100], Step [2300/17448], Loss: 253958899630080.0000\n",
      "Epoch [1/100], Step [2400/17448], Loss: 177780708868096.0000\n",
      "Epoch [1/100], Step [2500/17448], Loss: 149673788571648.0000\n",
      "Epoch [1/100], Step [2600/17448], Loss: 132604900671488.0000\n",
      "Epoch [1/100], Step [2700/17448], Loss: 150493523345408.0000\n",
      "Epoch [1/100], Step [2800/17448], Loss: 147728000614400.0000\n",
      "Epoch [1/100], Step [2900/17448], Loss: 118152126005248.0000\n",
      "Epoch [1/100], Step [3000/17448], Loss: 111737911115776.0000\n",
      "Epoch [1/100], Step [3100/17448], Loss: 72928477577216.0000\n",
      "Epoch [1/100], Step [3200/17448], Loss: 103129261539328.0000\n",
      "Epoch [1/100], Step [3300/17448], Loss: 96295616249856.0000\n",
      "Epoch [1/100], Step [3400/17448], Loss: 50251176083456.0000\n",
      "Epoch [1/100], Step [3500/17448], Loss: 62278284083200.0000\n",
      "Epoch [1/100], Step [3600/17448], Loss: 57366456303616.0000\n",
      "Epoch [1/100], Step [3700/17448], Loss: 38132120551424.0000\n",
      "Epoch [1/100], Step [3800/17448], Loss: 52975342977024.0000\n",
      "Epoch [1/100], Step [3900/17448], Loss: 50230946955264.0000\n",
      "Epoch [1/100], Step [4000/17448], Loss: 48335780052992.0000\n",
      "Epoch [1/100], Step [4100/17448], Loss: 32770317352960.0000\n",
      "Epoch [1/100], Step [4200/17448], Loss: 12564068040704.0000\n",
      "Epoch [1/100], Step [4300/17448], Loss: 24764382969856.0000\n",
      "Epoch [1/100], Step [4400/17448], Loss: 12999020511232.0000\n",
      "Epoch [1/100], Step [4500/17448], Loss: 13217260634112.0000\n",
      "Epoch [1/100], Step [4600/17448], Loss: 4121072500736.0000\n",
      "Epoch [1/100], Step [4700/17448], Loss: 7383429414912.0000\n",
      "Epoch [1/100], Step [4800/17448], Loss: 3007355813888.0000\n",
      "Epoch [1/100], Step [4900/17448], Loss: 1798135087104.0000\n",
      "Epoch [1/100], Step [5000/17448], Loss: 1327401533440.0000\n",
      "Epoch [1/100], Step [5100/17448], Loss: 779380981760.0000\n",
      "Epoch [1/100], Step [5200/17448], Loss: 477968990208.0000\n",
      "Epoch [1/100], Step [5300/17448], Loss: 713005465600.0000\n",
      "Epoch [1/100], Step [5400/17448], Loss: 272532062208.0000\n",
      "Epoch [1/100], Step [5500/17448], Loss: 279067394048.0000\n",
      "Epoch [1/100], Step [5600/17448], Loss: 437736800256.0000\n",
      "Epoch [1/100], Step [5700/17448], Loss: 114162098176.0000\n",
      "Epoch [1/100], Step [5800/17448], Loss: 210721259520.0000\n",
      "Epoch [1/100], Step [5900/17448], Loss: 61211119616.0000\n",
      "Epoch [1/100], Step [6000/17448], Loss: 125251289088.0000\n",
      "Epoch [1/100], Step [6100/17448], Loss: 86472687616.0000\n",
      "Epoch [1/100], Step [6200/17448], Loss: 75135778816.0000\n",
      "Epoch [1/100], Step [6300/17448], Loss: 41091342336.0000\n",
      "Epoch [1/100], Step [6400/17448], Loss: 33355108352.0000\n",
      "Epoch [1/100], Step [6500/17448], Loss: 35419811840.0000\n",
      "Epoch [1/100], Step [6600/17448], Loss: 5539406848.0000\n",
      "Epoch [1/100], Step [6700/17448], Loss: 10501476352.0000\n",
      "Epoch [1/100], Step [6800/17448], Loss: 8938215424.0000\n",
      "Epoch [1/100], Step [6900/17448], Loss: 7099804672.0000\n",
      "Epoch [1/100], Step [7000/17448], Loss: 16548931584.0000\n",
      "Epoch [1/100], Step [7100/17448], Loss: 7885166592.0000\n",
      "Epoch [1/100], Step [7200/17448], Loss: 2812961792.0000\n",
      "Epoch [1/100], Step [7300/17448], Loss: 26249281536.0000\n",
      "Epoch [1/100], Step [7400/17448], Loss: 6086465536.0000\n",
      "Epoch [1/100], Step [7500/17448], Loss: 7711270400.0000\n",
      "Epoch [1/100], Step [7600/17448], Loss: 3059630080.0000\n",
      "Epoch [1/100], Step [7700/17448], Loss: 3187869440.0000\n",
      "Epoch [1/100], Step [7800/17448], Loss: 2663941888.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [2/100], Step [100/17448], Loss: 1090045952.0000\n",
      "Epoch [2/100], Step [200/17448], Loss: 2469232384.0000\n",
      "Epoch [2/100], Step [300/17448], Loss: 1028424064.0000\n",
      "Epoch [2/100], Step [400/17448], Loss: 74753032.0000\n",
      "Epoch [2/100], Step [500/17448], Loss: 243041072.0000\n",
      "Epoch [2/100], Step [600/17448], Loss: 806576320.0000\n",
      "Epoch [2/100], Step [700/17448], Loss: 8.2031\n",
      "Epoch [2/100], Step [800/17448], Loss: 751968896.0000\n",
      "Epoch [2/100], Step [900/17448], Loss: 414130176.0000\n",
      "Epoch [2/100], Step [1000/17448], Loss: 263091120.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [3/100], Step [100/17448], Loss: 502216.1250\n",
      "Epoch [3/100], Step [200/17448], Loss: 46242340.0000\n",
      "Epoch [3/100], Step [300/17448], Loss: 1878905728.0000\n",
      "Epoch [3/100], Step [400/17448], Loss: 6.0156\n",
      "Epoch [3/100], Step [500/17448], Loss: 40290408.0000\n",
      "Epoch [3/100], Step [600/17448], Loss: 215741872.0000\n",
      "Epoch [3/100], Step [700/17448], Loss: 125314456.0000\n",
      "Epoch [3/100], Step [800/17448], Loss: 818619200.0000\n",
      "Epoch [3/100], Step [900/17448], Loss: 667025664.0000\n",
      "Epoch [3/100], Step [1000/17448], Loss: 9171414.0000\n",
      "Epoch [3/100], Step [1100/17448], Loss: 2033113728.0000\n",
      "Epoch [3/100], Step [1200/17448], Loss: 7.0625\n",
      "Epoch [3/100], Step [1300/17448], Loss: 8.3750\n",
      "Epoch [3/100], Step [1400/17448], Loss: 6.8438\n",
      "Epoch [3/100], Step [1500/17448], Loss: 274586336.0000\n",
      "Epoch [3/100], Step [1600/17448], Loss: 496785184.0000\n",
      "Epoch [3/100], Step [1700/17448], Loss: 9.2813\n",
      "Epoch [3/100], Step [1800/17448], Loss: 154221536.0000\n",
      "Epoch [3/100], Step [1900/17448], Loss: 6.5937\n",
      "Epoch [3/100], Step [2000/17448], Loss: 7.1719\n",
      "Epoch [3/100], Step [2100/17448], Loss: 6.4688\n",
      "Epoch [3/100], Step [2200/17448], Loss: 6.3594\n",
      "Epoch [3/100], Step [2300/17448], Loss: 6.5314\n",
      "Epoch [3/100], Step [2400/17448], Loss: 6.9688\n",
      "Epoch [3/100], Step [2500/17448], Loss: 6.4533\n",
      "Epoch [3/100], Step [2600/17448], Loss: 37785548.0000\n",
      "Epoch [3/100], Step [2700/17448], Loss: 9.7813\n",
      "Epoch [3/100], Step [2800/17448], Loss: 11.3282\n",
      "Epoch [3/100], Step [2900/17448], Loss: 5.0156\n",
      "Epoch [3/100], Step [3000/17448], Loss: 8.1251\n",
      "Epoch [3/100], Step [3100/17448], Loss: 9.4064\n",
      "Epoch [3/100], Step [3200/17448], Loss: 7.1878\n",
      "Epoch [3/100], Step [3300/17448], Loss: 5.8751\n",
      "Epoch [3/100], Step [3400/17448], Loss: 8.9375\n",
      "Epoch [3/100], Step [3500/17448], Loss: 5.4375\n",
      "Epoch [3/100], Step [3600/17448], Loss: 6.5469\n",
      "Epoch [3/100], Step [3700/17448], Loss: 5.2344\n",
      "Epoch [3/100], Step [3800/17448], Loss: 7.5470\n",
      "Epoch [3/100], Step [3900/17448], Loss: 7.6251\n",
      "Epoch [3/100], Step [4000/17448], Loss: 11.4376\n",
      "Epoch [3/100], Step [4100/17448], Loss: 7.9219\n",
      "Epoch [3/100], Step [4200/17448], Loss: 9.5469\n",
      "Epoch [3/100], Step [4300/17448], Loss: 9.4220\n",
      "Epoch [3/100], Step [4400/17448], Loss: 5.7657\n",
      "Epoch [3/100], Step [4500/17448], Loss: 5.9376\n",
      "Epoch [3/100], Step [4600/17448], Loss: 7.9533\n",
      "Epoch [3/100], Step [4700/17448], Loss: 9.2031\n",
      "Epoch [3/100], Step [4800/17448], Loss: 60501548.0000\n",
      "Epoch [3/100], Step [4900/17448], Loss: 6.6875\n",
      "Epoch [3/100], Step [5000/17448], Loss: 8.0782\n",
      "Epoch [3/100], Step [5100/17448], Loss: 8.0157\n",
      "Epoch [3/100], Step [5200/17448], Loss: 7.3751\n",
      "Epoch [3/100], Step [5300/17448], Loss: 5.7969\n",
      "Epoch [3/100], Step [5400/17448], Loss: 10.7032\n",
      "Epoch [3/100], Step [5500/17448], Loss: 11.9531\n",
      "Epoch [3/100], Step [5600/17448], Loss: 6.8282\n",
      "Epoch [3/100], Step [5700/17448], Loss: 6.3906\n",
      "Epoch [3/100], Step [5800/17448], Loss: 5.7031\n",
      "Epoch [3/100], Step [5900/17448], Loss: 7.7970\n",
      "Epoch [3/100], Step [6000/17448], Loss: 8.1875\n",
      "Epoch [3/100], Step [6100/17448], Loss: 8.1563\n",
      "Epoch [3/100], Step [6200/17448], Loss: 6.7656\n",
      "Epoch [3/100], Step [6300/17448], Loss: 5.7345\n",
      "Epoch [3/100], Step [6400/17448], Loss: 10.4688\n",
      "Epoch [3/100], Step [6500/17448], Loss: 8.2344\n",
      "Epoch [3/100], Step [6600/17448], Loss: 7.7500\n",
      "Epoch [3/100], Step [6700/17448], Loss: 6.6251\n",
      "Epoch [3/100], Step [6800/17448], Loss: 5.4689\n",
      "Epoch [3/100], Step [6900/17448], Loss: 4.3125\n",
      "Epoch [3/100], Step [7000/17448], Loss: 4.7814\n",
      "Epoch [3/100], Step [7100/17448], Loss: 12.0157\n",
      "Epoch [3/100], Step [7200/17448], Loss: 6.6251\n",
      "Epoch [3/100], Step [7300/17448], Loss: 9.6563\n",
      "Epoch [3/100], Step [7400/17448], Loss: 5.5469\n",
      "Epoch [3/100], Step [7500/17448], Loss: 8.4062\n",
      "Epoch [3/100], Step [7600/17448], Loss: 6.7813\n",
      "Epoch [3/100], Step [7700/17448], Loss: 7.5469\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [4/100], Step [100/17448], Loss: 6.3906\n",
      "Epoch [4/100], Step [200/17448], Loss: 9.3906\n",
      "Epoch [4/100], Step [300/17448], Loss: 7.5157\n",
      "Epoch [4/100], Step [400/17448], Loss: 7.7500\n",
      "Epoch [4/100], Step [500/17448], Loss: 9.4844\n",
      "Epoch [4/100], Step [600/17448], Loss: 4.9063\n",
      "Epoch [4/100], Step [700/17448], Loss: 9.0313\n",
      "Epoch [4/100], Step [800/17448], Loss: 4.3439\n",
      "Epoch [4/100], Step [900/17448], Loss: 7.9844\n",
      "Epoch [4/100], Step [1000/17448], Loss: 8.1251\n",
      "Epoch [4/100], Step [1100/17448], Loss: 7.5469\n",
      "Epoch [4/100], Step [1200/17448], Loss: 8.8282\n",
      "Epoch [4/100], Step [1300/17448], Loss: 9.6095\n",
      "Epoch [4/100], Step [1400/17448], Loss: 6.1250\n",
      "Epoch [4/100], Step [1500/17448], Loss: 6.6564\n",
      "Epoch [4/100], Step [1600/17448], Loss: 8.8594\n",
      "Epoch [4/100], Step [1700/17448], Loss: 7.4531\n",
      "Epoch [4/100], Step [1800/17448], Loss: 7.0781\n",
      "Epoch [4/100], Step [1900/17448], Loss: 10.0469\n",
      "Epoch [4/100], Step [2000/17448], Loss: 5.3125\n",
      "Epoch [4/100], Step [2100/17448], Loss: 8.0782\n",
      "Epoch [4/100], Step [2200/17448], Loss: 9.2031\n",
      "Epoch [4/100], Step [2300/17448], Loss: 5.7970\n",
      "Epoch [4/100], Step [2400/17448], Loss: 7.6563\n",
      "Epoch [4/100], Step [2500/17448], Loss: 7.1564\n",
      "Epoch [4/100], Step [2600/17448], Loss: 7.2969\n",
      "Epoch [4/100], Step [2700/17448], Loss: 10.1563\n",
      "Epoch [4/100], Step [2800/17448], Loss: 7.2188\n",
      "Epoch [4/100], Step [2900/17448], Loss: 7.0313\n",
      "Epoch [4/100], Step [3000/17448], Loss: 8.0469\n",
      "Epoch [4/100], Step [3100/17448], Loss: 5.7969\n",
      "Epoch [4/100], Step [3200/17448], Loss: 5.6720\n",
      "Epoch [4/100], Step [3300/17448], Loss: 7.3594\n",
      "Epoch [4/100], Step [3400/17448], Loss: 4.5938\n",
      "Epoch [4/100], Step [3500/17448], Loss: 10.0313\n",
      "Epoch [4/100], Step [3600/17448], Loss: 7.5469\n",
      "Epoch [4/100], Step [3700/17448], Loss: 8.6719\n",
      "Epoch [4/100], Step [3800/17448], Loss: 7.4688\n",
      "Epoch [4/100], Step [3900/17448], Loss: 4.9688\n",
      "Epoch [4/100], Step [4000/17448], Loss: 5467331887104.0000\n",
      "Epoch [4/100], Step [4100/17448], Loss: 9.4845\n",
      "Epoch [4/100], Step [4200/17448], Loss: 6.0938\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [5/100], Step [100/17448], Loss: 10.5000\n",
      "Epoch [5/100], Step [200/17448], Loss: 10.7656\n",
      "Epoch [5/100], Step [300/17448], Loss: 7.0313\n",
      "Epoch [5/100], Step [400/17448], Loss: 8.2188\n",
      "Epoch [5/100], Step [500/17448], Loss: 9.3751\n",
      "Epoch [5/100], Step [600/17448], Loss: 9.5938\n",
      "Epoch [5/100], Step [700/17448], Loss: 7.4844\n",
      "Epoch [5/100], Step [800/17448], Loss: 9.9376\n",
      "Epoch [5/100], Step [900/17448], Loss: 6.9376\n",
      "Epoch [5/100], Step [1000/17448], Loss: 9.1877\n",
      "Epoch [5/100], Step [1100/17448], Loss: 5.6719\n",
      "Epoch [5/100], Step [1200/17448], Loss: 4.4531\n",
      "Epoch [5/100], Step [1300/17448], Loss: 9.0469\n",
      "Epoch [5/100], Step [1400/17448], Loss: 7.7032\n",
      "Epoch [5/100], Step [1500/17448], Loss: 12.4688\n",
      "Epoch [5/100], Step [1600/17448], Loss: 5.8283\n",
      "Epoch [5/100], Step [1700/17448], Loss: 7.4844\n",
      "Epoch [5/100], Step [1800/17448], Loss: 5.9376\n",
      "Epoch [5/100], Step [1900/17448], Loss: 8.7970\n",
      "Epoch [5/100], Step [2000/17448], Loss: 8.0001\n",
      "Epoch [5/100], Step [2100/17448], Loss: 9.5156\n",
      "Epoch [5/100], Step [2200/17448], Loss: 11.4222\n",
      "Epoch [5/100], Step [2300/17448], Loss: 8.4687\n",
      "Epoch [5/100], Step [2400/17448], Loss: 10.8594\n",
      "Epoch [5/100], Step [2500/17448], Loss: 6.1875\n",
      "Epoch [5/100], Step [2600/17448], Loss: 6.2345\n",
      "Epoch [5/100], Step [2700/17448], Loss: 10.1407\n",
      "Epoch [5/100], Step [2800/17448], Loss: 6.9377\n",
      "Epoch [5/100], Step [2900/17448], Loss: 6.7345\n",
      "Epoch [5/100], Step [3000/17448], Loss: 7.3438\n",
      "Epoch [5/100], Step [3100/17448], Loss: 8.0783\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [6/100], Step [100/17448], Loss: 6.4376\n",
      "Epoch [6/100], Step [200/17448], Loss: 9.2656\n",
      "Epoch [6/100], Step [300/17448], Loss: 7.6719\n",
      "Epoch [6/100], Step [400/17448], Loss: 7.3281\n",
      "Epoch [6/100], Step [500/17448], Loss: 8.8750\n",
      "Epoch [6/100], Step [600/17448], Loss: 4.6094\n",
      "Epoch [6/100], Step [700/17448], Loss: 7.8907\n",
      "Epoch [6/100], Step [800/17448], Loss: 9.3594\n",
      "Epoch [6/100], Step [900/17448], Loss: 7.9532\n",
      "Epoch [6/100], Step [1000/17448], Loss: 9.0313\n",
      "Epoch [6/100], Step [1100/17448], Loss: 8.8438\n",
      "Epoch [6/100], Step [1200/17448], Loss: 12.1719\n",
      "Epoch [6/100], Step [1300/17448], Loss: 8.0313\n",
      "Epoch [6/100], Step [1400/17448], Loss: 7.6250\n",
      "Epoch [6/100], Step [1500/17448], Loss: 10.0782\n",
      "Epoch [6/100], Step [1600/17448], Loss: 7.5313\n",
      "Epoch [6/100], Step [1700/17448], Loss: 8.2501\n",
      "Epoch [6/100], Step [1800/17448], Loss: 7.9219\n",
      "Epoch [6/100], Step [1900/17448], Loss: 7.1719\n",
      "Epoch [6/100], Step [2000/17448], Loss: 4.8438\n",
      "Epoch [6/100], Step [2100/17448], Loss: 8.9688\n",
      "Epoch [6/100], Step [2200/17448], Loss: 7.9531\n",
      "Epoch [6/100], Step [2300/17448], Loss: 6.2813\n",
      "Epoch [6/100], Step [2400/17448], Loss: 8.9375\n",
      "Epoch [6/100], Step [2500/17448], Loss: 6.1250\n",
      "Epoch [6/100], Step [2600/17448], Loss: 8.9533\n",
      "Epoch [6/100], Step [2700/17448], Loss: 8.7969\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [7/100], Step [100/17448], Loss: 5.2344\n",
      "Epoch [7/100], Step [200/17448], Loss: 8.5157\n",
      "Epoch [7/100], Step [300/17448], Loss: 5.5628\n",
      "Epoch [7/100], Step [400/17448], Loss: 7.0625\n",
      "Epoch [7/100], Step [500/17448], Loss: 4.7970\n",
      "Epoch [7/100], Step [600/17448], Loss: 7.3438\n",
      "Epoch [7/100], Step [700/17448], Loss: 5.2971\n",
      "Epoch [7/100], Step [800/17448], Loss: 6.8907\n",
      "Epoch [7/100], Step [900/17448], Loss: 6.9533\n",
      "Epoch [7/100], Step [1000/17448], Loss: 6.3909\n",
      "Epoch [7/100], Step [1100/17448], Loss: 8.2813\n",
      "Epoch [7/100], Step [1200/17448], Loss: 7.7188\n",
      "Epoch [7/100], Step [1300/17448], Loss: 8.0469\n",
      "Epoch [7/100], Step [1400/17448], Loss: 7.5313\n",
      "Epoch [7/100], Step [1500/17448], Loss: 7.3438\n",
      "Epoch [7/100], Step [1600/17448], Loss: 5.9375\n",
      "Epoch [7/100], Step [1700/17448], Loss: 7.4375\n",
      "Epoch [7/100], Step [1800/17448], Loss: 8.2188\n",
      "Epoch [7/100], Step [1900/17448], Loss: 10.4375\n",
      "Epoch [7/100], Step [2000/17448], Loss: 6.7500\n",
      "Epoch [7/100], Step [2100/17448], Loss: 8.5470\n",
      "Epoch [7/100], Step [2200/17448], Loss: 8.8282\n",
      "Epoch [7/100], Step [2300/17448], Loss: 6.0938\n",
      "Epoch [7/100], Step [2400/17448], Loss: 11.4688\n",
      "Epoch [7/100], Step [2500/17448], Loss: 7.7500\n",
      "Epoch [7/100], Step [2600/17448], Loss: 7.8594\n",
      "Epoch [7/100], Step [2700/17448], Loss: 9.0938\n",
      "Epoch [7/100], Step [2800/17448], Loss: 8.0470\n",
      "Epoch [7/100], Step [2900/17448], Loss: 8.8595\n",
      "Epoch [7/100], Step [3000/17448], Loss: 9.2656\n",
      "Epoch [7/100], Step [3100/17448], Loss: 6.4063\n",
      "Epoch [7/100], Step [3200/17448], Loss: 5.0469\n",
      "Epoch [7/100], Step [3300/17448], Loss: 8.5626\n",
      "Epoch [7/100], Step [3400/17448], Loss: 8.5157\n",
      "Epoch [7/100], Step [3500/17448], Loss: 9.9844\n",
      "Epoch [7/100], Step [3600/17448], Loss: 10.7501\n",
      "Epoch [7/100], Step [3700/17448], Loss: 9.1250\n",
      "Epoch [7/100], Step [3800/17448], Loss: 9.0782\n",
      "Epoch [7/100], Step [3900/17448], Loss: 7.5313\n",
      "Epoch [7/100], Step [4000/17448], Loss: 5.5469\n",
      "Epoch [7/100], Step [4100/17448], Loss: 11.6251\n",
      "Epoch [7/100], Step [4200/17448], Loss: 8.7813\n",
      "Epoch [7/100], Step [4300/17448], Loss: 7.6720\n",
      "Epoch [7/100], Step [4400/17448], Loss: 5.7345\n",
      "Epoch [7/100], Step [4500/17448], Loss: 8.4688\n",
      "Epoch [7/100], Step [4600/17448], Loss: 10.6250\n",
      "Epoch [7/100], Step [4700/17448], Loss: 7.7813\n",
      "Epoch [7/100], Step [4800/17448], Loss: 11.4531\n",
      "Epoch [7/100], Step [4900/17448], Loss: 9.7657\n",
      "Epoch [7/100], Step [5000/17448], Loss: 6.7500\n",
      "Epoch [7/100], Step [5100/17448], Loss: 10.8594\n",
      "Epoch [7/100], Step [5200/17448], Loss: 10.6719\n",
      "Epoch [7/100], Step [5300/17448], Loss: 4.7033\n",
      "Epoch [7/100], Step [5400/17448], Loss: 10.6875\n",
      "Epoch [7/100], Step [5500/17448], Loss: 6.9220\n",
      "Epoch [7/100], Step [5600/17448], Loss: 7.3438\n",
      "Epoch [7/100], Step [5700/17448], Loss: 10.0625\n",
      "Epoch [7/100], Step [5800/17448], Loss: 6.2188\n",
      "Epoch [7/100], Step [5900/17448], Loss: 6.4220\n",
      "Epoch [7/100], Step [6000/17448], Loss: 8.0626\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [8/100], Step [100/17448], Loss: 8.6563\n",
      "Epoch [8/100], Step [200/17448], Loss: 6.6407\n",
      "Epoch [8/100], Step [300/17448], Loss: 5.6875\n",
      "Epoch [8/100], Step [400/17448], Loss: 7.2189\n",
      "Epoch [8/100], Step [500/17448], Loss: 6.1563\n",
      "Epoch [8/100], Step [600/17448], Loss: 8.2969\n",
      "Epoch [8/100], Step [700/17448], Loss: 6.5000\n",
      "Epoch [8/100], Step [800/17448], Loss: 6.7969\n",
      "Epoch [8/100], Step [900/17448], Loss: 5.7188\n",
      "Epoch [8/100], Step [1000/17448], Loss: 7.9531\n",
      "Epoch [8/100], Step [1100/17448], Loss: 7.5314\n",
      "Epoch [8/100], Step [1200/17448], Loss: 7.7031\n",
      "Epoch [8/100], Step [1300/17448], Loss: 6.8125\n",
      "Epoch [8/100], Step [1400/17448], Loss: 6.5626\n",
      "Epoch [8/100], Step [1500/17448], Loss: 10.9376\n",
      "Epoch [8/100], Step [1600/17448], Loss: 5.0781\n",
      "Epoch [8/100], Step [1700/17448], Loss: 9.3595\n",
      "Epoch [8/100], Step [1800/17448], Loss: 4.7032\n",
      "Epoch [8/100], Step [1900/17448], Loss: 8.8282\n",
      "Epoch [8/100], Step [2000/17448], Loss: 8.6251\n",
      "Epoch [8/100], Step [2100/17448], Loss: 8.2656\n",
      "Epoch [8/100], Step [2200/17448], Loss: 4.2344\n",
      "Epoch [8/100], Step [2300/17448], Loss: 6.3594\n",
      "Epoch [8/100], Step [2400/17448], Loss: 7.6719\n",
      "Epoch [8/100], Step [2500/17448], Loss: 6.2501\n",
      "Epoch [8/100], Step [2600/17448], Loss: 8.6250\n",
      "Epoch [8/100], Step [2700/17448], Loss: 4.7969\n",
      "Epoch [8/100], Step [2800/17448], Loss: 7.2813\n",
      "Epoch [8/100], Step [2900/17448], Loss: 5.4219\n",
      "Epoch [8/100], Step [3000/17448], Loss: 10.1563\n",
      "Epoch [8/100], Step [3100/17448], Loss: 6.2813\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [9/100], Step [100/17448], Loss: 7.0471\n",
      "Epoch [9/100], Step [200/17448], Loss: 4.6252\n",
      "Epoch [9/100], Step [300/17448], Loss: 7.9688\n",
      "Epoch [9/100], Step [400/17448], Loss: 8.7188\n",
      "Epoch [9/100], Step [500/17448], Loss: 6.0625\n",
      "Epoch [9/100], Step [600/17448], Loss: 7.3438\n",
      "Epoch [9/100], Step [700/17448], Loss: 7.0469\n",
      "Epoch [9/100], Step [800/17448], Loss: 8.7500\n",
      "Epoch [9/100], Step [900/17448], Loss: 6.3751\n",
      "Epoch [9/100], Step [1000/17448], Loss: 7.1563\n",
      "Epoch [9/100], Step [1100/17448], Loss: 6.3438\n",
      "Epoch [9/100], Step [1200/17448], Loss: 7.4219\n",
      "Epoch [9/100], Step [1300/17448], Loss: 5.6719\n",
      "Epoch [9/100], Step [1400/17448], Loss: 11.5001\n",
      "Epoch [9/100], Step [1500/17448], Loss: 7.8907\n",
      "Epoch [9/100], Step [1600/17448], Loss: 8.9844\n",
      "Epoch [9/100], Step [1700/17448], Loss: 4.7500\n",
      "Epoch [9/100], Step [1800/17448], Loss: 5.6407\n",
      "Epoch [9/100], Step [1900/17448], Loss: 6.3752\n",
      "Epoch [9/100], Step [2000/17448], Loss: 2.9377\n",
      "Epoch [9/100], Step [2100/17448], Loss: 6.7813\n",
      "Epoch [9/100], Step [2200/17448], Loss: 9.9375\n",
      "Epoch [9/100], Step [2300/17448], Loss: 8.9688\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [10/100], Step [100/17448], Loss: 8.9375\n",
      "Epoch [10/100], Step [200/17448], Loss: 13.3594\n",
      "Epoch [10/100], Step [300/17448], Loss: 9.2656\n",
      "Epoch [10/100], Step [400/17448], Loss: 6.9062\n",
      "Epoch [10/100], Step [500/17448], Loss: 9.2656\n",
      "Epoch [10/100], Step [600/17448], Loss: 9.8125\n",
      "Epoch [10/100], Step [700/17448], Loss: 6.9376\n",
      "Epoch [10/100], Step [800/17448], Loss: 5.5157\n",
      "Epoch [10/100], Step [900/17448], Loss: 7.8595\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [11/100], Step [100/17448], Loss: 4.6250\n",
      "Epoch [11/100], Step [200/17448], Loss: 9.3594\n",
      "Epoch [11/100], Step [300/17448], Loss: 6.6719\n",
      "Epoch [11/100], Step [400/17448], Loss: 5.0783\n",
      "Epoch [11/100], Step [500/17448], Loss: 6.1719\n",
      "Epoch [11/100], Step [600/17448], Loss: 8.1411\n",
      "Epoch [11/100], Step [700/17448], Loss: 7.2502\n",
      "Epoch [11/100], Step [800/17448], Loss: 7.8597\n",
      "Epoch [11/100], Step [900/17448], Loss: 3.1251\n",
      "Epoch [11/100], Step [1000/17448], Loss: 7.9689\n",
      "Epoch [11/100], Step [1100/17448], Loss: 7.5313\n",
      "Epoch [11/100], Step [1200/17448], Loss: 4.1094\n",
      "Epoch [11/100], Step [1300/17448], Loss: 10.0469\n",
      "Epoch [11/100], Step [1400/17448], Loss: 9.0781\n",
      "Epoch [11/100], Step [1500/17448], Loss: 8.0469\n",
      "Epoch [11/100], Step [1600/17448], Loss: 8.4375\n",
      "Epoch [11/100], Step [1700/17448], Loss: 7.7344\n",
      "Epoch [11/100], Step [1800/17448], Loss: 4.7501\n",
      "Epoch [11/100], Step [1900/17448], Loss: 7.7188\n",
      "Epoch [11/100], Step [2000/17448], Loss: 9.1252\n",
      "Epoch [11/100], Step [2100/17448], Loss: 7.3282\n",
      "Epoch [11/100], Step [2200/17448], Loss: 9.2813\n",
      "Epoch [11/100], Step [2300/17448], Loss: 9.5938\n",
      "Epoch [11/100], Step [2400/17448], Loss: 10.0782\n",
      "Epoch [11/100], Step [2500/17448], Loss: 4.3282\n",
      "Epoch [11/100], Step [2600/17448], Loss: 11.4063\n",
      "Epoch [11/100], Step [2700/17448], Loss: 9.0938\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [12/100], Step [100/17448], Loss: 8.4844\n",
      "Epoch [12/100], Step [200/17448], Loss: 7.5156\n",
      "Epoch [12/100], Step [300/17448], Loss: 7.0156\n",
      "Epoch [12/100], Step [400/17448], Loss: 7.1876\n",
      "Epoch [12/100], Step [500/17448], Loss: 9.8750\n",
      "Epoch [12/100], Step [600/17448], Loss: 6.4531\n",
      "Epoch [12/100], Step [700/17448], Loss: 9.2656\n",
      "Epoch [12/100], Step [800/17448], Loss: 12.0625\n",
      "Epoch [12/100], Step [900/17448], Loss: 4.2032\n",
      "Epoch [12/100], Step [1000/17448], Loss: 10.7969\n",
      "Epoch [12/100], Step [1100/17448], Loss: 11.7813\n",
      "Epoch [12/100], Step [1200/17448], Loss: 9.0157\n",
      "Epoch [12/100], Step [1300/17448], Loss: 12.4688\n",
      "Epoch [12/100], Step [1400/17448], Loss: 7.3750\n",
      "Epoch [12/100], Step [1500/17448], Loss: 6.3751\n",
      "Epoch [12/100], Step [1600/17448], Loss: 6.0469\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [13/100], Step [100/17448], Loss: 7.7188\n",
      "Epoch [13/100], Step [200/17448], Loss: 10.0156\n",
      "Epoch [13/100], Step [300/17448], Loss: 5.8906\n",
      "Epoch [13/100], Step [400/17448], Loss: 6.5470\n",
      "Epoch [13/100], Step [500/17448], Loss: 5.3750\n",
      "Epoch [13/100], Step [600/17448], Loss: 6.7970\n",
      "Epoch [13/100], Step [700/17448], Loss: 4.2501\n",
      "Epoch [13/100], Step [800/17448], Loss: 5.8594\n",
      "Epoch [13/100], Step [900/17448], Loss: 7.6719\n",
      "Epoch [13/100], Step [1000/17448], Loss: 8.7500\n",
      "Epoch [13/100], Step [1100/17448], Loss: 8.2658\n",
      "Epoch [13/100], Step [1200/17448], Loss: 4.9532\n",
      "Epoch [13/100], Step [1300/17448], Loss: 8.0000\n",
      "Epoch [13/100], Step [1400/17448], Loss: 10.7031\n",
      "Epoch [13/100], Step [1500/17448], Loss: 6.5313\n",
      "Epoch [13/100], Step [1600/17448], Loss: 10.6719\n",
      "Epoch [13/100], Step [1700/17448], Loss: 9.3594\n",
      "Epoch [13/100], Step [1800/17448], Loss: 7.5313\n",
      "Epoch [13/100], Step [1900/17448], Loss: 7.6407\n",
      "Epoch [13/100], Step [2000/17448], Loss: 4.5158\n",
      "Epoch [13/100], Step [2100/17448], Loss: 6.6095\n",
      "Epoch [13/100], Step [2200/17448], Loss: 5.7500\n",
      "Epoch [13/100], Step [2300/17448], Loss: 5.1406\n",
      "Epoch [13/100], Step [2400/17448], Loss: 8.5001\n",
      "Epoch [13/100], Step [2500/17448], Loss: 4.9688\n",
      "Epoch [13/100], Step [2600/17448], Loss: 7.1876\n",
      "Epoch [13/100], Step [2700/17448], Loss: 7.0469\n",
      "Epoch [13/100], Step [2800/17448], Loss: 10.6093\n",
      "Epoch [13/100], Step [2900/17448], Loss: 6.0782\n",
      "Epoch [13/100], Step [3000/17448], Loss: 7.6719\n",
      "Epoch [13/100], Step [3100/17448], Loss: 6.4220\n",
      "Epoch [13/100], Step [3200/17448], Loss: 7.5157\n",
      "Epoch [13/100], Step [3300/17448], Loss: 10.9688\n",
      "Epoch [13/100], Step [3400/17448], Loss: 3.9219\n",
      "Epoch [13/100], Step [3500/17448], Loss: 10.0000\n",
      "Epoch [13/100], Step [3600/17448], Loss: 7.0938\n",
      "Epoch [13/100], Step [3700/17448], Loss: 10.2501\n",
      "Epoch [13/100], Step [3800/17448], Loss: 11.8751\n",
      "Epoch [13/100], Step [3900/17448], Loss: 7.6564\n",
      "Epoch [13/100], Step [4000/17448], Loss: 7.8906\n",
      "Epoch [13/100], Step [4100/17448], Loss: 8.3910\n",
      "Epoch [13/100], Step [4200/17448], Loss: 7.2657\n",
      "Epoch [13/100], Step [4300/17448], Loss: 10.2032\n",
      "Epoch [13/100], Step [4400/17448], Loss: 8.2813\n",
      "Epoch [13/100], Step [4500/17448], Loss: 6.4688\n",
      "Epoch [13/100], Step [4600/17448], Loss: 8.2659\n",
      "Epoch [13/100], Step [4700/17448], Loss: 9.2345\n",
      "Epoch [13/100], Step [4800/17448], Loss: 5.5000\n",
      "Epoch [13/100], Step [4900/17448], Loss: 7.4844\n",
      "Epoch [13/100], Step [5000/17448], Loss: 9.4375\n",
      "Epoch [13/100], Step [5100/17448], Loss: 8.5001\n",
      "Epoch [13/100], Step [5200/17448], Loss: 10.1875\n",
      "Epoch [13/100], Step [5300/17448], Loss: 3.6250\n",
      "Epoch [13/100], Step [5400/17448], Loss: 11.0469\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [14/100], Step [100/17448], Loss: 7.8126\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [15/100], Step [100/17448], Loss: 5.9845\n",
      "Epoch [15/100], Step [200/17448], Loss: 6.8282\n",
      "Epoch [15/100], Step [300/17448], Loss: 8.0000\n",
      "Epoch [15/100], Step [400/17448], Loss: 7.3126\n",
      "Epoch [15/100], Step [500/17448], Loss: 5.9531\n",
      "Epoch [15/100], Step [600/17448], Loss: 5.3283\n",
      "Epoch [15/100], Step [700/17448], Loss: 8.8438\n",
      "Epoch [15/100], Step [800/17448], Loss: 8.3594\n",
      "Epoch [15/100], Step [900/17448], Loss: 10.6406\n",
      "Epoch [15/100], Step [1000/17448], Loss: 6.1408\n",
      "Epoch [15/100], Step [1100/17448], Loss: 5.9375\n",
      "Epoch [15/100], Step [1200/17448], Loss: 7.7813\n",
      "Epoch [15/100], Step [1300/17448], Loss: 8.1250\n",
      "Epoch [15/100], Step [1400/17448], Loss: 9.0938\n",
      "Epoch [15/100], Step [1500/17448], Loss: 4.7500\n",
      "Epoch [15/100], Step [1600/17448], Loss: 9.0156\n",
      "Epoch [15/100], Step [1700/17448], Loss: 6.7345\n",
      "Epoch [15/100], Step [1800/17448], Loss: 6.5469\n",
      "Epoch [15/100], Step [1900/17448], Loss: 5.0313\n",
      "Epoch [15/100], Step [2000/17448], Loss: 7.5470\n",
      "Epoch [15/100], Step [2100/17448], Loss: 8.8438\n",
      "Epoch [15/100], Step [2200/17448], Loss: 12.1406\n",
      "Epoch [15/100], Step [2300/17448], Loss: 6.8594\n",
      "Epoch [15/100], Step [2400/17448], Loss: 5.3907\n",
      "Epoch [15/100], Step [2500/17448], Loss: 8.8751\n",
      "Epoch [15/100], Step [2600/17448], Loss: 8.7032\n",
      "Epoch [15/100], Step [2700/17448], Loss: 6.2656\n",
      "Epoch [15/100], Step [2800/17448], Loss: 11.3594\n",
      "Epoch [15/100], Step [2900/17448], Loss: 6.4532\n",
      "Epoch [15/100], Step [3000/17448], Loss: 6.0157\n",
      "Epoch [15/100], Step [3100/17448], Loss: 9.9375\n",
      "Epoch [15/100], Step [3200/17448], Loss: 5.6250\n",
      "Epoch [15/100], Step [3300/17448], Loss: 6.5938\n",
      "Epoch [15/100], Step [3400/17448], Loss: 6.0313\n",
      "Epoch [15/100], Step [3500/17448], Loss: 6.3751\n",
      "Epoch [15/100], Step [3600/17448], Loss: 7.0000\n",
      "Epoch [15/100], Step [3700/17448], Loss: 7.4063\n",
      "Epoch [15/100], Step [3800/17448], Loss: 5.5782\n",
      "Epoch [15/100], Step [3900/17448], Loss: 8.9064\n",
      "Epoch [15/100], Step [4000/17448], Loss: 5.6250\n",
      "Epoch [15/100], Step [4100/17448], Loss: 6.0782\n",
      "Epoch [15/100], Step [4200/17448], Loss: 7.4219\n",
      "Epoch [15/100], Step [4300/17448], Loss: 11.5781\n",
      "Epoch [15/100], Step [4400/17448], Loss: 6.7032\n",
      "Epoch [15/100], Step [4500/17448], Loss: 8.8907\n",
      "Epoch [15/100], Step [4600/17448], Loss: 8.0156\n",
      "Epoch [15/100], Step [4700/17448], Loss: 10.1094\n",
      "Epoch [15/100], Step [4800/17448], Loss: 3.7656\n",
      "Epoch [15/100], Step [4900/17448], Loss: 12.0626\n",
      "Epoch [15/100], Step [5000/17448], Loss: 9.0158\n",
      "Epoch [15/100], Step [5100/17448], Loss: 12.9532\n",
      "Epoch [15/100], Step [5200/17448], Loss: 7.1563\n",
      "Epoch [15/100], Step [5300/17448], Loss: 10.0156\n",
      "Epoch [15/100], Step [5400/17448], Loss: 5.5473\n",
      "Epoch [15/100], Step [5500/17448], Loss: 7.0002\n",
      "Epoch [15/100], Step [5600/17448], Loss: 6.7658\n",
      "Epoch [15/100], Step [5700/17448], Loss: 7.2814\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [16/100], Step [100/17448], Loss: 4.8438\n",
      "Epoch [16/100], Step [200/17448], Loss: 6.2658\n",
      "Epoch [16/100], Step [300/17448], Loss: 6.6565\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [17/100], Step [100/17448], Loss: 9.3282\n",
      "Epoch [17/100], Step [200/17448], Loss: 7.1407\n",
      "Epoch [17/100], Step [300/17448], Loss: 8.9531\n",
      "Epoch [17/100], Step [400/17448], Loss: 10.7656\n",
      "Epoch [17/100], Step [500/17448], Loss: 9.3439\n",
      "Epoch [17/100], Step [600/17448], Loss: 7.4375\n",
      "Epoch [17/100], Step [700/17448], Loss: 8.1563\n",
      "Epoch [17/100], Step [800/17448], Loss: 8.8281\n",
      "Epoch [17/100], Step [900/17448], Loss: 11.3751\n",
      "Epoch [17/100], Step [1000/17448], Loss: 7.3594\n",
      "Epoch [17/100], Step [1100/17448], Loss: 9.1406\n",
      "Epoch [17/100], Step [1200/17448], Loss: 7.3438\n",
      "Epoch [17/100], Step [1300/17448], Loss: 6.8751\n",
      "Epoch [17/100], Step [1400/17448], Loss: 5.7500\n",
      "Epoch [17/100], Step [1500/17448], Loss: 10.4844\n",
      "Epoch [17/100], Step [1600/17448], Loss: 10.0156\n",
      "Epoch [17/100], Step [1700/17448], Loss: 7.6250\n",
      "Epoch [17/100], Step [1800/17448], Loss: 5.8907\n",
      "Epoch [17/100], Step [1900/17448], Loss: 10.1719\n",
      "Epoch [17/100], Step [2000/17448], Loss: 5.8594\n",
      "Epoch [17/100], Step [2100/17448], Loss: 8.1876\n",
      "Epoch [17/100], Step [2200/17448], Loss: 6.5000\n",
      "Epoch [17/100], Step [2300/17448], Loss: 6.3438\n",
      "Epoch [17/100], Step [2400/17448], Loss: 7.8438\n",
      "Epoch [17/100], Step [2500/17448], Loss: 7.0781\n",
      "Epoch [17/100], Step [2600/17448], Loss: 10.6719\n",
      "Epoch [17/100], Step [2700/17448], Loss: 7.5313\n",
      "Epoch [17/100], Step [2800/17448], Loss: 7.6719\n",
      "Epoch [17/100], Step [2900/17448], Loss: 4.1250\n",
      "Epoch [17/100], Step [3000/17448], Loss: 7.7501\n",
      "Epoch [17/100], Step [3100/17448], Loss: 9.6875\n",
      "Epoch [17/100], Step [3200/17448], Loss: 9.0938\n",
      "Epoch [17/100], Step [3300/17448], Loss: 6.1875\n",
      "Epoch [17/100], Step [3400/17448], Loss: 7.1250\n",
      "Epoch [17/100], Step [3500/17448], Loss: 8.4220\n",
      "Epoch [17/100], Step [3600/17448], Loss: 10.2969\n",
      "Epoch [17/100], Step [3700/17448], Loss: 4.2813\n",
      "Epoch [17/100], Step [3800/17448], Loss: 8.2188\n",
      "Epoch [17/100], Step [3900/17448], Loss: 7.5782\n",
      "Epoch [17/100], Step [4000/17448], Loss: 7.9844\n",
      "Epoch [17/100], Step [4100/17448], Loss: 8.4688\n",
      "Epoch [17/100], Step [4200/17448], Loss: 11.2969\n",
      "Epoch [17/100], Step [4300/17448], Loss: 5.7188\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [18/100], Step [100/17448], Loss: 7.7188\n",
      "Epoch [18/100], Step [200/17448], Loss: 7.7813\n",
      "Epoch [18/100], Step [300/17448], Loss: 6.7813\n",
      "Epoch [18/100], Step [400/17448], Loss: 8.3750\n",
      "Epoch [18/100], Step [500/17448], Loss: 10.1876\n",
      "Epoch [18/100], Step [600/17448], Loss: 8.6563\n",
      "Epoch [18/100], Step [700/17448], Loss: 7.3126\n",
      "Epoch [18/100], Step [800/17448], Loss: 6.0157\n",
      "Epoch [18/100], Step [900/17448], Loss: 5.9376\n",
      "Epoch [18/100], Step [1000/17448], Loss: 5.5627\n",
      "Epoch [18/100], Step [1100/17448], Loss: 7.4219\n",
      "Epoch [18/100], Step [1200/17448], Loss: 11.1095\n",
      "Epoch [18/100], Step [1300/17448], Loss: 8.3282\n",
      "Epoch [18/100], Step [1400/17448], Loss: 6.8281\n",
      "Epoch [18/100], Step [1500/17448], Loss: 6.7031\n",
      "Epoch [18/100], Step [1600/17448], Loss: 7.1408\n",
      "Epoch [18/100], Step [1700/17448], Loss: 9.5469\n",
      "Epoch [18/100], Step [1800/17448], Loss: 7.9219\n",
      "Epoch [18/100], Step [1900/17448], Loss: 6.8906\n",
      "Epoch [18/100], Step [2000/17448], Loss: 7.3282\n",
      "Epoch [18/100], Step [2100/17448], Loss: 6.1094\n",
      "Epoch [18/100], Step [2200/17448], Loss: 6.0313\n",
      "Epoch [18/100], Step [2300/17448], Loss: 6.9219\n",
      "Epoch [18/100], Step [2400/17448], Loss: 5.4063\n",
      "Epoch [18/100], Step [2500/17448], Loss: 9.9219\n",
      "Epoch [18/100], Step [2600/17448], Loss: 7.5782\n",
      "Epoch [18/100], Step [2700/17448], Loss: 11.4531\n",
      "Epoch [18/100], Step [2800/17448], Loss: 6.5469\n",
      "Epoch [18/100], Step [2900/17448], Loss: 9.8906\n",
      "Epoch [18/100], Step [3000/17448], Loss: 7.2033\n",
      "Epoch [18/100], Step [3100/17448], Loss: 7.6251\n",
      "Epoch [18/100], Step [3200/17448], Loss: 9.1719\n",
      "Epoch [18/100], Step [3300/17448], Loss: 8.6250\n",
      "Epoch [18/100], Step [3400/17448], Loss: 8.5000\n",
      "Epoch [18/100], Step [3500/17448], Loss: 5.6408\n",
      "Epoch [18/100], Step [3600/17448], Loss: 5.0938\n",
      "Epoch [18/100], Step [3700/17448], Loss: 6.8907\n",
      "Epoch [18/100], Step [3800/17448], Loss: 10.3750\n",
      "Epoch [18/100], Step [3900/17448], Loss: 7.6878\n",
      "Epoch [18/100], Step [4000/17448], Loss: 6.7970\n",
      "Epoch [18/100], Step [4100/17448], Loss: 8.2189\n",
      "Epoch [18/100], Step [4200/17448], Loss: 7.2500\n",
      "Epoch [18/100], Step [4300/17448], Loss: 7.8282\n",
      "Epoch [18/100], Step [4400/17448], Loss: 7.0938\n",
      "Epoch [18/100], Step [4500/17448], Loss: 6.2500\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [19/100], Step [100/17448], Loss: 8.9063\n",
      "Epoch [19/100], Step [200/17448], Loss: 8.6407\n",
      "Epoch [19/100], Step [300/17448], Loss: 8.2969\n",
      "Epoch [19/100], Step [400/17448], Loss: 10.8282\n",
      "Epoch [19/100], Step [500/17448], Loss: 8.7500\n",
      "Epoch [19/100], Step [600/17448], Loss: 9.0156\n",
      "Epoch [19/100], Step [700/17448], Loss: 6.8438\n",
      "Epoch [19/100], Step [800/17448], Loss: 7.4845\n",
      "Epoch [19/100], Step [900/17448], Loss: 9.2657\n",
      "Epoch [19/100], Step [1000/17448], Loss: 5.9219\n",
      "Epoch [19/100], Step [1100/17448], Loss: 5.0157\n",
      "Epoch [19/100], Step [1200/17448], Loss: 5.5625\n",
      "Epoch [19/100], Step [1300/17448], Loss: 4.9063\n",
      "Epoch [19/100], Step [1400/17448], Loss: 8.3595\n",
      "Epoch [19/100], Step [1500/17448], Loss: 6.8282\n",
      "Epoch [19/100], Step [1600/17448], Loss: 6.4219\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [20/100], Step [100/17448], Loss: 10.7813\n",
      "Epoch [20/100], Step [200/17448], Loss: 7.2188\n",
      "Epoch [20/100], Step [300/17448], Loss: 7.1719\n",
      "Epoch [20/100], Step [400/17448], Loss: 6.9688\n",
      "Epoch [20/100], Step [500/17448], Loss: 10.9375\n",
      "Epoch [20/100], Step [600/17448], Loss: 9.9688\n",
      "Epoch [20/100], Step [700/17448], Loss: 4.2500\n",
      "Epoch [20/100], Step [800/17448], Loss: 5.5626\n",
      "Epoch [20/100], Step [900/17448], Loss: 7.1407\n",
      "Epoch [20/100], Step [1000/17448], Loss: 7.5938\n",
      "Epoch [20/100], Step [1100/17448], Loss: 8.7188\n",
      "Epoch [20/100], Step [1200/17448], Loss: 6.0157\n",
      "Epoch [20/100], Step [1300/17448], Loss: 10.0626\n",
      "Epoch [20/100], Step [1400/17448], Loss: 8.7814\n",
      "Epoch [20/100], Step [1500/17448], Loss: 7.9063\n",
      "Epoch [20/100], Step [1600/17448], Loss: 7.0625\n",
      "Epoch [20/100], Step [1700/17448], Loss: 8.2188\n",
      "Epoch [20/100], Step [1800/17448], Loss: 8.5313\n",
      "Epoch [20/100], Step [1900/17448], Loss: 9.2657\n",
      "Epoch [20/100], Step [2000/17448], Loss: 10.7656\n",
      "Epoch [20/100], Step [2100/17448], Loss: 8.2500\n",
      "Epoch [20/100], Step [2200/17448], Loss: 9.8283\n",
      "Epoch [20/100], Step [2300/17448], Loss: 8.1250\n",
      "Epoch [20/100], Step [2400/17448], Loss: 7.1094\n",
      "Epoch [20/100], Step [2500/17448], Loss: 10.3594\n",
      "Epoch [20/100], Step [2600/17448], Loss: 7.7188\n",
      "Epoch [20/100], Step [2700/17448], Loss: 9.0781\n",
      "Epoch [20/100], Step [2800/17448], Loss: 6.8438\n",
      "Epoch [20/100], Step [2900/17448], Loss: 9.7188\n",
      "Epoch [20/100], Step [3000/17448], Loss: 7.3438\n",
      "Epoch [20/100], Step [3100/17448], Loss: 10.7657\n",
      "Epoch [20/100], Step [3200/17448], Loss: 7.6719\n",
      "Epoch [20/100], Step [3300/17448], Loss: 8.2188\n",
      "Epoch [20/100], Step [3400/17448], Loss: 6.3126\n",
      "Epoch [20/100], Step [3500/17448], Loss: 7.0157\n",
      "Epoch [20/100], Step [3600/17448], Loss: 6.7031\n",
      "Epoch [20/100], Step [3700/17448], Loss: 8.8126\n",
      "Epoch [20/100], Step [3800/17448], Loss: 7.7658\n",
      "Epoch [20/100], Step [3900/17448], Loss: 12.3750\n",
      "Epoch [20/100], Step [4000/17448], Loss: 5.3750\n",
      "Epoch [20/100], Step [4100/17448], Loss: 7.2813\n",
      "Epoch [20/100], Step [4200/17448], Loss: 8.5937\n",
      "Epoch [20/100], Step [4300/17448], Loss: 8.9376\n",
      "Epoch [20/100], Step [4400/17448], Loss: 6.9531\n",
      "Epoch [20/100], Step [4500/17448], Loss: 6.4219\n",
      "Epoch [20/100], Step [4600/17448], Loss: 8.4062\n",
      "Epoch [20/100], Step [4700/17448], Loss: 6.8282\n",
      "Epoch [20/100], Step [4800/17448], Loss: 8.8595\n",
      "Epoch [20/100], Step [4900/17448], Loss: 7.2188\n",
      "Epoch [20/100], Step [5000/17448], Loss: 7.7969\n",
      "Epoch [20/100], Step [5100/17448], Loss: 7.7500\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [21/100], Step [100/17448], Loss: 9.6251\n",
      "Epoch [21/100], Step [200/17448], Loss: 9.1563\n",
      "Epoch [21/100], Step [300/17448], Loss: 6.6406\n",
      "Epoch [21/100], Step [400/17448], Loss: 10.1563\n",
      "Epoch [21/100], Step [500/17448], Loss: 8.5000\n",
      "Epoch [21/100], Step [600/17448], Loss: 7.6407\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [22/100], Step [100/17448], Loss: 7.4845\n",
      "Epoch [22/100], Step [200/17448], Loss: 4.8594\n",
      "Epoch [22/100], Step [300/17448], Loss: 7.3282\n",
      "Epoch [22/100], Step [400/17448], Loss: 5.3906\n",
      "Epoch [22/100], Step [500/17448], Loss: 7.7971\n",
      "Epoch [22/100], Step [600/17448], Loss: 6.4844\n",
      "Epoch [22/100], Step [700/17448], Loss: 6.1406\n",
      "Epoch [22/100], Step [800/17448], Loss: 5.7656\n",
      "Epoch [22/100], Step [900/17448], Loss: 11.3907\n",
      "Epoch [22/100], Step [1000/17448], Loss: 7.9532\n",
      "Epoch [22/100], Step [1100/17448], Loss: 7.7969\n",
      "Epoch [22/100], Step [1200/17448], Loss: 9.9376\n",
      "Epoch [22/100], Step [1300/17448], Loss: 7.7032\n",
      "Epoch [22/100], Step [1400/17448], Loss: 8.5313\n",
      "Epoch [22/100], Step [1500/17448], Loss: 5.7189\n",
      "Epoch [22/100], Step [1600/17448], Loss: 7.2501\n",
      "Epoch [22/100], Step [1700/17448], Loss: 7.8750\n",
      "Epoch [22/100], Step [1800/17448], Loss: 6.4532\n",
      "Epoch [22/100], Step [1900/17448], Loss: 7.0470\n",
      "Epoch [22/100], Step [2000/17448], Loss: 7.6719\n",
      "Epoch [22/100], Step [2100/17448], Loss: 7.1406\n",
      "Epoch [22/100], Step [2200/17448], Loss: 5.1875\n",
      "Epoch [22/100], Step [2300/17448], Loss: 8.0938\n",
      "Epoch [22/100], Step [2400/17448], Loss: 5.5938\n",
      "Epoch [22/100], Step [2500/17448], Loss: 7.9688\n",
      "Epoch [22/100], Step [2600/17448], Loss: 5.0625\n",
      "Epoch [22/100], Step [2700/17448], Loss: 6.6250\n",
      "Epoch [22/100], Step [2800/17448], Loss: 9.7813\n",
      "Epoch [22/100], Step [2900/17448], Loss: 6.6251\n",
      "Epoch [22/100], Step [3000/17448], Loss: 7.3595\n",
      "Epoch [22/100], Step [3100/17448], Loss: 5.4375\n",
      "Epoch [22/100], Step [3200/17448], Loss: 8.0000\n",
      "Epoch [22/100], Step [3300/17448], Loss: 9.2657\n",
      "Epoch [22/100], Step [3400/17448], Loss: 6.6563\n",
      "Epoch [22/100], Step [3500/17448], Loss: 7.7032\n",
      "Epoch [22/100], Step [3600/17448], Loss: 8.7969\n",
      "Epoch [22/100], Step [3700/17448], Loss: 8.9063\n",
      "Epoch [22/100], Step [3800/17448], Loss: 10.0938\n",
      "Epoch [22/100], Step [3900/17448], Loss: 9.9531\n",
      "Epoch [22/100], Step [4000/17448], Loss: 10.8908\n",
      "Epoch [22/100], Step [4100/17448], Loss: 8.8750\n",
      "Epoch [22/100], Step [4200/17448], Loss: 8.7657\n",
      "Epoch [22/100], Step [4300/17448], Loss: 8.8126\n",
      "Epoch [22/100], Step [4400/17448], Loss: 8.5000\n",
      "Epoch [22/100], Step [4500/17448], Loss: 11.7188\n",
      "Epoch [22/100], Step [4600/17448], Loss: 5.8283\n",
      "Epoch [22/100], Step [4700/17448], Loss: 9.7344\n",
      "Epoch [22/100], Step [4800/17448], Loss: 6.4688\n",
      "Epoch [22/100], Step [4900/17448], Loss: 9.0783\n",
      "Epoch [22/100], Step [5000/17448], Loss: 5.1250\n",
      "Epoch [22/100], Step [5100/17448], Loss: 5.7813\n",
      "Epoch [22/100], Step [5200/17448], Loss: 6.9688\n",
      "Epoch [22/100], Step [5300/17448], Loss: 9.5312\n",
      "Epoch [22/100], Step [5400/17448], Loss: 8.5469\n",
      "Epoch [22/100], Step [5500/17448], Loss: 6.7813\n",
      "Epoch [22/100], Step [5600/17448], Loss: 10.2969\n",
      "Epoch [22/100], Step [5700/17448], Loss: 9.8906\n",
      "Epoch [22/100], Step [5800/17448], Loss: 9.2188\n",
      "Epoch [22/100], Step [5900/17448], Loss: 11.3750\n",
      "Epoch [22/100], Step [6000/17448], Loss: 6.4689\n",
      "Epoch [22/100], Step [6100/17448], Loss: 7.0938\n",
      "Epoch [22/100], Step [6200/17448], Loss: 6.8594\n",
      "Epoch [22/100], Step [6300/17448], Loss: 12.1563\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [23/100], Step [100/17448], Loss: 5.7344\n",
      "Epoch [23/100], Step [200/17448], Loss: 5.2188\n",
      "Epoch [23/100], Step [300/17448], Loss: 8.4688\n",
      "Epoch [23/100], Step [400/17448], Loss: 6.7500\n",
      "Epoch [23/100], Step [500/17448], Loss: 6.5782\n",
      "Epoch [23/100], Step [600/17448], Loss: 6.4063\n",
      "Epoch [23/100], Step [700/17448], Loss: 5.5313\n",
      "Epoch [23/100], Step [800/17448], Loss: 6.2970\n",
      "Epoch [23/100], Step [900/17448], Loss: 8.1875\n",
      "Epoch [23/100], Step [1000/17448], Loss: 10.4063\n",
      "Epoch [23/100], Step [1100/17448], Loss: 6.8438\n",
      "Epoch [23/100], Step [1200/17448], Loss: 5.5157\n",
      "Epoch [23/100], Step [1300/17448], Loss: 6.3125\n",
      "Epoch [23/100], Step [1400/17448], Loss: 5.5782\n",
      "Epoch [23/100], Step [1500/17448], Loss: 8.9531\n",
      "Epoch [23/100], Step [1600/17448], Loss: 8.1095\n",
      "Epoch [23/100], Step [1700/17448], Loss: 7.4532\n",
      "Epoch [23/100], Step [1800/17448], Loss: 9.8126\n",
      "Epoch [23/100], Step [1900/17448], Loss: 6.4844\n",
      "Epoch [23/100], Step [2000/17448], Loss: 8.1407\n",
      "Epoch [23/100], Step [2100/17448], Loss: 9.2813\n",
      "Epoch [23/100], Step [2200/17448], Loss: 7.3282\n",
      "Epoch [23/100], Step [2300/17448], Loss: 8.3282\n",
      "Epoch [23/100], Step [2400/17448], Loss: 7.0938\n",
      "Epoch [23/100], Step [2500/17448], Loss: 7.9064\n",
      "Epoch [23/100], Step [2600/17448], Loss: 8.4688\n",
      "Epoch [23/100], Step [2700/17448], Loss: 6.0938\n",
      "Epoch [23/100], Step [2800/17448], Loss: 6.5313\n",
      "Epoch [23/100], Step [2900/17448], Loss: 7.2033\n",
      "Epoch [23/100], Step [3000/17448], Loss: 8.8437\n",
      "Epoch [23/100], Step [3100/17448], Loss: 7.4220\n",
      "Epoch [23/100], Step [3200/17448], Loss: 8.4219\n",
      "Epoch [23/100], Step [3300/17448], Loss: 6.0938\n",
      "Epoch [23/100], Step [3400/17448], Loss: 7.4063\n",
      "Epoch [23/100], Step [3500/17448], Loss: 5.4689\n",
      "Epoch [23/100], Step [3600/17448], Loss: 4.8907\n",
      "Epoch [23/100], Step [3700/17448], Loss: 8.0782\n",
      "Epoch [23/100], Step [3800/17448], Loss: 10.5000\n",
      "Epoch [23/100], Step [3900/17448], Loss: 7.6720\n",
      "Epoch [23/100], Step [4000/17448], Loss: 8.0313\n",
      "Epoch [23/100], Step [4100/17448], Loss: 7.4063\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [24/100], Step [100/17448], Loss: 3.6409\n",
      "Epoch [24/100], Step [200/17448], Loss: 8.6875\n",
      "Epoch [24/100], Step [300/17448], Loss: 6.9219\n",
      "Epoch [24/100], Step [400/17448], Loss: 11.2032\n",
      "Epoch [24/100], Step [500/17448], Loss: 10.4064\n",
      "Epoch [24/100], Step [600/17448], Loss: 8.8439\n",
      "Epoch [24/100], Step [700/17448], Loss: 6.9221\n",
      "Epoch [24/100], Step [800/17448], Loss: 7.1875\n",
      "Epoch [24/100], Step [900/17448], Loss: 6.8125\n",
      "Epoch [24/100], Step [1000/17448], Loss: 9.8907\n",
      "Epoch [24/100], Step [1100/17448], Loss: 9.6875\n",
      "Epoch [24/100], Step [1200/17448], Loss: 5.9844\n",
      "Epoch [24/100], Step [1300/17448], Loss: 6.7657\n",
      "Epoch [24/100], Step [1400/17448], Loss: 9.7501\n",
      "Epoch [24/100], Step [1500/17448], Loss: 10.4844\n",
      "Epoch [24/100], Step [1600/17448], Loss: 7.2502\n",
      "Epoch [24/100], Step [1700/17448], Loss: 8.0000\n",
      "Epoch [24/100], Step [1800/17448], Loss: 7.2500\n",
      "Epoch [24/100], Step [1900/17448], Loss: 6.2189\n",
      "Epoch [24/100], Step [2000/17448], Loss: 8.7031\n",
      "Epoch [24/100], Step [2100/17448], Loss: 6.3594\n",
      "Epoch [24/100], Step [2200/17448], Loss: 11.6875\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [25/100], Step [100/17448], Loss: 6.2188\n",
      "Epoch [25/100], Step [200/17448], Loss: 11.2657\n",
      "Epoch [25/100], Step [300/17448], Loss: 7.5156\n",
      "Epoch [25/100], Step [400/17448], Loss: 7.7657\n",
      "Epoch [25/100], Step [500/17448], Loss: 8.2969\n",
      "Epoch [25/100], Step [600/17448], Loss: 8.7813\n",
      "Epoch [25/100], Step [700/17448], Loss: 9.2501\n",
      "Epoch [25/100], Step [800/17448], Loss: 7.5156\n",
      "Epoch [25/100], Step [900/17448], Loss: 10.0156\n",
      "Epoch [25/100], Step [1000/17448], Loss: 8.9219\n",
      "Epoch [25/100], Step [1100/17448], Loss: 7.0157\n",
      "Epoch [25/100], Step [1200/17448], Loss: 9.8438\n",
      "Epoch [25/100], Step [1300/17448], Loss: 10.0625\n",
      "Epoch [25/100], Step [1400/17448], Loss: 8.4531\n",
      "Epoch [25/100], Step [1500/17448], Loss: 9.5000\n",
      "Epoch [25/100], Step [1600/17448], Loss: 12.5156\n",
      "Epoch [25/100], Step [1700/17448], Loss: 9.4844\n",
      "Epoch [25/100], Step [1800/17448], Loss: 5.8907\n",
      "Epoch [25/100], Step [1900/17448], Loss: 6.7657\n",
      "Epoch [25/100], Step [2000/17448], Loss: 7.4531\n",
      "Epoch [25/100], Step [2100/17448], Loss: 4.4066\n",
      "Epoch [25/100], Step [2200/17448], Loss: 5.9532\n",
      "Epoch [25/100], Step [2300/17448], Loss: 4.9375\n",
      "Epoch [25/100], Step [2400/17448], Loss: 7.6250\n",
      "Epoch [25/100], Step [2500/17448], Loss: 7.8750\n",
      "Epoch [25/100], Step [2600/17448], Loss: 7.3282\n",
      "Epoch [25/100], Step [2700/17448], Loss: 7.1879\n",
      "Epoch [25/100], Step [2800/17448], Loss: 6.1250\n",
      "Epoch [25/100], Step [2900/17448], Loss: 14.4531\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [26/100], Step [100/17448], Loss: 9.7969\n",
      "Epoch [26/100], Step [200/17448], Loss: 6.5156\n",
      "Epoch [26/100], Step [300/17448], Loss: 6.5938\n",
      "Epoch [26/100], Step [400/17448], Loss: 6.9532\n",
      "Epoch [26/100], Step [500/17448], Loss: 8.6719\n",
      "Epoch [26/100], Step [600/17448], Loss: 10.5781\n",
      "Epoch [26/100], Step [700/17448], Loss: 10.3906\n",
      "Epoch [26/100], Step [800/17448], Loss: 10.8438\n",
      "Epoch [26/100], Step [900/17448], Loss: 8.3282\n",
      "Epoch [26/100], Step [1000/17448], Loss: 6.4844\n",
      "Epoch [26/100], Step [1100/17448], Loss: 8.0782\n",
      "Epoch [26/100], Step [1200/17448], Loss: 6.6094\n",
      "Epoch [26/100], Step [1300/17448], Loss: 5.5939\n",
      "Epoch [26/100], Step [1400/17448], Loss: 8.7500\n",
      "Epoch [26/100], Step [1500/17448], Loss: 11.8907\n",
      "Epoch [26/100], Step [1600/17448], Loss: 8.4532\n",
      "Epoch [26/100], Step [1700/17448], Loss: 4.8751\n",
      "Epoch [26/100], Step [1800/17448], Loss: 6.2500\n",
      "Epoch [26/100], Step [1900/17448], Loss: 5.2031\n",
      "Epoch [26/100], Step [2000/17448], Loss: 7.6875\n",
      "Epoch [26/100], Step [2100/17448], Loss: 5.7813\n",
      "Epoch [26/100], Step [2200/17448], Loss: 7.3438\n",
      "Epoch [26/100], Step [2300/17448], Loss: 7.2345\n",
      "Epoch [26/100], Step [2400/17448], Loss: 7.0470\n",
      "Epoch [26/100], Step [2500/17448], Loss: 6.5313\n",
      "Epoch [26/100], Step [2600/17448], Loss: 7.1563\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [27/100], Step [100/17448], Loss: 7.1719\n",
      "Epoch [27/100], Step [200/17448], Loss: 11.9220\n",
      "Epoch [27/100], Step [300/17448], Loss: 9.7656\n",
      "Epoch [27/100], Step [400/17448], Loss: 5.2344\n",
      "Epoch [27/100], Step [500/17448], Loss: 6.0313\n",
      "Epoch [27/100], Step [600/17448], Loss: 6.9220\n",
      "Epoch [27/100], Step [700/17448], Loss: 7.8750\n",
      "Epoch [27/100], Step [800/17448], Loss: 7.8125\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [28/100], Step [100/17448], Loss: 5.1250\n",
      "Epoch [28/100], Step [200/17448], Loss: 9.3281\n",
      "Epoch [28/100], Step [300/17448], Loss: 9.7033\n",
      "Epoch [28/100], Step [400/17448], Loss: 5.9064\n",
      "Epoch [28/100], Step [500/17448], Loss: 5.2813\n",
      "Epoch [28/100], Step [600/17448], Loss: 7.0313\n",
      "Epoch [28/100], Step [700/17448], Loss: 8.6250\n",
      "Epoch [28/100], Step [800/17448], Loss: 7.7188\n",
      "Epoch [28/100], Step [900/17448], Loss: 7.3595\n",
      "Epoch [28/100], Step [1000/17448], Loss: 6.1407\n",
      "Epoch [28/100], Step [1100/17448], Loss: 6.5781\n",
      "Epoch [28/100], Step [1200/17448], Loss: 8.2188\n",
      "Epoch [28/100], Step [1300/17448], Loss: 8.3438\n",
      "Epoch [28/100], Step [1400/17448], Loss: 7.1721\n",
      "Epoch [28/100], Step [1500/17448], Loss: 8.5469\n",
      "Epoch [28/100], Step [1600/17448], Loss: 9.2031\n",
      "Epoch [28/100], Step [1700/17448], Loss: 7.6875\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [29/100], Step [100/17448], Loss: 9.5625\n",
      "Epoch [29/100], Step [200/17448], Loss: 6.2656\n",
      "Epoch [29/100], Step [300/17448], Loss: 7.1407\n",
      "Epoch [29/100], Step [400/17448], Loss: 10.8438\n",
      "Epoch [29/100], Step [500/17448], Loss: 7.0938\n",
      "Epoch [29/100], Step [600/17448], Loss: 5.6095\n",
      "Epoch [29/100], Step [700/17448], Loss: 9.0157\n",
      "Epoch [29/100], Step [800/17448], Loss: 7.7032\n",
      "Epoch [29/100], Step [900/17448], Loss: 8.0158\n",
      "Epoch [29/100], Step [1000/17448], Loss: 6.4688\n",
      "Epoch [29/100], Step [1100/17448], Loss: 7.5782\n",
      "Epoch [29/100], Step [1200/17448], Loss: 8.9063\n",
      "Epoch [29/100], Step [1300/17448], Loss: 8.6251\n",
      "Epoch [29/100], Step [1400/17448], Loss: 6.1406\n",
      "Epoch [29/100], Step [1500/17448], Loss: 7.5625\n",
      "Epoch [29/100], Step [1600/17448], Loss: 7.9846\n",
      "Epoch [29/100], Step [1700/17448], Loss: 8.8750\n",
      "Epoch [29/100], Step [1800/17448], Loss: 5.2033\n",
      "Epoch [29/100], Step [1900/17448], Loss: 11.5938\n",
      "Epoch [29/100], Step [2000/17448], Loss: 9.5000\n",
      "Epoch [29/100], Step [2100/17448], Loss: 5.7813\n",
      "Epoch [29/100], Step [2200/17448], Loss: 8.0781\n",
      "Epoch [29/100], Step [2300/17448], Loss: 6.7813\n",
      "Epoch [29/100], Step [2400/17448], Loss: 11.3750\n",
      "Epoch [29/100], Step [2500/17448], Loss: 7.5469\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [30/100], Step [100/17448], Loss: 4.9844\n",
      "Epoch [30/100], Step [200/17448], Loss: 6.9844\n",
      "Epoch [30/100], Step [300/17448], Loss: 9.3281\n",
      "Epoch [30/100], Step [400/17448], Loss: 8.0938\n",
      "Epoch [30/100], Step [500/17448], Loss: 8.1719\n",
      "Epoch [30/100], Step [600/17448], Loss: 9.7657\n",
      "Epoch [30/100], Step [700/17448], Loss: 5.6406\n",
      "Epoch [30/100], Step [800/17448], Loss: 8.8438\n",
      "Epoch [30/100], Step [900/17448], Loss: 9.8125\n",
      "Epoch [30/100], Step [1000/17448], Loss: 9.5000\n",
      "Epoch [30/100], Step [1100/17448], Loss: 7.7657\n",
      "Epoch [30/100], Step [1200/17448], Loss: 8.5938\n",
      "Epoch [30/100], Step [1300/17448], Loss: 8.6094\n",
      "Epoch [30/100], Step [1400/17448], Loss: 9.3126\n",
      "Epoch [30/100], Step [1500/17448], Loss: 7.8750\n",
      "Epoch [30/100], Step [1600/17448], Loss: 5.2188\n",
      "Epoch [30/100], Step [1700/17448], Loss: 5.9219\n",
      "Epoch [30/100], Step [1800/17448], Loss: 9.2500\n",
      "Epoch [30/100], Step [1900/17448], Loss: 6.9063\n",
      "Epoch [30/100], Step [2000/17448], Loss: 6.7344\n",
      "Epoch [30/100], Step [2100/17448], Loss: 9.8906\n",
      "Epoch [30/100], Step [2200/17448], Loss: 7.1407\n",
      "Epoch [30/100], Step [2300/17448], Loss: 9.3125\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [31/100], Step [100/17448], Loss: 3.8438\n",
      "Epoch [31/100], Step [200/17448], Loss: 5.8594\n",
      "Epoch [31/100], Step [300/17448], Loss: 7.9844\n",
      "Epoch [31/100], Step [400/17448], Loss: 9.8906\n",
      "Epoch [31/100], Step [500/17448], Loss: 7.9845\n",
      "Epoch [31/100], Step [600/17448], Loss: 5.7188\n",
      "Epoch [31/100], Step [700/17448], Loss: 10.9532\n",
      "Epoch [31/100], Step [800/17448], Loss: 7.8595\n",
      "Epoch [31/100], Step [900/17448], Loss: 6.1097\n",
      "Epoch [31/100], Step [1000/17448], Loss: 5.9220\n",
      "Epoch [31/100], Step [1100/17448], Loss: 6.7813\n",
      "Epoch [31/100], Step [1200/17448], Loss: 6.3126\n",
      "Epoch [31/100], Step [1300/17448], Loss: 6.5938\n",
      "Epoch [31/100], Step [1400/17448], Loss: 7.1875\n",
      "Epoch [31/100], Step [1500/17448], Loss: 12.2656\n",
      "Epoch [31/100], Step [1600/17448], Loss: 10.3438\n",
      "Epoch [31/100], Step [1700/17448], Loss: 6.9375\n",
      "Epoch [31/100], Step [1800/17448], Loss: 7.1250\n",
      "Epoch [31/100], Step [1900/17448], Loss: 7.7813\n",
      "Epoch [31/100], Step [2000/17448], Loss: 5.8906\n",
      "Epoch [31/100], Step [2100/17448], Loss: 10.6875\n",
      "Epoch [31/100], Step [2200/17448], Loss: 6.5782\n",
      "Epoch [31/100], Step [2300/17448], Loss: 6.4844\n",
      "Epoch [31/100], Step [2400/17448], Loss: 6.0157\n",
      "Epoch [31/100], Step [2500/17448], Loss: 6.6875\n",
      "Epoch [31/100], Step [2600/17448], Loss: 5.7032\n",
      "Epoch [31/100], Step [2700/17448], Loss: 10.0314\n",
      "Epoch [31/100], Step [2800/17448], Loss: 6.9688\n",
      "Epoch [31/100], Step [2900/17448], Loss: 5.8125\n",
      "Epoch [31/100], Step [3000/17448], Loss: 7.9844\n",
      "Epoch [31/100], Step [3100/17448], Loss: 3.8281\n",
      "Epoch [31/100], Step [3200/17448], Loss: 8.4376\n",
      "Epoch [31/100], Step [3300/17448], Loss: 6.7032\n",
      "Epoch [31/100], Step [3400/17448], Loss: 7.8594\n",
      "Epoch [31/100], Step [3500/17448], Loss: 4.7500\n",
      "Epoch [31/100], Step [3600/17448], Loss: 7.8439\n",
      "Epoch [31/100], Step [3700/17448], Loss: 9.8125\n",
      "Epoch [31/100], Step [3800/17448], Loss: 8.7969\n",
      "Epoch [31/100], Step [3900/17448], Loss: 6.6251\n",
      "Epoch [31/100], Step [4000/17448], Loss: 8.8437\n",
      "Epoch [31/100], Step [4100/17448], Loss: 14.8281\n",
      "Epoch [31/100], Step [4200/17448], Loss: 7.8594\n",
      "Epoch [31/100], Step [4300/17448], Loss: 5.2344\n",
      "Epoch [31/100], Step [4400/17448], Loss: 7.3281\n",
      "Epoch [31/100], Step [4500/17448], Loss: 11.4688\n",
      "Epoch [31/100], Step [4600/17448], Loss: 7.0156\n",
      "Epoch [31/100], Step [4700/17448], Loss: 7.9063\n",
      "Epoch [31/100], Step [4800/17448], Loss: 8.1250\n",
      "Epoch [31/100], Step [4900/17448], Loss: 6.7344\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [32/100], Step [100/17448], Loss: 9.7812\n",
      "Epoch [32/100], Step [200/17448], Loss: 10.1875\n",
      "Epoch [32/100], Step [300/17448], Loss: 10.7031\n",
      "Epoch [32/100], Step [400/17448], Loss: 6.3594\n",
      "Epoch [32/100], Step [500/17448], Loss: 7.4844\n",
      "Epoch [32/100], Step [600/17448], Loss: 6.6407\n",
      "Epoch [32/100], Step [700/17448], Loss: 8.0469\n",
      "Epoch [32/100], Step [800/17448], Loss: 7.6250\n",
      "Epoch [32/100], Step [900/17448], Loss: 11.4375\n",
      "Epoch [32/100], Step [1000/17448], Loss: 7.6563\n",
      "Epoch [32/100], Step [1100/17448], Loss: 6.0001\n",
      "Epoch [32/100], Step [1200/17448], Loss: 8.4688\n",
      "Epoch [32/100], Step [1300/17448], Loss: 9.5781\n",
      "Epoch [32/100], Step [1400/17448], Loss: 7.4375\n",
      "Epoch [32/100], Step [1500/17448], Loss: 7.8594\n",
      "Epoch [32/100], Step [1600/17448], Loss: 7.9063\n",
      "Epoch [32/100], Step [1700/17448], Loss: 7.8594\n",
      "Epoch [32/100], Step [1800/17448], Loss: 9.6094\n",
      "Epoch [32/100], Step [1900/17448], Loss: 5.2188\n",
      "Epoch [32/100], Step [2000/17448], Loss: 4.9378\n",
      "Epoch [32/100], Step [2100/17448], Loss: 7.8907\n",
      "Epoch [32/100], Step [2200/17448], Loss: 5.9063\n",
      "Epoch [32/100], Step [2300/17448], Loss: 5.2344\n",
      "Epoch [32/100], Step [2400/17448], Loss: 8.3594\n",
      "Epoch [32/100], Step [2500/17448], Loss: 9.0626\n",
      "Epoch [32/100], Step [2600/17448], Loss: 8.6094\n",
      "Epoch [32/100], Step [2700/17448], Loss: 10.8125\n",
      "Epoch [32/100], Step [2800/17448], Loss: 8.3438\n",
      "Epoch [32/100], Step [2900/17448], Loss: 8.7189\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [33/100], Step [100/17448], Loss: 6.0470\n",
      "Epoch [33/100], Step [200/17448], Loss: 7.7657\n",
      "Epoch [33/100], Step [300/17448], Loss: 7.2969\n",
      "Epoch [33/100], Step [400/17448], Loss: 10.4532\n",
      "Epoch [33/100], Step [500/17448], Loss: 5.3908\n",
      "Epoch [33/100], Step [600/17448], Loss: 8.5625\n",
      "Epoch [33/100], Step [700/17448], Loss: 7.5313\n",
      "Epoch [33/100], Step [800/17448], Loss: 7.8750\n",
      "Epoch [33/100], Step [900/17448], Loss: 9.3438\n",
      "Epoch [33/100], Step [1000/17448], Loss: 8.2813\n",
      "Epoch [33/100], Step [1100/17448], Loss: 8.5469\n",
      "Epoch [33/100], Step [1200/17448], Loss: 7.2344\n",
      "Epoch [33/100], Step [1300/17448], Loss: 8.9844\n",
      "Epoch [33/100], Step [1400/17448], Loss: 8.0469\n",
      "Epoch [33/100], Step [1500/17448], Loss: 6.7031\n",
      "Epoch [33/100], Step [1600/17448], Loss: 4.4063\n",
      "Epoch [33/100], Step [1700/17448], Loss: 9.1876\n",
      "Epoch [33/100], Step [1800/17448], Loss: 5.9220\n",
      "Epoch [33/100], Step [1900/17448], Loss: 5.5157\n",
      "Epoch [33/100], Step [2000/17448], Loss: 7.9532\n",
      "Epoch [33/100], Step [2100/17448], Loss: 9.0313\n",
      "Epoch [33/100], Step [2200/17448], Loss: 6.6407\n",
      "Epoch [33/100], Step [2300/17448], Loss: 9.4375\n",
      "Epoch [33/100], Step [2400/17448], Loss: 5.8594\n",
      "Epoch [33/100], Step [2500/17448], Loss: 7.3281\n",
      "Epoch [33/100], Step [2600/17448], Loss: 6.2345\n",
      "Epoch [33/100], Step [2700/17448], Loss: 10.5625\n",
      "Epoch [33/100], Step [2800/17448], Loss: 5.5781\n",
      "Epoch [33/100], Step [2900/17448], Loss: 6.8750\n",
      "Epoch [33/100], Step [3000/17448], Loss: 7.1719\n",
      "Epoch [33/100], Step [3100/17448], Loss: 11.3594\n",
      "Epoch [33/100], Step [3200/17448], Loss: 6.4531\n",
      "Epoch [33/100], Step [3300/17448], Loss: 6.5156\n",
      "Epoch [33/100], Step [3400/17448], Loss: 7.0157\n",
      "Epoch [33/100], Step [3500/17448], Loss: 6.5000\n",
      "Epoch [33/100], Step [3600/17448], Loss: 8.0626\n",
      "Epoch [33/100], Step [3700/17448], Loss: 5.6407\n",
      "Epoch [33/100], Step [3800/17448], Loss: 6.7970\n",
      "Epoch [33/100], Step [3900/17448], Loss: 9.4376\n",
      "Epoch [33/100], Step [4000/17448], Loss: 6.3438\n",
      "Epoch [33/100], Step [4100/17448], Loss: 10.3281\n",
      "Epoch [33/100], Step [4200/17448], Loss: 5.6407\n",
      "Epoch [33/100], Step [4300/17448], Loss: 8.7032\n",
      "Epoch [33/100], Step [4400/17448], Loss: 11.4844\n",
      "Epoch [33/100], Step [4500/17448], Loss: 10.0625\n",
      "Epoch [33/100], Step [4600/17448], Loss: 9.1250\n",
      "Epoch [33/100], Step [4700/17448], Loss: 10.3125\n",
      "Epoch [33/100], Step [4800/17448], Loss: 5.2344\n",
      "Epoch [33/100], Step [4900/17448], Loss: 7.7032\n",
      "Epoch [33/100], Step [5000/17448], Loss: 6.6720\n",
      "Epoch [33/100], Step [5100/17448], Loss: 5.7344\n",
      "Epoch [33/100], Step [5200/17448], Loss: 6.3907\n",
      "Epoch [33/100], Step [5300/17448], Loss: 7.8906\n",
      "Epoch [33/100], Step [5400/17448], Loss: 6.2188\n",
      "Epoch [33/100], Step [5500/17448], Loss: 7.9689\n",
      "Epoch [33/100], Step [5600/17448], Loss: 6.7500\n",
      "Epoch [33/100], Step [5700/17448], Loss: 4.3907\n",
      "Epoch [33/100], Step [5800/17448], Loss: 8.9219\n",
      "Epoch [33/100], Step [5900/17448], Loss: 11.4219\n",
      "Epoch [33/100], Step [6000/17448], Loss: 11.0626\n",
      "Epoch [33/100], Step [6100/17448], Loss: 8.1094\n",
      "Epoch [33/100], Step [6200/17448], Loss: 6.2031\n",
      "Epoch [33/100], Step [6300/17448], Loss: 8.7969\n",
      "Epoch [33/100], Step [6400/17448], Loss: 4.2345\n",
      "Epoch [33/100], Step [6500/17448], Loss: 6.6407\n",
      "Epoch [33/100], Step [6600/17448], Loss: 11.3438\n",
      "Epoch [33/100], Step [6700/17448], Loss: 7.4689\n",
      "Epoch [33/100], Step [6800/17448], Loss: 9.7813\n",
      "Epoch [33/100], Step [6900/17448], Loss: 4.8907\n",
      "Epoch [33/100], Step [7000/17448], Loss: 8.2501\n",
      "Epoch [33/100], Step [7100/17448], Loss: 6.6875\n",
      "Epoch [33/100], Step [7200/17448], Loss: 8.4688\n",
      "Epoch [33/100], Step [7300/17448], Loss: 8.5782\n",
      "Epoch [33/100], Step [7400/17448], Loss: 9.5938\n",
      "Epoch [33/100], Step [7500/17448], Loss: 5.8438\n",
      "Epoch [33/100], Step [7600/17448], Loss: 6.4376\n",
      "Epoch [33/100], Step [7700/17448], Loss: 6.6563\n",
      "Epoch [33/100], Step [7800/17448], Loss: 7.6719\n",
      "Epoch [33/100], Step [7900/17448], Loss: 5.2188\n",
      "Epoch [33/100], Step [8000/17448], Loss: 7.9688\n",
      "Epoch [33/100], Step [8100/17448], Loss: 9.2501\n",
      "Epoch [33/100], Step [8200/17448], Loss: 11.8594\n",
      "Epoch [33/100], Step [8300/17448], Loss: 8.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [34/100], Step [100/17448], Loss: 6.9064\n",
      "Epoch [34/100], Step [200/17448], Loss: 7.1250\n",
      "Epoch [34/100], Step [300/17448], Loss: 9.9375\n",
      "Epoch [34/100], Step [400/17448], Loss: 8.1406\n",
      "Epoch [34/100], Step [500/17448], Loss: 5.7188\n",
      "Epoch [34/100], Step [600/17448], Loss: 11.0156\n",
      "Epoch [34/100], Step [700/17448], Loss: 7.6563\n",
      "Epoch [34/100], Step [800/17448], Loss: 4.9376\n",
      "Epoch [34/100], Step [900/17448], Loss: 9.5313\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [35/100], Step [100/17448], Loss: 5.7813\n",
      "Epoch [35/100], Step [200/17448], Loss: 7.0625\n",
      "Epoch [35/100], Step [300/17448], Loss: 8.5938\n",
      "Epoch [35/100], Step [400/17448], Loss: 6.0628\n",
      "Epoch [35/100], Step [500/17448], Loss: 5.1719\n",
      "Epoch [35/100], Step [600/17448], Loss: 7.4375\n",
      "Epoch [35/100], Step [700/17448], Loss: 9.9532\n",
      "Epoch [35/100], Step [800/17448], Loss: 7.3594\n",
      "Epoch [35/100], Step [900/17448], Loss: 11.1564\n",
      "Epoch [35/100], Step [1000/17448], Loss: 7.7501\n",
      "Epoch [35/100], Step [1100/17448], Loss: 9.2345\n",
      "Epoch [35/100], Step [1200/17448], Loss: 7.1406\n",
      "Epoch [35/100], Step [1300/17448], Loss: 6.3128\n",
      "Epoch [35/100], Step [1400/17448], Loss: 4.0626\n",
      "Epoch [35/100], Step [1500/17448], Loss: 9.3125\n",
      "Epoch [35/100], Step [1600/17448], Loss: 4.9219\n",
      "Epoch [35/100], Step [1700/17448], Loss: 5.7188\n",
      "Epoch [35/100], Step [1800/17448], Loss: 7.1876\n",
      "Epoch [35/100], Step [1900/17448], Loss: 6.7657\n",
      "Epoch [35/100], Step [2000/17448], Loss: 7.2657\n",
      "Epoch [35/100], Step [2100/17448], Loss: 8.0470\n",
      "Epoch [35/100], Step [2200/17448], Loss: 8.4689\n",
      "Epoch [35/100], Step [2300/17448], Loss: 7.7345\n",
      "Epoch [35/100], Step [2400/17448], Loss: 10.7814\n",
      "Epoch [35/100], Step [2500/17448], Loss: 7.1563\n",
      "Epoch [35/100], Step [2600/17448], Loss: 6.5001\n",
      "Epoch [35/100], Step [2700/17448], Loss: 7.0000\n",
      "Epoch [35/100], Step [2800/17448], Loss: 6.2031\n",
      "Epoch [35/100], Step [2900/17448], Loss: 7.4375\n",
      "Epoch [35/100], Step [3000/17448], Loss: 3.7970\n",
      "Epoch [35/100], Step [3100/17448], Loss: 9.0313\n",
      "Epoch [35/100], Step [3200/17448], Loss: 5.7969\n",
      "Epoch [35/100], Step [3300/17448], Loss: 8.6877\n",
      "Epoch [35/100], Step [3400/17448], Loss: 7.1563\n",
      "Epoch [35/100], Step [3500/17448], Loss: 9.6251\n",
      "Epoch [35/100], Step [3600/17448], Loss: 7.9062\n",
      "Epoch [35/100], Step [3700/17448], Loss: 8.0001\n",
      "Epoch [35/100], Step [3800/17448], Loss: 6.9375\n",
      "Epoch [35/100], Step [3900/17448], Loss: 9.6407\n",
      "Epoch [35/100], Step [4000/17448], Loss: 6.8750\n",
      "Epoch [35/100], Step [4100/17448], Loss: 4.8750\n",
      "Epoch [35/100], Step [4200/17448], Loss: 7.9375\n",
      "Epoch [35/100], Step [4300/17448], Loss: 6.9689\n",
      "Epoch [35/100], Step [4400/17448], Loss: 6.0001\n",
      "Epoch [35/100], Step [4500/17448], Loss: 8.9378\n",
      "Epoch [35/100], Step [4600/17448], Loss: 8.4219\n",
      "Epoch [35/100], Step [4700/17448], Loss: 6.3594\n",
      "Epoch [35/100], Step [4800/17448], Loss: 5.2657\n",
      "Epoch [35/100], Step [4900/17448], Loss: 6.8125\n",
      "Epoch [35/100], Step [5000/17448], Loss: 7.5156\n",
      "Epoch [35/100], Step [5100/17448], Loss: 6.2813\n",
      "Epoch [35/100], Step [5200/17448], Loss: 7.1094\n",
      "Epoch [35/100], Step [5300/17448], Loss: 5.5938\n",
      "Epoch [35/100], Step [5400/17448], Loss: 5.9220\n",
      "Epoch [35/100], Step [5500/17448], Loss: 10.7969\n",
      "Epoch [35/100], Step [5600/17448], Loss: 8.0938\n",
      "Epoch [35/100], Step [5700/17448], Loss: 9.8907\n",
      "Epoch [35/100], Step [5800/17448], Loss: 6.0782\n",
      "Epoch [35/100], Step [5900/17448], Loss: 5.7501\n",
      "Epoch [35/100], Step [6000/17448], Loss: 8.5157\n",
      "Epoch [35/100], Step [6100/17448], Loss: 12.4219\n",
      "Epoch [35/100], Step [6200/17448], Loss: 6.9220\n",
      "Epoch [35/100], Step [6300/17448], Loss: 7.0314\n",
      "Epoch [35/100], Step [6400/17448], Loss: 5.2657\n",
      "Epoch [35/100], Step [6500/17448], Loss: 9.3126\n",
      "Epoch [35/100], Step [6600/17448], Loss: 9.4064\n",
      "Epoch [35/100], Step [6700/17448], Loss: 9.2189\n",
      "Epoch [35/100], Step [6800/17448], Loss: 5.5000\n",
      "Epoch [35/100], Step [6900/17448], Loss: 6.7032\n",
      "Epoch [35/100], Step [7000/17448], Loss: 5.6564\n",
      "Epoch [35/100], Step [7100/17448], Loss: 9.9063\n",
      "Epoch [35/100], Step [7200/17448], Loss: 5.5626\n",
      "Epoch [35/100], Step [7300/17448], Loss: 8.6250\n",
      "Epoch [35/100], Step [7400/17448], Loss: 5.6875\n",
      "Epoch [35/100], Step [7500/17448], Loss: 8.3594\n",
      "Epoch [35/100], Step [7600/17448], Loss: 6.4531\n",
      "Epoch [35/100], Step [7700/17448], Loss: 12.6875\n",
      "Epoch [35/100], Step [7800/17448], Loss: 7.4844\n",
      "Epoch [35/100], Step [7900/17448], Loss: 7.3439\n",
      "Epoch [35/100], Step [8000/17448], Loss: 6.0469\n",
      "Epoch [35/100], Step [8100/17448], Loss: 5.4532\n",
      "Epoch [35/100], Step [8200/17448], Loss: 10.9844\n",
      "Epoch [35/100], Step [8300/17448], Loss: 7.2344\n",
      "Epoch [35/100], Step [8400/17448], Loss: 6.5782\n",
      "Epoch [35/100], Step [8500/17448], Loss: 6.1876\n",
      "Epoch [35/100], Step [8600/17448], Loss: 10.0156\n",
      "Epoch [35/100], Step [8700/17448], Loss: 4.4844\n",
      "Epoch [35/100], Step [8800/17448], Loss: 10.6720\n",
      "Epoch [35/100], Step [8900/17448], Loss: 3.9221\n",
      "Epoch [35/100], Step [9000/17448], Loss: 9.7188\n",
      "Epoch [35/100], Step [9100/17448], Loss: 8.8438\n",
      "Epoch [35/100], Step [9200/17448], Loss: 8.1096\n",
      "Epoch [35/100], Step [9300/17448], Loss: 6.0938\n",
      "Epoch [35/100], Step [9400/17448], Loss: 9.6094\n",
      "Epoch [35/100], Step [9500/17448], Loss: 7.9375\n",
      "Epoch [35/100], Step [9600/17448], Loss: 7.0469\n",
      "Epoch [35/100], Step [9700/17448], Loss: 9.3125\n",
      "Epoch [35/100], Step [9800/17448], Loss: 6.5469\n",
      "Epoch [35/100], Step [9900/17448], Loss: 8.7500\n",
      "Epoch [35/100], Step [10000/17448], Loss: 9.0938\n",
      "Epoch [35/100], Step [10100/17448], Loss: 6.1719\n",
      "Epoch [35/100], Step [10200/17448], Loss: 5.2188\n",
      "Epoch [35/100], Step [10300/17448], Loss: 6.2500\n",
      "Epoch [35/100], Step [10400/17448], Loss: 7.8907\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [36/100], Step [100/17448], Loss: 8.2344\n",
      "Epoch [36/100], Step [200/17448], Loss: 7.1876\n",
      "Epoch [36/100], Step [300/17448], Loss: 7.9688\n",
      "Epoch [36/100], Step [400/17448], Loss: 7.8751\n",
      "Epoch [36/100], Step [500/17448], Loss: 6.7971\n",
      "Epoch [36/100], Step [600/17448], Loss: 5.9532\n",
      "Epoch [36/100], Step [700/17448], Loss: 6.8751\n",
      "Epoch [36/100], Step [800/17448], Loss: 6.0157\n",
      "Epoch [36/100], Step [900/17448], Loss: 8.8438\n",
      "Epoch [36/100], Step [1000/17448], Loss: 6.7188\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [37/100], Step [100/17448], Loss: 8.8125\n",
      "Epoch [37/100], Step [200/17448], Loss: 11.9844\n",
      "Epoch [37/100], Step [300/17448], Loss: 6.7657\n",
      "Epoch [37/100], Step [400/17448], Loss: 6.8750\n",
      "Epoch [37/100], Step [500/17448], Loss: 9.3594\n",
      "Epoch [37/100], Step [600/17448], Loss: 8.8907\n",
      "Epoch [37/100], Step [700/17448], Loss: 8.2814\n",
      "Epoch [37/100], Step [800/17448], Loss: 8.5313\n",
      "Epoch [37/100], Step [900/17448], Loss: 7.4063\n",
      "Epoch [37/100], Step [1000/17448], Loss: 8.3751\n",
      "Epoch [37/100], Step [1100/17448], Loss: 8.6250\n",
      "Epoch [37/100], Step [1200/17448], Loss: 6.2813\n",
      "Epoch [37/100], Step [1300/17448], Loss: 7.1250\n",
      "Epoch [37/100], Step [1400/17448], Loss: 7.2345\n",
      "Epoch [37/100], Step [1500/17448], Loss: 7.5626\n",
      "Epoch [37/100], Step [1600/17448], Loss: 8.4844\n",
      "Epoch [37/100], Step [1700/17448], Loss: 9.4219\n",
      "Epoch [37/100], Step [1800/17448], Loss: 8.7657\n",
      "Epoch [37/100], Step [1900/17448], Loss: 10.5625\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [38/100], Step [100/17448], Loss: 6.2188\n",
      "Epoch [38/100], Step [200/17448], Loss: 6.9376\n",
      "Epoch [38/100], Step [300/17448], Loss: 3.6563\n",
      "Epoch [38/100], Step [400/17448], Loss: 5.4063\n",
      "Epoch [38/100], Step [500/17448], Loss: 5.7815\n",
      "Epoch [38/100], Step [600/17448], Loss: 3.9375\n",
      "Epoch [38/100], Step [700/17448], Loss: 11.3907\n",
      "Epoch [38/100], Step [800/17448], Loss: 7.7657\n",
      "Epoch [38/100], Step [900/17448], Loss: 9.2813\n",
      "Epoch [38/100], Step [1000/17448], Loss: 8.6407\n",
      "Epoch [38/100], Step [1100/17448], Loss: 4.7344\n",
      "Epoch [38/100], Step [1200/17448], Loss: 12.1563\n",
      "Epoch [38/100], Step [1300/17448], Loss: 9.1563\n",
      "Epoch [38/100], Step [1400/17448], Loss: 7.5781\n",
      "Epoch [38/100], Step [1500/17448], Loss: 7.8438\n",
      "Epoch [38/100], Step [1600/17448], Loss: 6.0625\n",
      "Epoch [38/100], Step [1700/17448], Loss: 8.5938\n",
      "Epoch [38/100], Step [1800/17448], Loss: 8.3438\n",
      "Epoch [38/100], Step [1900/17448], Loss: 7.5938\n",
      "Epoch [38/100], Step [2000/17448], Loss: 7.2969\n",
      "Epoch [38/100], Step [2100/17448], Loss: 7.1407\n",
      "Epoch [38/100], Step [2200/17448], Loss: 7.9844\n",
      "Epoch [38/100], Step [2300/17448], Loss: 6.9688\n",
      "Epoch [38/100], Step [2400/17448], Loss: 5.8282\n",
      "Epoch [38/100], Step [2500/17448], Loss: 9.2969\n",
      "Epoch [38/100], Step [2600/17448], Loss: 6.8596\n",
      "Epoch [38/100], Step [2700/17448], Loss: 7.5938\n",
      "Epoch [38/100], Step [2800/17448], Loss: 7.0625\n",
      "Epoch [38/100], Step [2900/17448], Loss: 6.1406\n",
      "Epoch [38/100], Step [3000/17448], Loss: 7.7188\n",
      "Epoch [38/100], Step [3100/17448], Loss: 8.1875\n",
      "Epoch [38/100], Step [3200/17448], Loss: 6.1719\n",
      "Epoch [38/100], Step [3300/17448], Loss: 11.9375\n",
      "Epoch [38/100], Step [3400/17448], Loss: 6.7813\n",
      "Epoch [38/100], Step [3500/17448], Loss: 6.9063\n",
      "Epoch [38/100], Step [3600/17448], Loss: 11.6876\n",
      "Epoch [38/100], Step [3700/17448], Loss: 7.1407\n",
      "Epoch [38/100], Step [3800/17448], Loss: 7.4219\n",
      "Epoch [38/100], Step [3900/17448], Loss: 3.8908\n",
      "Epoch [38/100], Step [4000/17448], Loss: 8.0002\n",
      "Epoch [38/100], Step [4100/17448], Loss: 7.9219\n",
      "Epoch [38/100], Step [4200/17448], Loss: 11.0313\n",
      "Epoch [38/100], Step [4300/17448], Loss: 8.3281\n",
      "Epoch [38/100], Step [4400/17448], Loss: 6.0000\n",
      "Epoch [38/100], Step [4500/17448], Loss: 6.0782\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [39/100], Step [100/17448], Loss: 9.3281\n",
      "Epoch [39/100], Step [200/17448], Loss: 4.2657\n",
      "Epoch [39/100], Step [300/17448], Loss: 7.8438\n",
      "Epoch [39/100], Step [400/17448], Loss: 7.2813\n",
      "Epoch [39/100], Step [500/17448], Loss: 4.7188\n",
      "Epoch [39/100], Step [600/17448], Loss: 8.1875\n",
      "Epoch [39/100], Step [700/17448], Loss: 7.4531\n",
      "Epoch [39/100], Step [800/17448], Loss: 5.8125\n",
      "Epoch [39/100], Step [900/17448], Loss: 7.9219\n",
      "Epoch [39/100], Step [1000/17448], Loss: 7.1719\n",
      "Epoch [39/100], Step [1100/17448], Loss: 6.5469\n",
      "Epoch [39/100], Step [1200/17448], Loss: 8.0156\n",
      "Epoch [39/100], Step [1300/17448], Loss: 7.3281\n",
      "Epoch [39/100], Step [1400/17448], Loss: 11.1563\n",
      "Epoch [39/100], Step [1500/17448], Loss: 6.2189\n",
      "Epoch [39/100], Step [1600/17448], Loss: 5.4688\n",
      "Epoch [39/100], Step [1700/17448], Loss: 5.7969\n",
      "Epoch [39/100], Step [1800/17448], Loss: 6.5000\n",
      "Epoch [39/100], Step [1900/17448], Loss: 8.0470\n",
      "Epoch [39/100], Step [2000/17448], Loss: 6.6094\n",
      "Epoch [39/100], Step [2100/17448], Loss: 6.1719\n",
      "Epoch [39/100], Step [2200/17448], Loss: 6.3594\n",
      "Epoch [39/100], Step [2300/17448], Loss: 8.1094\n",
      "Epoch [39/100], Step [2400/17448], Loss: 10.5469\n",
      "Epoch [39/100], Step [2500/17448], Loss: 8.8907\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [40/100], Step [100/17448], Loss: 7.5158\n",
      "Epoch [40/100], Step [200/17448], Loss: 8.3594\n",
      "Epoch [40/100], Step [300/17448], Loss: 10.0470\n",
      "Epoch [40/100], Step [400/17448], Loss: 8.5782\n",
      "Epoch [40/100], Step [500/17448], Loss: 6.8438\n",
      "Epoch [40/100], Step [600/17448], Loss: 7.5781\n",
      "Epoch [40/100], Step [700/17448], Loss: 7.5781\n",
      "Epoch [40/100], Step [800/17448], Loss: 8.9688\n",
      "Epoch [40/100], Step [900/17448], Loss: 7.3438\n",
      "Epoch [40/100], Step [1000/17448], Loss: 5.7188\n",
      "Epoch [40/100], Step [1100/17448], Loss: 7.0938\n",
      "Epoch [40/100], Step [1200/17448], Loss: 8.0625\n",
      "Epoch [40/100], Step [1300/17448], Loss: 5.6563\n",
      "Epoch [40/100], Step [1400/17448], Loss: 5.9688\n",
      "Epoch [40/100], Step [1500/17448], Loss: 6.4375\n",
      "Epoch [40/100], Step [1600/17448], Loss: 5.5000\n",
      "Epoch [40/100], Step [1700/17448], Loss: 6.4220\n",
      "Epoch [40/100], Step [1800/17448], Loss: 9.1095\n",
      "Epoch [40/100], Step [1900/17448], Loss: 8.6094\n",
      "Epoch [40/100], Step [2000/17448], Loss: 7.0782\n",
      "Epoch [40/100], Step [2100/17448], Loss: 10.4219\n",
      "Epoch [40/100], Step [2200/17448], Loss: 7.5625\n",
      "Epoch [40/100], Step [2300/17448], Loss: 10.6875\n",
      "Epoch [40/100], Step [2400/17448], Loss: 8.8751\n",
      "Epoch [40/100], Step [2500/17448], Loss: 10.4375\n",
      "Epoch [40/100], Step [2600/17448], Loss: 7.0157\n",
      "Epoch [40/100], Step [2700/17448], Loss: 7.6095\n",
      "Epoch [40/100], Step [2800/17448], Loss: 7.8907\n",
      "Epoch [40/100], Step [2900/17448], Loss: 7.4376\n",
      "Epoch [40/100], Step [3000/17448], Loss: 5.0470\n",
      "Epoch [40/100], Step [3100/17448], Loss: 6.8907\n",
      "Epoch [40/100], Step [3200/17448], Loss: 7.4219\n",
      "Epoch [40/100], Step [3300/17448], Loss: 5.3750\n",
      "Epoch [40/100], Step [3400/17448], Loss: 9.1094\n",
      "Epoch [40/100], Step [3500/17448], Loss: 5.3125\n",
      "Epoch [40/100], Step [3600/17448], Loss: 9.4689\n",
      "Epoch [40/100], Step [3700/17448], Loss: 7.7656\n",
      "Epoch [40/100], Step [3800/17448], Loss: 5.0470\n",
      "Epoch [40/100], Step [3900/17448], Loss: 7.9375\n",
      "Epoch [40/100], Step [4000/17448], Loss: 5.5938\n",
      "Epoch [40/100], Step [4100/17448], Loss: 9.0938\n",
      "Epoch [40/100], Step [4200/17448], Loss: 4.5782\n",
      "Epoch [40/100], Step [4300/17448], Loss: 8.2345\n",
      "Epoch [40/100], Step [4400/17448], Loss: 6.8752\n",
      "Epoch [40/100], Step [4500/17448], Loss: 13.0625\n",
      "Epoch [40/100], Step [4600/17448], Loss: 5.9533\n",
      "Epoch [40/100], Step [4700/17448], Loss: 7.9844\n",
      "Epoch [40/100], Step [4800/17448], Loss: 5.0469\n",
      "Epoch [40/100], Step [4900/17448], Loss: 6.2188\n",
      "Epoch [40/100], Step [5000/17448], Loss: 6.4532\n",
      "Epoch [40/100], Step [5100/17448], Loss: 6.9063\n",
      "Epoch [40/100], Step [5200/17448], Loss: 7.1876\n",
      "Epoch [40/100], Step [5300/17448], Loss: 7.9844\n",
      "Epoch [40/100], Step [5400/17448], Loss: 7.1563\n",
      "Epoch [40/100], Step [5500/17448], Loss: 5.3907\n",
      "Epoch [40/100], Step [5600/17448], Loss: 8.3281\n",
      "Epoch [40/100], Step [5700/17448], Loss: 10.0938\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [41/100], Step [100/17448], Loss: 5.9531\n",
      "Epoch [41/100], Step [200/17448], Loss: 9.1875\n",
      "Epoch [41/100], Step [300/17448], Loss: 5.8126\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [42/100], Step [100/17448], Loss: 5.6407\n",
      "Epoch [42/100], Step [200/17448], Loss: 7.5313\n",
      "Epoch [42/100], Step [300/17448], Loss: 10.2031\n",
      "Epoch [42/100], Step [400/17448], Loss: 7.2033\n",
      "Epoch [42/100], Step [500/17448], Loss: 8.3750\n",
      "Epoch [42/100], Step [600/17448], Loss: 7.3751\n",
      "Epoch [42/100], Step [700/17448], Loss: 10.9531\n",
      "Epoch [42/100], Step [800/17448], Loss: 9.8751\n",
      "Epoch [42/100], Step [900/17448], Loss: 6.9688\n",
      "Epoch [42/100], Step [1000/17448], Loss: 6.3594\n",
      "Epoch [42/100], Step [1100/17448], Loss: 6.8281\n",
      "Epoch [42/100], Step [1200/17448], Loss: 5.3127\n",
      "Epoch [42/100], Step [1300/17448], Loss: 5.2032\n",
      "Epoch [42/100], Step [1400/17448], Loss: 6.0157\n",
      "Epoch [42/100], Step [1500/17448], Loss: 7.9532\n",
      "Epoch [42/100], Step [1600/17448], Loss: 8.7501\n",
      "Epoch [42/100], Step [1700/17448], Loss: 6.7813\n",
      "Epoch [42/100], Step [1800/17448], Loss: 6.4376\n",
      "Epoch [42/100], Step [1900/17448], Loss: 8.8906\n",
      "Epoch [42/100], Step [2000/17448], Loss: 8.4689\n",
      "Epoch [42/100], Step [2100/17448], Loss: 9.0313\n",
      "Epoch [42/100], Step [2200/17448], Loss: 9.9844\n",
      "Epoch [42/100], Step [2300/17448], Loss: 8.5157\n",
      "Epoch [42/100], Step [2400/17448], Loss: 6.0781\n",
      "Epoch [42/100], Step [2500/17448], Loss: 6.9688\n",
      "Epoch [42/100], Step [2600/17448], Loss: 13.5000\n",
      "Epoch [42/100], Step [2700/17448], Loss: 7.8439\n",
      "Epoch [42/100], Step [2800/17448], Loss: 9.7813\n",
      "Epoch [42/100], Step [2900/17448], Loss: 7.6880\n",
      "Epoch [42/100], Step [3000/17448], Loss: 6.1563\n",
      "Epoch [42/100], Step [3100/17448], Loss: 5.0782\n",
      "Epoch [42/100], Step [3200/17448], Loss: 5.7344\n",
      "Epoch [42/100], Step [3300/17448], Loss: 6.6095\n",
      "Epoch [42/100], Step [3400/17448], Loss: 7.0469\n",
      "Epoch [42/100], Step [3500/17448], Loss: 8.4375\n",
      "Epoch [42/100], Step [3600/17448], Loss: 5.5782\n",
      "Epoch [42/100], Step [3700/17448], Loss: 10.1250\n",
      "Epoch [42/100], Step [3800/17448], Loss: 8.2813\n",
      "Epoch [42/100], Step [3900/17448], Loss: 6.4844\n",
      "Epoch [42/100], Step [4000/17448], Loss: 9.5938\n",
      "Epoch [42/100], Step [4100/17448], Loss: 9.7969\n",
      "Epoch [42/100], Step [4200/17448], Loss: 5.5938\n",
      "Epoch [42/100], Step [4300/17448], Loss: 7.1721\n",
      "Epoch [42/100], Step [4400/17448], Loss: 7.5157\n",
      "Epoch [42/100], Step [4500/17448], Loss: 7.4688\n",
      "Epoch [42/100], Step [4600/17448], Loss: 8.2344\n",
      "Epoch [42/100], Step [4700/17448], Loss: 8.7656\n",
      "Epoch [42/100], Step [4800/17448], Loss: 7.0938\n",
      "Epoch [42/100], Step [4900/17448], Loss: 9.0625\n",
      "Epoch [42/100], Step [5000/17448], Loss: 8.8281\n",
      "Epoch [42/100], Step [5100/17448], Loss: 6.0000\n",
      "Epoch [42/100], Step [5200/17448], Loss: 8.7500\n",
      "Epoch [42/100], Step [5300/17448], Loss: 5.3438\n",
      "Epoch [42/100], Step [5400/17448], Loss: 8.0469\n",
      "Epoch [42/100], Step [5500/17448], Loss: 9.3281\n",
      "Epoch [42/100], Step [5600/17448], Loss: 7.1719\n",
      "Epoch [42/100], Step [5700/17448], Loss: 9.5469\n",
      "Epoch [42/100], Step [5800/17448], Loss: 11.0156\n",
      "Epoch [42/100], Step [5900/17448], Loss: 5.6563\n",
      "Epoch [42/100], Step [6000/17448], Loss: 5.1250\n",
      "Epoch [42/100], Step [6100/17448], Loss: 10.6719\n",
      "Epoch [42/100], Step [6200/17448], Loss: 8.2813\n",
      "Epoch [42/100], Step [6300/17448], Loss: 7.1094\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [43/100], Step [100/17448], Loss: 8.5938\n",
      "Epoch [43/100], Step [200/17448], Loss: 9.1250\n",
      "Epoch [43/100], Step [300/17448], Loss: 5.9688\n",
      "Epoch [43/100], Step [400/17448], Loss: 6.3751\n",
      "Epoch [43/100], Step [500/17448], Loss: 7.7344\n",
      "Epoch [43/100], Step [600/17448], Loss: 8.0000\n",
      "Epoch [43/100], Step [700/17448], Loss: 10.1250\n",
      "Epoch [43/100], Step [800/17448], Loss: 7.8125\n",
      "Epoch [43/100], Step [900/17448], Loss: 6.3751\n",
      "Epoch [43/100], Step [1000/17448], Loss: 6.2813\n",
      "Epoch [43/100], Step [1100/17448], Loss: 4.0469\n",
      "Epoch [43/100], Step [1200/17448], Loss: 12.1875\n",
      "Epoch [43/100], Step [1300/17448], Loss: 7.0782\n",
      "Epoch [43/100], Step [1400/17448], Loss: 5.9532\n",
      "Epoch [43/100], Step [1500/17448], Loss: 7.2188\n",
      "Epoch [43/100], Step [1600/17448], Loss: 6.6719\n",
      "Epoch [43/100], Step [1700/17448], Loss: 7.4220\n",
      "Epoch [43/100], Step [1800/17448], Loss: 7.9063\n",
      "Epoch [43/100], Step [1900/17448], Loss: 8.0156\n",
      "Epoch [43/100], Step [2000/17448], Loss: 7.1250\n",
      "Epoch [43/100], Step [2100/17448], Loss: 8.8906\n",
      "Epoch [43/100], Step [2200/17448], Loss: 6.7969\n",
      "Epoch [43/100], Step [2300/17448], Loss: 9.4844\n",
      "Epoch [43/100], Step [2400/17448], Loss: 8.3750\n",
      "Epoch [43/100], Step [2500/17448], Loss: 7.2188\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [44/100], Step [100/17448], Loss: 7.3282\n",
      "Epoch [44/100], Step [200/17448], Loss: 9.4844\n",
      "Epoch [44/100], Step [300/17448], Loss: 6.4063\n",
      "Epoch [44/100], Step [400/17448], Loss: 7.1877\n",
      "Epoch [44/100], Step [500/17448], Loss: 8.7657\n",
      "Epoch [44/100], Step [600/17448], Loss: 4.7031\n",
      "Epoch [44/100], Step [700/17448], Loss: 10.2032\n",
      "Epoch [44/100], Step [800/17448], Loss: 11.5469\n",
      "Epoch [44/100], Step [900/17448], Loss: 5.7188\n",
      "Epoch [44/100], Step [1000/17448], Loss: 4.1094\n",
      "Epoch [44/100], Step [1100/17448], Loss: 6.0470\n",
      "Epoch [44/100], Step [1200/17448], Loss: 6.8750\n",
      "Epoch [44/100], Step [1300/17448], Loss: 7.4532\n",
      "Epoch [44/100], Step [1400/17448], Loss: 6.8908\n",
      "Epoch [44/100], Step [1500/17448], Loss: 7.9375\n",
      "Epoch [44/100], Step [1600/17448], Loss: 5.4532\n",
      "Epoch [44/100], Step [1700/17448], Loss: 6.2031\n",
      "Epoch [44/100], Step [1800/17448], Loss: 9.9532\n",
      "Epoch [44/100], Step [1900/17448], Loss: 8.0000\n",
      "Epoch [44/100], Step [2000/17448], Loss: 4.2501\n",
      "Epoch [44/100], Step [2100/17448], Loss: 5.9063\n",
      "Epoch [44/100], Step [2200/17448], Loss: 5.9219\n",
      "Epoch [44/100], Step [2300/17448], Loss: 8.8753\n",
      "Epoch [44/100], Step [2400/17448], Loss: 7.3126\n",
      "Epoch [44/100], Step [2500/17448], Loss: 4.8906\n",
      "Epoch [44/100], Step [2600/17448], Loss: 7.2344\n",
      "Epoch [44/100], Step [2700/17448], Loss: 7.2032\n",
      "Epoch [44/100], Step [2800/17448], Loss: 8.6719\n",
      "Epoch [44/100], Step [2900/17448], Loss: 10.8750\n",
      "Epoch [44/100], Step [3000/17448], Loss: 5.1251\n",
      "Epoch [44/100], Step [3100/17448], Loss: 9.5157\n",
      "Epoch [44/100], Step [3200/17448], Loss: 6.0313\n",
      "Epoch [44/100], Step [3300/17448], Loss: 9.5938\n",
      "Epoch [44/100], Step [3400/17448], Loss: 8.9376\n",
      "Epoch [44/100], Step [3500/17448], Loss: 8.7813\n",
      "Epoch [44/100], Step [3600/17448], Loss: 6.5313\n",
      "Epoch [44/100], Step [3700/17448], Loss: 6.4063\n",
      "Epoch [44/100], Step [3800/17448], Loss: 6.6719\n",
      "Epoch [44/100], Step [3900/17448], Loss: 10.5000\n",
      "Epoch [44/100], Step [4000/17448], Loss: 7.6250\n",
      "Epoch [44/100], Step [4100/17448], Loss: 6.2970\n",
      "Epoch [44/100], Step [4200/17448], Loss: 5.0157\n",
      "Epoch [44/100], Step [4300/17448], Loss: 4.1407\n",
      "Epoch [44/100], Step [4400/17448], Loss: 9.6719\n",
      "Epoch [44/100], Step [4500/17448], Loss: 5.5001\n",
      "Epoch [44/100], Step [4600/17448], Loss: 6.1875\n",
      "Epoch [44/100], Step [4700/17448], Loss: 8.4375\n",
      "Epoch [44/100], Step [4800/17448], Loss: 8.1564\n",
      "Epoch [44/100], Step [4900/17448], Loss: 6.5313\n",
      "Epoch [44/100], Step [5000/17448], Loss: 4.4377\n",
      "Epoch [44/100], Step [5100/17448], Loss: 6.6876\n",
      "Epoch [44/100], Step [5200/17448], Loss: 8.1407\n",
      "Epoch [44/100], Step [5300/17448], Loss: 11.0625\n",
      "Epoch [44/100], Step [5400/17448], Loss: 7.1719\n",
      "Epoch [44/100], Step [5500/17448], Loss: 9.8594\n",
      "Epoch [44/100], Step [5600/17448], Loss: 7.9063\n",
      "Epoch [44/100], Step [5700/17448], Loss: 9.5782\n",
      "Epoch [44/100], Step [5800/17448], Loss: 7.8750\n",
      "Epoch [44/100], Step [5900/17448], Loss: 6.3594\n",
      "Epoch [44/100], Step [6000/17448], Loss: 7.8125\n",
      "Epoch [44/100], Step [6100/17448], Loss: 5.1563\n",
      "Epoch [44/100], Step [6200/17448], Loss: 6.4532\n",
      "Epoch [44/100], Step [6300/17448], Loss: 6.2500\n",
      "Epoch [44/100], Step [6400/17448], Loss: 8.5313\n",
      "Epoch [44/100], Step [6500/17448], Loss: 11.8750\n",
      "Epoch [44/100], Step [6600/17448], Loss: 6.6563\n",
      "Epoch [44/100], Step [6700/17448], Loss: 7.3907\n",
      "Epoch [44/100], Step [6800/17448], Loss: 6.8281\n",
      "Epoch [44/100], Step [6900/17448], Loss: 6.3594\n",
      "Epoch [44/100], Step [7000/17448], Loss: 8.3282\n",
      "Epoch [44/100], Step [7100/17448], Loss: 7.6251\n",
      "Epoch [44/100], Step [7200/17448], Loss: 9.6876\n",
      "Epoch [44/100], Step [7300/17448], Loss: 3.6251\n",
      "Epoch [44/100], Step [7400/17448], Loss: 6.6251\n",
      "Epoch [44/100], Step [7500/17448], Loss: 7.0001\n",
      "Epoch [44/100], Step [7600/17448], Loss: 7.2500\n",
      "Epoch [44/100], Step [7700/17448], Loss: 7.8751\n",
      "Epoch [44/100], Step [7800/17448], Loss: 6.8282\n",
      "Epoch [44/100], Step [7900/17448], Loss: 7.7033\n",
      "Epoch [44/100], Step [8000/17448], Loss: 10.3281\n",
      "Epoch [44/100], Step [8100/17448], Loss: 10.7500\n",
      "Epoch [44/100], Step [8200/17448], Loss: 7.2970\n",
      "Epoch [44/100], Step [8300/17448], Loss: 8.7032\n",
      "Epoch [44/100], Step [8400/17448], Loss: 9.3125\n",
      "Epoch [44/100], Step [8500/17448], Loss: 5.3438\n",
      "Epoch [44/100], Step [8600/17448], Loss: 9.4532\n",
      "Epoch [44/100], Step [8700/17448], Loss: 4.0938\n",
      "Epoch [44/100], Step [8800/17448], Loss: 7.6563\n",
      "Epoch [44/100], Step [8900/17448], Loss: 5.8594\n",
      "Epoch [44/100], Step [9000/17448], Loss: 4.4219\n",
      "Epoch [44/100], Step [9100/17448], Loss: 6.5313\n",
      "Epoch [44/100], Step [9200/17448], Loss: 8.5781\n",
      "Epoch [44/100], Step [9300/17448], Loss: 7.8907\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [45/100], Step [100/17448], Loss: 8.7188\n",
      "Epoch [45/100], Step [200/17448], Loss: 8.8126\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [46/100], Step [100/17448], Loss: 7.0781\n",
      "Epoch [46/100], Step [200/17448], Loss: 7.0938\n",
      "Epoch [46/100], Step [300/17448], Loss: 6.6407\n",
      "Epoch [46/100], Step [400/17448], Loss: 8.0625\n",
      "Epoch [46/100], Step [500/17448], Loss: 10.3906\n",
      "Epoch [46/100], Step [600/17448], Loss: 7.3438\n",
      "Epoch [46/100], Step [700/17448], Loss: 5.6563\n",
      "Epoch [46/100], Step [800/17448], Loss: 6.6250\n",
      "Epoch [46/100], Step [900/17448], Loss: 8.6719\n",
      "Epoch [46/100], Step [1000/17448], Loss: 10.0938\n",
      "Epoch [46/100], Step [1100/17448], Loss: 9.6251\n",
      "Epoch [46/100], Step [1200/17448], Loss: 8.3126\n",
      "Epoch [46/100], Step [1300/17448], Loss: 7.0470\n",
      "Epoch [46/100], Step [1400/17448], Loss: 10.6250\n",
      "Epoch [46/100], Step [1500/17448], Loss: 5.5313\n",
      "Epoch [46/100], Step [1600/17448], Loss: 7.2344\n",
      "Epoch [46/100], Step [1700/17448], Loss: 10.8907\n",
      "Epoch [46/100], Step [1800/17448], Loss: 9.0313\n",
      "Epoch [46/100], Step [1900/17448], Loss: 8.0626\n",
      "Epoch [46/100], Step [2000/17448], Loss: 7.3281\n",
      "Epoch [46/100], Step [2100/17448], Loss: 5.9064\n",
      "Epoch [46/100], Step [2200/17448], Loss: 7.2031\n",
      "Epoch [46/100], Step [2300/17448], Loss: 11.9063\n",
      "Epoch [46/100], Step [2400/17448], Loss: 7.4375\n",
      "Epoch [46/100], Step [2500/17448], Loss: 9.4844\n",
      "Epoch [46/100], Step [2600/17448], Loss: 6.4219\n",
      "Epoch [46/100], Step [2700/17448], Loss: 7.8594\n",
      "Epoch [46/100], Step [2800/17448], Loss: 6.9375\n",
      "Epoch [46/100], Step [2900/17448], Loss: 8.6564\n",
      "Epoch [46/100], Step [3000/17448], Loss: 6.6250\n",
      "Epoch [46/100], Step [3100/17448], Loss: 10.4688\n",
      "Epoch [46/100], Step [3200/17448], Loss: 12.3438\n",
      "Epoch [46/100], Step [3300/17448], Loss: 7.3596\n",
      "Epoch [46/100], Step [3400/17448], Loss: 5.3594\n",
      "Epoch [46/100], Step [3500/17448], Loss: 7.0001\n",
      "Epoch [46/100], Step [3600/17448], Loss: 5.6407\n",
      "Epoch [46/100], Step [3700/17448], Loss: 7.5313\n",
      "Epoch [46/100], Step [3800/17448], Loss: 7.0782\n",
      "Epoch [46/100], Step [3900/17448], Loss: 8.4376\n",
      "Epoch [46/100], Step [4000/17448], Loss: 7.4531\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [47/100], Step [100/17448], Loss: 7.8751\n",
      "Epoch [47/100], Step [200/17448], Loss: 8.0938\n",
      "Epoch [47/100], Step [300/17448], Loss: 10.0782\n",
      "Epoch [47/100], Step [400/17448], Loss: 9.1406\n",
      "Epoch [47/100], Step [500/17448], Loss: 8.5156\n",
      "Epoch [47/100], Step [600/17448], Loss: 9.0001\n",
      "Epoch [47/100], Step [700/17448], Loss: 6.7501\n",
      "Epoch [47/100], Step [800/17448], Loss: 8.3125\n",
      "Epoch [47/100], Step [900/17448], Loss: 9.5782\n",
      "Epoch [47/100], Step [1000/17448], Loss: 7.2188\n",
      "Epoch [47/100], Step [1100/17448], Loss: 10.0312\n",
      "Epoch [47/100], Step [1200/17448], Loss: 6.3281\n",
      "Epoch [47/100], Step [1300/17448], Loss: 5.8126\n",
      "Epoch [47/100], Step [1400/17448], Loss: 8.2969\n",
      "Epoch [47/100], Step [1500/17448], Loss: 6.9844\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [48/100], Step [100/17448], Loss: 5.8126\n",
      "Epoch [48/100], Step [200/17448], Loss: 3.8906\n",
      "Epoch [48/100], Step [300/17448], Loss: 11.2031\n",
      "Epoch [48/100], Step [400/17448], Loss: 4.9845\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [49/100], Step [100/17448], Loss: 7.5313\n",
      "Epoch [49/100], Step [200/17448], Loss: 5.3438\n",
      "Epoch [49/100], Step [300/17448], Loss: 6.7344\n",
      "Epoch [49/100], Step [400/17448], Loss: 6.5781\n",
      "Epoch [49/100], Step [500/17448], Loss: 9.6875\n",
      "Epoch [49/100], Step [600/17448], Loss: 5.0470\n",
      "Epoch [49/100], Step [700/17448], Loss: 10.1407\n",
      "Epoch [49/100], Step [800/17448], Loss: 6.2500\n",
      "Epoch [49/100], Step [900/17448], Loss: 11.6094\n",
      "Epoch [49/100], Step [1000/17448], Loss: 6.8594\n",
      "Epoch [49/100], Step [1100/17448], Loss: 8.0000\n",
      "Epoch [49/100], Step [1200/17448], Loss: 9.8595\n",
      "Epoch [49/100], Step [1300/17448], Loss: 4.8594\n",
      "Epoch [49/100], Step [1400/17448], Loss: 8.5001\n",
      "Epoch [49/100], Step [1500/17448], Loss: 7.1720\n",
      "Epoch [49/100], Step [1600/17448], Loss: 8.7657\n",
      "Epoch [49/100], Step [1700/17448], Loss: 8.2345\n",
      "Epoch [49/100], Step [1800/17448], Loss: 6.9063\n",
      "Epoch [49/100], Step [1900/17448], Loss: 7.3594\n",
      "Epoch [49/100], Step [2000/17448], Loss: 9.8751\n",
      "Epoch [49/100], Step [2100/17448], Loss: 8.3907\n",
      "Epoch [49/100], Step [2200/17448], Loss: 6.1875\n",
      "Epoch [49/100], Step [2300/17448], Loss: 9.6875\n",
      "Epoch [49/100], Step [2400/17448], Loss: 6.6251\n",
      "Epoch [49/100], Step [2500/17448], Loss: 6.6406\n",
      "Epoch [49/100], Step [2600/17448], Loss: 8.3284\n",
      "Epoch [49/100], Step [2700/17448], Loss: 9.4063\n",
      "Epoch [49/100], Step [2800/17448], Loss: 9.5938\n",
      "Epoch [49/100], Step [2900/17448], Loss: 11.4688\n",
      "Epoch [49/100], Step [3000/17448], Loss: 8.2969\n",
      "Epoch [49/100], Step [3100/17448], Loss: 6.8906\n",
      "Epoch [49/100], Step [3200/17448], Loss: 4.6720\n",
      "Epoch [49/100], Step [3300/17448], Loss: 8.4533\n",
      "Epoch [49/100], Step [3400/17448], Loss: 9.3125\n",
      "Epoch [49/100], Step [3500/17448], Loss: 7.7813\n",
      "Epoch [49/100], Step [3600/17448], Loss: 6.0000\n",
      "Epoch [49/100], Step [3700/17448], Loss: 6.5469\n",
      "Epoch [49/100], Step [3800/17448], Loss: 8.5469\n",
      "Epoch [49/100], Step [3900/17448], Loss: 9.6406\n",
      "Epoch [49/100], Step [4000/17448], Loss: 6.5314\n",
      "Epoch [49/100], Step [4100/17448], Loss: 6.3125\n",
      "Epoch [49/100], Step [4200/17448], Loss: 5.1875\n",
      "Epoch [49/100], Step [4300/17448], Loss: 8.0314\n",
      "Epoch [49/100], Step [4400/17448], Loss: 6.1875\n",
      "Epoch [49/100], Step [4500/17448], Loss: 7.8907\n",
      "Epoch [49/100], Step [4600/17448], Loss: 8.2501\n",
      "Epoch [49/100], Step [4700/17448], Loss: 9.5000\n",
      "Epoch [49/100], Step [4800/17448], Loss: 6.2032\n",
      "Epoch [49/100], Step [4900/17448], Loss: 4.7344\n",
      "Epoch [49/100], Step [5000/17448], Loss: 6.9532\n",
      "Epoch [49/100], Step [5100/17448], Loss: 6.4533\n",
      "Epoch [49/100], Step [5200/17448], Loss: 8.3439\n",
      "Epoch [49/100], Step [5300/17448], Loss: 6.5313\n",
      "Epoch [49/100], Step [5400/17448], Loss: 7.0313\n",
      "Epoch [49/100], Step [5500/17448], Loss: 7.1564\n",
      "Epoch [49/100], Step [5600/17448], Loss: 7.3594\n",
      "Epoch [49/100], Step [5700/17448], Loss: 7.0625\n",
      "Epoch [49/100], Step [5800/17448], Loss: 5.4845\n",
      "Epoch [49/100], Step [5900/17448], Loss: 7.6094\n",
      "Epoch [49/100], Step [6000/17448], Loss: 7.0157\n",
      "Epoch [49/100], Step [6100/17448], Loss: 9.3281\n",
      "Epoch [49/100], Step [6200/17448], Loss: 6.4219\n",
      "Epoch [49/100], Step [6300/17448], Loss: 5.3282\n",
      "Epoch [49/100], Step [6400/17448], Loss: 9.1406\n",
      "Epoch [49/100], Step [6500/17448], Loss: 8.6094\n",
      "Epoch [49/100], Step [6600/17448], Loss: 7.0313\n",
      "Epoch [49/100], Step [6700/17448], Loss: 8.1407\n",
      "Epoch [49/100], Step [6800/17448], Loss: 8.0628\n",
      "Epoch [49/100], Step [6900/17448], Loss: 3.7188\n",
      "Epoch [49/100], Step [7000/17448], Loss: 6.9531\n",
      "Epoch [49/100], Step [7100/17448], Loss: 8.8594\n",
      "Epoch [49/100], Step [7200/17448], Loss: 7.2032\n",
      "Epoch [49/100], Step [7300/17448], Loss: 9.2502\n",
      "Epoch [49/100], Step [7400/17448], Loss: 7.4375\n",
      "Epoch [49/100], Step [7500/17448], Loss: 6.4220\n",
      "Epoch [49/100], Step [7600/17448], Loss: 6.3906\n",
      "Epoch [49/100], Step [7700/17448], Loss: 6.3282\n",
      "Epoch [49/100], Step [7800/17448], Loss: 5.7344\n",
      "Epoch [49/100], Step [7900/17448], Loss: 6.8751\n",
      "Epoch [49/100], Step [8000/17448], Loss: 5.6720\n",
      "Epoch [49/100], Step [8100/17448], Loss: 11.2189\n",
      "Epoch [49/100], Step [8200/17448], Loss: 6.8907\n",
      "Epoch [49/100], Step [8300/17448], Loss: 4.7969\n",
      "Epoch [49/100], Step [8400/17448], Loss: 4.0625\n",
      "Epoch [49/100], Step [8500/17448], Loss: 8.2032\n",
      "Epoch [49/100], Step [8600/17448], Loss: 7.8125\n",
      "Epoch [49/100], Step [8700/17448], Loss: 7.8284\n",
      "Epoch [49/100], Step [8800/17448], Loss: 6.9689\n",
      "Epoch [49/100], Step [8900/17448], Loss: 9.4063\n",
      "Epoch [49/100], Step [9000/17448], Loss: 11.6563\n",
      "Epoch [49/100], Step [9100/17448], Loss: 6.6095\n",
      "Epoch [49/100], Step [9200/17448], Loss: 4.3282\n",
      "Epoch [49/100], Step [9300/17448], Loss: 7.6094\n",
      "Epoch [49/100], Step [9400/17448], Loss: 11.1564\n",
      "Epoch [49/100], Step [9500/17448], Loss: 5.8283\n",
      "Epoch [49/100], Step [9600/17448], Loss: 6.6875\n",
      "Epoch [49/100], Step [9700/17448], Loss: 12.3594\n",
      "Epoch [49/100], Step [9800/17448], Loss: 7.9533\n",
      "Epoch [49/100], Step [9900/17448], Loss: 8.1875\n",
      "Epoch [49/100], Step [10000/17448], Loss: 6.0625\n",
      "Epoch [49/100], Step [10100/17448], Loss: 11.9063\n",
      "Epoch [49/100], Step [10200/17448], Loss: 6.0782\n",
      "Epoch [49/100], Step [10300/17448], Loss: 9.2657\n",
      "Epoch [49/100], Step [10400/17448], Loss: 7.2969\n",
      "Epoch [49/100], Step [10500/17448], Loss: 6.5158\n",
      "Epoch [49/100], Step [10600/17448], Loss: 9.1406\n",
      "Epoch [49/100], Step [10700/17448], Loss: 7.0469\n",
      "Epoch [49/100], Step [10800/17448], Loss: 11.1094\n",
      "Epoch [49/100], Step [10900/17448], Loss: 10.8750\n",
      "Epoch [49/100], Step [11000/17448], Loss: 5.2032\n",
      "Epoch [49/100], Step [11100/17448], Loss: 9.2813\n",
      "Epoch [49/100], Step [11200/17448], Loss: 6.0002\n",
      "Epoch [49/100], Step [11300/17448], Loss: 9.0156\n",
      "Epoch [49/100], Step [11400/17448], Loss: 9.7032\n",
      "Epoch [49/100], Step [11500/17448], Loss: 6.6719\n",
      "Epoch [49/100], Step [11600/17448], Loss: 9.9063\n",
      "Epoch [49/100], Step [11700/17448], Loss: 7.1564\n",
      "Epoch [49/100], Step [11800/17448], Loss: 6.4846\n",
      "Epoch [49/100], Step [11900/17448], Loss: 10.3125\n",
      "Epoch [49/100], Step [12000/17448], Loss: 9.4688\n",
      "Epoch [49/100], Step [12100/17448], Loss: 9.1406\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [50/100], Step [100/17448], Loss: 7.2031\n",
      "Epoch [50/100], Step [200/17448], Loss: 8.1875\n",
      "Epoch [50/100], Step [300/17448], Loss: 8.8438\n",
      "Epoch [50/100], Step [400/17448], Loss: 6.6250\n",
      "Epoch [50/100], Step [500/17448], Loss: 5.9533\n",
      "Epoch [50/100], Step [600/17448], Loss: 6.2032\n",
      "Epoch [50/100], Step [700/17448], Loss: 7.7032\n",
      "Epoch [50/100], Step [800/17448], Loss: 9.7500\n",
      "Epoch [50/100], Step [900/17448], Loss: 5.6720\n",
      "Epoch [50/100], Step [1000/17448], Loss: 8.0938\n",
      "Epoch [50/100], Step [1100/17448], Loss: 6.3438\n",
      "Epoch [50/100], Step [1200/17448], Loss: 4.3750\n",
      "Epoch [50/100], Step [1300/17448], Loss: 7.0938\n",
      "Epoch [50/100], Step [1400/17448], Loss: 5.3438\n",
      "Epoch [50/100], Step [1500/17448], Loss: 6.7969\n",
      "Epoch [50/100], Step [1600/17448], Loss: 11.3281\n",
      "Epoch [50/100], Step [1700/17448], Loss: 6.3907\n",
      "Epoch [50/100], Step [1800/17448], Loss: 6.7032\n",
      "Epoch [50/100], Step [1900/17448], Loss: 7.7813\n",
      "Epoch [50/100], Step [2000/17448], Loss: 5.9845\n",
      "Epoch [50/100], Step [2100/17448], Loss: 8.8439\n",
      "Epoch [50/100], Step [2200/17448], Loss: 7.1406\n",
      "Epoch [50/100], Step [2300/17448], Loss: 8.2032\n",
      "Epoch [50/100], Step [2400/17448], Loss: 8.5469\n",
      "Epoch [50/100], Step [2500/17448], Loss: 6.4063\n",
      "Epoch [50/100], Step [2600/17448], Loss: 6.9376\n",
      "Epoch [50/100], Step [2700/17448], Loss: 8.2344\n",
      "Epoch [50/100], Step [2800/17448], Loss: 10.4532\n",
      "Epoch [50/100], Step [2900/17448], Loss: 8.4534\n",
      "Epoch [50/100], Step [3000/17448], Loss: 5.7501\n",
      "Epoch [50/100], Step [3100/17448], Loss: 8.0783\n",
      "Epoch [50/100], Step [3200/17448], Loss: 8.6877\n",
      "Epoch [50/100], Step [3300/17448], Loss: 6.9844\n",
      "Epoch [50/100], Step [3400/17448], Loss: 9.5159\n",
      "Epoch [50/100], Step [3500/17448], Loss: 7.6407\n",
      "Epoch [50/100], Step [3600/17448], Loss: 7.7345\n",
      "Epoch [50/100], Step [3700/17448], Loss: 8.7188\n",
      "Epoch [50/100], Step [3800/17448], Loss: 7.9063\n",
      "Epoch [50/100], Step [3900/17448], Loss: 7.7345\n",
      "Epoch [50/100], Step [4000/17448], Loss: 9.6406\n",
      "Epoch [50/100], Step [4100/17448], Loss: 9.8750\n",
      "Epoch [50/100], Step [4200/17448], Loss: 7.4534\n",
      "Epoch [50/100], Step [4300/17448], Loss: 5.6094\n",
      "Epoch [50/100], Step [4400/17448], Loss: 7.7813\n",
      "Epoch [50/100], Step [4500/17448], Loss: 7.6564\n",
      "Epoch [50/100], Step [4600/17448], Loss: 6.8907\n",
      "Epoch [50/100], Step [4700/17448], Loss: 7.8438\n",
      "Epoch [50/100], Step [4800/17448], Loss: 6.1876\n",
      "Epoch [50/100], Step [4900/17448], Loss: 10.0469\n",
      "Epoch [50/100], Step [5000/17448], Loss: 9.7969\n",
      "Epoch [50/100], Step [5100/17448], Loss: 8.5782\n",
      "Epoch [50/100], Step [5200/17448], Loss: 9.4531\n",
      "Epoch [50/100], Step [5300/17448], Loss: 11.4063\n",
      "Epoch [50/100], Step [5400/17448], Loss: 6.1719\n",
      "Epoch [50/100], Step [5500/17448], Loss: 8.5156\n",
      "Epoch [50/100], Step [5600/17448], Loss: 5.9219\n",
      "Epoch [50/100], Step [5700/17448], Loss: 6.5158\n",
      "Epoch [50/100], Step [5800/17448], Loss: 8.3281\n",
      "Epoch [50/100], Step [5900/17448], Loss: 6.9531\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [51/100], Step [100/17448], Loss: 8.9064\n",
      "Epoch [51/100], Step [200/17448], Loss: 5.3594\n",
      "Epoch [51/100], Step [300/17448], Loss: 7.0626\n",
      "Epoch [51/100], Step [400/17448], Loss: 5.5469\n",
      "Epoch [51/100], Step [500/17448], Loss: 6.4063\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [52/100], Step [100/17448], Loss: 5.7970\n",
      "Epoch [52/100], Step [200/17448], Loss: 8.5313\n",
      "Epoch [52/100], Step [300/17448], Loss: 7.4063\n",
      "Epoch [52/100], Step [400/17448], Loss: 8.0157\n",
      "Epoch [52/100], Step [500/17448], Loss: 9.9689\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [53/100], Step [100/17448], Loss: 5.9376\n",
      "Epoch [53/100], Step [200/17448], Loss: 8.5782\n",
      "Epoch [53/100], Step [300/17448], Loss: 7.0157\n",
      "Epoch [53/100], Step [400/17448], Loss: 7.5001\n",
      "Epoch [53/100], Step [500/17448], Loss: 8.0000\n",
      "Epoch [53/100], Step [600/17448], Loss: 5.1563\n",
      "Epoch [53/100], Step [700/17448], Loss: 5.6094\n",
      "Epoch [53/100], Step [800/17448], Loss: 6.8282\n",
      "Epoch [53/100], Step [900/17448], Loss: 3.9219\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [54/100], Step [100/17448], Loss: 6.7031\n",
      "Epoch [54/100], Step [200/17448], Loss: 4.7500\n",
      "Epoch [54/100], Step [300/17448], Loss: 3.9688\n",
      "Epoch [54/100], Step [400/17448], Loss: 5.9531\n",
      "Epoch [54/100], Step [500/17448], Loss: 6.2501\n",
      "Epoch [54/100], Step [600/17448], Loss: 7.1094\n",
      "Epoch [54/100], Step [700/17448], Loss: 9.2344\n",
      "Epoch [54/100], Step [800/17448], Loss: 6.3751\n",
      "Epoch [54/100], Step [900/17448], Loss: 7.1719\n",
      "Epoch [54/100], Step [1000/17448], Loss: 5.6250\n",
      "Epoch [54/100], Step [1100/17448], Loss: 11.2813\n",
      "Epoch [54/100], Step [1200/17448], Loss: 7.1094\n",
      "Epoch [54/100], Step [1300/17448], Loss: 8.9219\n",
      "Epoch [54/100], Step [1400/17448], Loss: 5.2031\n",
      "Epoch [54/100], Step [1500/17448], Loss: 7.1094\n",
      "Epoch [54/100], Step [1600/17448], Loss: 4.2501\n",
      "Epoch [54/100], Step [1700/17448], Loss: 10.0312\n",
      "Epoch [54/100], Step [1800/17448], Loss: 7.2031\n",
      "Epoch [54/100], Step [1900/17448], Loss: 6.3281\n",
      "Epoch [54/100], Step [2000/17448], Loss: 8.1095\n",
      "Epoch [54/100], Step [2100/17448], Loss: 7.2969\n",
      "Epoch [54/100], Step [2200/17448], Loss: 8.7656\n",
      "Epoch [54/100], Step [2300/17448], Loss: 10.3438\n",
      "Epoch [54/100], Step [2400/17448], Loss: 7.1252\n",
      "Epoch [54/100], Step [2500/17448], Loss: 6.4689\n",
      "Epoch [54/100], Step [2600/17448], Loss: 5.5939\n",
      "Epoch [54/100], Step [2700/17448], Loss: 4.9532\n",
      "Epoch [54/100], Step [2800/17448], Loss: 5.9531\n",
      "Epoch [54/100], Step [2900/17448], Loss: 8.4063\n",
      "Epoch [54/100], Step [3000/17448], Loss: 9.6406\n",
      "Epoch [54/100], Step [3100/17448], Loss: 10.1875\n",
      "Epoch [54/100], Step [3200/17448], Loss: 5.2971\n",
      "Epoch [54/100], Step [3300/17448], Loss: 7.6875\n",
      "Epoch [54/100], Step [3400/17448], Loss: 9.5938\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [55/100], Step [100/17448], Loss: 11.8125\n",
      "Epoch [55/100], Step [200/17448], Loss: 7.9219\n",
      "Epoch [55/100], Step [300/17448], Loss: 8.0157\n",
      "Epoch [55/100], Step [400/17448], Loss: 5.4688\n",
      "Epoch [55/100], Step [500/17448], Loss: 5.9376\n",
      "Epoch [55/100], Step [600/17448], Loss: 6.9845\n",
      "Epoch [55/100], Step [700/17448], Loss: 6.0001\n",
      "Epoch [55/100], Step [800/17448], Loss: 5.0000\n",
      "Epoch [55/100], Step [900/17448], Loss: 10.1563\n",
      "Epoch [55/100], Step [1000/17448], Loss: 6.2813\n",
      "Epoch [55/100], Step [1100/17448], Loss: 11.1875\n",
      "Epoch [55/100], Step [1200/17448], Loss: 8.8907\n",
      "Epoch [55/100], Step [1300/17448], Loss: 9.1094\n",
      "Epoch [55/100], Step [1400/17448], Loss: 6.9219\n",
      "Epoch [55/100], Step [1500/17448], Loss: 9.4531\n",
      "Epoch [55/100], Step [1600/17448], Loss: 6.8285\n",
      "Epoch [55/100], Step [1700/17448], Loss: 8.3594\n",
      "Epoch [55/100], Step [1800/17448], Loss: 6.5156\n",
      "Epoch [55/100], Step [1900/17448], Loss: 6.0313\n",
      "Epoch [55/100], Step [2000/17448], Loss: 4.5157\n",
      "Epoch [55/100], Step [2100/17448], Loss: 9.9531\n",
      "Epoch [55/100], Step [2200/17448], Loss: 6.4688\n",
      "Epoch [55/100], Step [2300/17448], Loss: 3.3594\n",
      "Epoch [55/100], Step [2400/17448], Loss: 8.9531\n",
      "Epoch [55/100], Step [2500/17448], Loss: 5.2345\n",
      "Epoch [55/100], Step [2600/17448], Loss: 12.5782\n",
      "Epoch [55/100], Step [2700/17448], Loss: 9.0313\n",
      "Epoch [55/100], Step [2800/17448], Loss: 7.2344\n",
      "Epoch [55/100], Step [2900/17448], Loss: 8.7344\n",
      "Epoch [55/100], Step [3000/17448], Loss: 5.8438\n",
      "Epoch [55/100], Step [3100/17448], Loss: 11.7034\n",
      "Epoch [55/100], Step [3200/17448], Loss: 6.0938\n",
      "Epoch [55/100], Step [3300/17448], Loss: 6.4063\n",
      "Epoch [55/100], Step [3400/17448], Loss: 11.0938\n",
      "Epoch [55/100], Step [3500/17448], Loss: 6.0938\n",
      "Epoch [55/100], Step [3600/17448], Loss: 8.0000\n",
      "Epoch [55/100], Step [3700/17448], Loss: 6.3125\n",
      "Epoch [55/100], Step [3800/17448], Loss: 9.2657\n",
      "Epoch [55/100], Step [3900/17448], Loss: 5.5313\n",
      "Epoch [55/100], Step [4000/17448], Loss: 8.4532\n",
      "Epoch [55/100], Step [4100/17448], Loss: 12.7500\n",
      "Epoch [55/100], Step [4200/17448], Loss: 7.8594\n",
      "Epoch [55/100], Step [4300/17448], Loss: 6.4533\n",
      "Epoch [55/100], Step [4400/17448], Loss: 5.5000\n",
      "Epoch [55/100], Step [4500/17448], Loss: 5.6250\n",
      "Epoch [55/100], Step [4600/17448], Loss: 6.4688\n",
      "Epoch [55/100], Step [4700/17448], Loss: 7.5625\n",
      "Epoch [55/100], Step [4800/17448], Loss: 8.7813\n",
      "Epoch [55/100], Step [4900/17448], Loss: 10.6406\n",
      "Epoch [55/100], Step [5000/17448], Loss: 8.6094\n",
      "Epoch [55/100], Step [5100/17448], Loss: 9.6406\n",
      "Epoch [55/100], Step [5200/17448], Loss: 8.6719\n",
      "Epoch [55/100], Step [5300/17448], Loss: 8.1719\n",
      "Epoch [55/100], Step [5400/17448], Loss: 8.2656\n",
      "Epoch [55/100], Step [5500/17448], Loss: 11.0313\n",
      "Epoch [55/100], Step [5600/17448], Loss: 6.7189\n",
      "Epoch [55/100], Step [5700/17448], Loss: 8.5156\n",
      "Epoch [55/100], Step [5800/17448], Loss: 8.1408\n",
      "Epoch [55/100], Step [5900/17448], Loss: 8.3594\n",
      "Epoch [55/100], Step [6000/17448], Loss: 6.8594\n",
      "Epoch [55/100], Step [6100/17448], Loss: 11.1407\n",
      "Epoch [55/100], Step [6200/17448], Loss: 7.8751\n",
      "Epoch [55/100], Step [6300/17448], Loss: 8.0157\n",
      "Epoch [55/100], Step [6400/17448], Loss: 7.3750\n",
      "Epoch [55/100], Step [6500/17448], Loss: 6.0469\n",
      "Epoch [55/100], Step [6600/17448], Loss: 6.4222\n",
      "Epoch [55/100], Step [6700/17448], Loss: 6.4533\n",
      "Epoch [55/100], Step [6800/17448], Loss: 7.1094\n",
      "Epoch [55/100], Step [6900/17448], Loss: 7.6721\n",
      "Epoch [55/100], Step [7000/17448], Loss: 9.3281\n",
      "Epoch [55/100], Step [7100/17448], Loss: 10.2344\n",
      "Epoch [55/100], Step [7200/17448], Loss: 8.5313\n",
      "Epoch [55/100], Step [7300/17448], Loss: 10.2344\n",
      "Epoch [55/100], Step [7400/17448], Loss: 5.8906\n",
      "Epoch [55/100], Step [7500/17448], Loss: 5.3125\n",
      "Epoch [55/100], Step [7600/17448], Loss: 8.7656\n",
      "Epoch [55/100], Step [7700/17448], Loss: 10.0626\n",
      "Epoch [55/100], Step [7800/17448], Loss: 9.5782\n",
      "Epoch [55/100], Step [7900/17448], Loss: 8.5782\n",
      "Epoch [55/100], Step [8000/17448], Loss: 9.5469\n",
      "Epoch [55/100], Step [8100/17448], Loss: 8.2656\n",
      "Epoch [55/100], Step [8200/17448], Loss: 12.8438\n",
      "Epoch [55/100], Step [8300/17448], Loss: 7.9845\n",
      "Epoch [55/100], Step [8400/17448], Loss: 8.5000\n",
      "Epoch [55/100], Step [8500/17448], Loss: 8.6563\n",
      "Epoch [55/100], Step [8600/17448], Loss: 5.1094\n",
      "Epoch [55/100], Step [8700/17448], Loss: 7.1875\n",
      "Epoch [55/100], Step [8800/17448], Loss: 6.8750\n",
      "Epoch [55/100], Step [8900/17448], Loss: 10.2032\n",
      "Epoch [55/100], Step [9000/17448], Loss: 10.1563\n",
      "Epoch [55/100], Step [9100/17448], Loss: 7.4063\n",
      "Epoch [55/100], Step [9200/17448], Loss: 6.5626\n",
      "Epoch [55/100], Step [9300/17448], Loss: 6.9063\n",
      "Epoch [55/100], Step [9400/17448], Loss: 9.6719\n",
      "Epoch [55/100], Step [9500/17448], Loss: 10.2032\n",
      "Epoch [55/100], Step [9600/17448], Loss: 8.2970\n",
      "Epoch [55/100], Step [9700/17448], Loss: 8.7032\n",
      "Epoch [55/100], Step [9800/17448], Loss: 5.7814\n",
      "Epoch [55/100], Step [9900/17448], Loss: 6.0625\n",
      "Epoch [55/100], Step [10000/17448], Loss: 8.6095\n",
      "Epoch [55/100], Step [10100/17448], Loss: 8.2500\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [56/100], Step [100/17448], Loss: 9.3439\n",
      "Epoch [56/100], Step [200/17448], Loss: 7.8594\n",
      "Epoch [56/100], Step [300/17448], Loss: 9.2657\n",
      "Epoch [56/100], Step [400/17448], Loss: 5.0312\n",
      "Epoch [56/100], Step [500/17448], Loss: 6.3438\n",
      "Epoch [56/100], Step [600/17448], Loss: 6.2188\n",
      "Epoch [56/100], Step [700/17448], Loss: 9.3281\n",
      "Epoch [56/100], Step [800/17448], Loss: 8.2812\n",
      "Epoch [56/100], Step [900/17448], Loss: 4.9220\n",
      "Epoch [56/100], Step [1000/17448], Loss: 4.3594\n",
      "Epoch [56/100], Step [1100/17448], Loss: 4.8439\n",
      "Epoch [56/100], Step [1200/17448], Loss: 4.4689\n",
      "Epoch [56/100], Step [1300/17448], Loss: 6.9532\n",
      "Epoch [56/100], Step [1400/17448], Loss: 8.9065\n",
      "Epoch [56/100], Step [1500/17448], Loss: 8.4063\n",
      "Epoch [56/100], Step [1600/17448], Loss: 8.9688\n",
      "Epoch [56/100], Step [1700/17448], Loss: 6.7188\n",
      "Epoch [56/100], Step [1800/17448], Loss: 8.0781\n",
      "Epoch [56/100], Step [1900/17448], Loss: 8.3751\n",
      "Epoch [56/100], Step [2000/17448], Loss: 7.4531\n",
      "Epoch [56/100], Step [2100/17448], Loss: 7.1563\n",
      "Epoch [56/100], Step [2200/17448], Loss: 9.3438\n",
      "Epoch [56/100], Step [2300/17448], Loss: 7.2658\n",
      "Epoch [56/100], Step [2400/17448], Loss: 9.1407\n",
      "Epoch [56/100], Step [2500/17448], Loss: 4.1407\n",
      "Epoch [56/100], Step [2600/17448], Loss: 10.5781\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [57/100], Step [100/17448], Loss: 12.3750\n",
      "Epoch [57/100], Step [200/17448], Loss: 9.2189\n",
      "Epoch [57/100], Step [300/17448], Loss: 6.7813\n",
      "Epoch [57/100], Step [400/17448], Loss: 8.3750\n",
      "Epoch [57/100], Step [500/17448], Loss: 10.5157\n",
      "Epoch [57/100], Step [600/17448], Loss: 7.2501\n",
      "Epoch [57/100], Step [700/17448], Loss: 6.5782\n",
      "Epoch [57/100], Step [800/17448], Loss: 7.8751\n",
      "Epoch [57/100], Step [900/17448], Loss: 8.3126\n",
      "Epoch [57/100], Step [1000/17448], Loss: 8.0625\n",
      "Epoch [57/100], Step [1100/17448], Loss: 8.7657\n",
      "Epoch [57/100], Step [1200/17448], Loss: 7.2969\n",
      "Epoch [57/100], Step [1300/17448], Loss: 8.2500\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [58/100], Step [100/17448], Loss: 6.7969\n",
      "Epoch [58/100], Step [200/17448], Loss: 8.5001\n",
      "Epoch [58/100], Step [300/17448], Loss: 5.9220\n",
      "Epoch [58/100], Step [400/17448], Loss: 6.3125\n",
      "Epoch [58/100], Step [500/17448], Loss: 7.5000\n",
      "Epoch [58/100], Step [600/17448], Loss: 9.4219\n",
      "Epoch [58/100], Step [700/17448], Loss: 6.9063\n",
      "Epoch [58/100], Step [800/17448], Loss: 7.6563\n",
      "Epoch [58/100], Step [900/17448], Loss: 5.9063\n",
      "Epoch [58/100], Step [1000/17448], Loss: 7.6719\n",
      "Epoch [58/100], Step [1100/17448], Loss: 6.4844\n",
      "Epoch [58/100], Step [1200/17448], Loss: 5.1563\n",
      "Epoch [58/100], Step [1300/17448], Loss: 6.3750\n",
      "Epoch [58/100], Step [1400/17448], Loss: 8.0158\n",
      "Epoch [58/100], Step [1500/17448], Loss: 8.6564\n",
      "Epoch [58/100], Step [1600/17448], Loss: 6.5314\n",
      "Epoch [58/100], Step [1700/17448], Loss: 11.0781\n",
      "Epoch [58/100], Step [1800/17448], Loss: 6.5938\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [59/100], Step [100/17448], Loss: 8.6876\n",
      "Epoch [59/100], Step [200/17448], Loss: 7.8750\n",
      "Epoch [59/100], Step [300/17448], Loss: 6.6563\n",
      "Epoch [59/100], Step [400/17448], Loss: 7.8438\n",
      "Epoch [59/100], Step [500/17448], Loss: 5.1563\n",
      "Epoch [59/100], Step [600/17448], Loss: 4.7345\n",
      "Epoch [59/100], Step [700/17448], Loss: 7.0313\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [60/100], Step [100/17448], Loss: 11.3125\n",
      "Epoch [60/100], Step [200/17448], Loss: 6.3594\n",
      "Epoch [60/100], Step [300/17448], Loss: 7.0781\n",
      "Epoch [60/100], Step [400/17448], Loss: 9.2188\n",
      "Epoch [60/100], Step [500/17448], Loss: 5.2813\n",
      "Epoch [60/100], Step [600/17448], Loss: 7.0625\n",
      "Epoch [60/100], Step [700/17448], Loss: 7.8750\n",
      "Epoch [60/100], Step [800/17448], Loss: 8.9531\n",
      "Epoch [60/100], Step [900/17448], Loss: 7.1094\n",
      "Epoch [60/100], Step [1000/17448], Loss: 8.7658\n",
      "Epoch [60/100], Step [1100/17448], Loss: 7.3906\n",
      "Epoch [60/100], Step [1200/17448], Loss: 9.4219\n",
      "Epoch [60/100], Step [1300/17448], Loss: 6.3751\n",
      "Epoch [60/100], Step [1400/17448], Loss: 6.8127\n",
      "Epoch [60/100], Step [1500/17448], Loss: 6.5313\n",
      "Epoch [60/100], Step [1600/17448], Loss: 7.1875\n",
      "Epoch [60/100], Step [1700/17448], Loss: 7.4844\n",
      "Epoch [60/100], Step [1800/17448], Loss: 10.7031\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [61/100], Step [100/17448], Loss: 6.6875\n",
      "Epoch [61/100], Step [200/17448], Loss: 6.7032\n",
      "Epoch [61/100], Step [300/17448], Loss: 6.3751\n",
      "Epoch [61/100], Step [400/17448], Loss: 10.0313\n",
      "Epoch [61/100], Step [500/17448], Loss: 7.7344\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [62/100], Step [100/17448], Loss: 6.3907\n",
      "Epoch [62/100], Step [200/17448], Loss: 7.3125\n",
      "Epoch [62/100], Step [300/17448], Loss: 9.3594\n",
      "Epoch [62/100], Step [400/17448], Loss: 9.0313\n",
      "Epoch [62/100], Step [500/17448], Loss: 9.0782\n",
      "Epoch [62/100], Step [600/17448], Loss: 9.2032\n",
      "Epoch [62/100], Step [700/17448], Loss: 8.6250\n",
      "Epoch [62/100], Step [800/17448], Loss: 7.9219\n",
      "Epoch [62/100], Step [900/17448], Loss: 8.1719\n",
      "Epoch [62/100], Step [1000/17448], Loss: 7.3594\n",
      "Epoch [62/100], Step [1100/17448], Loss: 7.1563\n",
      "Epoch [62/100], Step [1200/17448], Loss: 7.4532\n",
      "Epoch [62/100], Step [1300/17448], Loss: 6.4531\n",
      "Epoch [62/100], Step [1400/17448], Loss: 7.2031\n",
      "Epoch [62/100], Step [1500/17448], Loss: 6.0625\n",
      "Epoch [62/100], Step [1600/17448], Loss: 8.1250\n",
      "Epoch [62/100], Step [1700/17448], Loss: 7.9375\n",
      "Epoch [62/100], Step [1800/17448], Loss: 5.1876\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [63/100], Step [100/17448], Loss: 7.5313\n",
      "Epoch [63/100], Step [200/17448], Loss: 7.2188\n",
      "Epoch [63/100], Step [300/17448], Loss: 8.0313\n",
      "Epoch [63/100], Step [400/17448], Loss: 5.9219\n",
      "Epoch [63/100], Step [500/17448], Loss: 11.0781\n",
      "Epoch [63/100], Step [600/17448], Loss: 7.0625\n",
      "Epoch [63/100], Step [700/17448], Loss: 8.1564\n",
      "Epoch [63/100], Step [800/17448], Loss: 11.4531\n",
      "Epoch [63/100], Step [900/17448], Loss: 8.7190\n",
      "Epoch [63/100], Step [1000/17448], Loss: 7.7656\n",
      "Epoch [63/100], Step [1100/17448], Loss: 9.1875\n",
      "Epoch [63/100], Step [1200/17448], Loss: 6.6094\n",
      "Epoch [63/100], Step [1300/17448], Loss: 8.1875\n",
      "Epoch [63/100], Step [1400/17448], Loss: 8.5001\n",
      "Epoch [63/100], Step [1500/17448], Loss: 7.3282\n",
      "Epoch [63/100], Step [1600/17448], Loss: 3.7657\n",
      "Epoch [63/100], Step [1700/17448], Loss: 10.2032\n",
      "Epoch [63/100], Step [1800/17448], Loss: 6.1406\n",
      "Epoch [63/100], Step [1900/17448], Loss: 6.3594\n",
      "Epoch [63/100], Step [2000/17448], Loss: 8.3750\n",
      "Epoch [63/100], Step [2100/17448], Loss: 6.0314\n",
      "Epoch [63/100], Step [2200/17448], Loss: 6.4845\n",
      "Epoch [63/100], Step [2300/17448], Loss: 8.3125\n",
      "Epoch [63/100], Step [2400/17448], Loss: 10.8907\n",
      "Epoch [63/100], Step [2500/17448], Loss: 8.2344\n",
      "Epoch [63/100], Step [2600/17448], Loss: 12.5938\n",
      "Epoch [63/100], Step [2700/17448], Loss: 4.9688\n",
      "Epoch [63/100], Step [2800/17448], Loss: 4.3282\n",
      "Epoch [63/100], Step [2900/17448], Loss: 6.6720\n",
      "Epoch [63/100], Step [3000/17448], Loss: 6.4063\n",
      "Epoch [63/100], Step [3100/17448], Loss: 12.6564\n",
      "Epoch [63/100], Step [3200/17448], Loss: 10.5626\n",
      "Epoch [63/100], Step [3300/17448], Loss: 11.3594\n",
      "Epoch [63/100], Step [3400/17448], Loss: 5.8594\n",
      "Epoch [63/100], Step [3500/17448], Loss: 6.9063\n",
      "Epoch [63/100], Step [3600/17448], Loss: 5.3594\n",
      "Epoch [63/100], Step [3700/17448], Loss: 5.9219\n",
      "Epoch [63/100], Step [3800/17448], Loss: 8.3125\n",
      "Epoch [63/100], Step [3900/17448], Loss: 7.0782\n",
      "Epoch [63/100], Step [4000/17448], Loss: 9.4689\n",
      "Epoch [63/100], Step [4100/17448], Loss: 9.3125\n",
      "Epoch [63/100], Step [4200/17448], Loss: 6.0313\n",
      "Epoch [63/100], Step [4300/17448], Loss: 7.2344\n",
      "Epoch [63/100], Step [4400/17448], Loss: 6.6250\n",
      "Epoch [63/100], Step [4500/17448], Loss: 6.2345\n",
      "Epoch [63/100], Step [4600/17448], Loss: 6.0469\n",
      "Epoch [63/100], Step [4700/17448], Loss: 5.7501\n",
      "Epoch [63/100], Step [4800/17448], Loss: 11.4063\n",
      "Epoch [63/100], Step [4900/17448], Loss: 5.6251\n",
      "Epoch [63/100], Step [5000/17448], Loss: 8.7814\n",
      "Epoch [63/100], Step [5100/17448], Loss: 5.1719\n",
      "Epoch [63/100], Step [5200/17448], Loss: 10.5938\n",
      "Epoch [63/100], Step [5300/17448], Loss: 9.7031\n",
      "Epoch [63/100], Step [5400/17448], Loss: 6.6251\n",
      "Epoch [63/100], Step [5500/17448], Loss: 6.7970\n",
      "Epoch [63/100], Step [5600/17448], Loss: 6.5782\n",
      "Epoch [63/100], Step [5700/17448], Loss: 10.2500\n",
      "Epoch [63/100], Step [5800/17448], Loss: 7.7656\n",
      "Epoch [63/100], Step [5900/17448], Loss: 10.4688\n",
      "Epoch [63/100], Step [6000/17448], Loss: 7.7501\n",
      "Epoch [63/100], Step [6100/17448], Loss: 6.6407\n",
      "Epoch [63/100], Step [6200/17448], Loss: 7.4063\n",
      "Epoch [63/100], Step [6300/17448], Loss: 5.5938\n",
      "Epoch [63/100], Step [6400/17448], Loss: 4.5939\n",
      "Epoch [63/100], Step [6500/17448], Loss: 6.7032\n",
      "Epoch [63/100], Step [6600/17448], Loss: 6.5001\n",
      "Epoch [63/100], Step [6700/17448], Loss: 10.1720\n",
      "Epoch [63/100], Step [6800/17448], Loss: 7.8282\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [64/100], Step [100/17448], Loss: 7.7033\n",
      "Epoch [64/100], Step [200/17448], Loss: 8.7501\n",
      "Epoch [64/100], Step [300/17448], Loss: 11.0938\n",
      "Epoch [64/100], Step [400/17448], Loss: 5.3127\n",
      "Epoch [64/100], Step [500/17448], Loss: 5.6406\n",
      "Epoch [64/100], Step [600/17448], Loss: 6.3438\n",
      "Epoch [64/100], Step [700/17448], Loss: 10.1719\n",
      "Epoch [64/100], Step [800/17448], Loss: 6.5938\n",
      "Epoch [64/100], Step [900/17448], Loss: 7.7188\n",
      "Epoch [64/100], Step [1000/17448], Loss: 7.6719\n",
      "Epoch [64/100], Step [1100/17448], Loss: 3.1094\n",
      "Epoch [64/100], Step [1200/17448], Loss: 8.4375\n",
      "Epoch [64/100], Step [1300/17448], Loss: 9.3439\n",
      "Epoch [64/100], Step [1400/17448], Loss: 9.5781\n",
      "Epoch [64/100], Step [1500/17448], Loss: 9.1564\n",
      "Epoch [64/100], Step [1600/17448], Loss: 8.4531\n",
      "Epoch [64/100], Step [1700/17448], Loss: 7.9063\n",
      "Epoch [64/100], Step [1800/17448], Loss: 4.5157\n",
      "Epoch [64/100], Step [1900/17448], Loss: 5.7657\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [65/100], Step [100/17448], Loss: 6.5313\n",
      "Epoch [65/100], Step [200/17448], Loss: 7.4375\n",
      "Epoch [65/100], Step [300/17448], Loss: 9.1250\n",
      "Epoch [65/100], Step [400/17448], Loss: 7.7813\n",
      "Epoch [65/100], Step [500/17448], Loss: 10.8594\n",
      "Epoch [65/100], Step [600/17448], Loss: 6.6563\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [66/100], Step [100/17448], Loss: 10.5625\n",
      "Epoch [66/100], Step [200/17448], Loss: 5.7815\n",
      "Epoch [66/100], Step [300/17448], Loss: 8.5157\n",
      "Epoch [66/100], Step [400/17448], Loss: 5.5469\n",
      "Epoch [66/100], Step [500/17448], Loss: 7.2501\n",
      "Epoch [66/100], Step [600/17448], Loss: 8.8438\n",
      "Epoch [66/100], Step [700/17448], Loss: 7.7656\n",
      "Epoch [66/100], Step [800/17448], Loss: 6.4688\n",
      "Epoch [66/100], Step [900/17448], Loss: 4.9219\n",
      "Epoch [66/100], Step [1000/17448], Loss: 8.9688\n",
      "Epoch [66/100], Step [1100/17448], Loss: 8.0938\n",
      "Epoch [66/100], Step [1200/17448], Loss: 4.8125\n",
      "Epoch [66/100], Step [1300/17448], Loss: 6.3438\n",
      "Epoch [66/100], Step [1400/17448], Loss: 4.7658\n",
      "Epoch [66/100], Step [1500/17448], Loss: 12.5469\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [67/100], Step [100/17448], Loss: 4.4688\n",
      "Epoch [67/100], Step [200/17448], Loss: 9.7500\n",
      "Epoch [67/100], Step [300/17448], Loss: 7.0938\n",
      "Epoch [67/100], Step [400/17448], Loss: 4.9063\n",
      "Epoch [67/100], Step [500/17448], Loss: 6.6719\n",
      "Epoch [67/100], Step [600/17448], Loss: 7.5625\n",
      "Epoch [67/100], Step [700/17448], Loss: 7.4532\n",
      "Epoch [67/100], Step [800/17448], Loss: 4.6094\n",
      "Epoch [67/100], Step [900/17448], Loss: 8.1254\n",
      "Epoch [67/100], Step [1000/17448], Loss: 11.7813\n",
      "Epoch [67/100], Step [1100/17448], Loss: 7.7031\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [68/100], Step [100/17448], Loss: 9.0938\n",
      "Epoch [68/100], Step [200/17448], Loss: 8.0939\n",
      "Epoch [68/100], Step [300/17448], Loss: 13.7969\n",
      "Epoch [68/100], Step [400/17448], Loss: 9.1875\n",
      "Epoch [68/100], Step [500/17448], Loss: 9.7344\n",
      "Epoch [68/100], Step [600/17448], Loss: 9.8440\n",
      "Epoch [68/100], Step [700/17448], Loss: 9.4219\n",
      "Epoch [68/100], Step [800/17448], Loss: 8.0158\n",
      "Epoch [68/100], Step [900/17448], Loss: 10.2345\n",
      "Epoch [68/100], Step [1000/17448], Loss: 9.4688\n",
      "Epoch [68/100], Step [1100/17448], Loss: 5.3281\n",
      "Epoch [68/100], Step [1200/17448], Loss: 9.7813\n",
      "Epoch [68/100], Step [1300/17448], Loss: 8.8751\n",
      "Epoch [68/100], Step [1400/17448], Loss: 6.2656\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [69/100], Step [100/17448], Loss: 10.2500\n",
      "Epoch [69/100], Step [200/17448], Loss: 8.7345\n",
      "Epoch [69/100], Step [300/17448], Loss: 7.3907\n",
      "Epoch [69/100], Step [400/17448], Loss: 9.7969\n",
      "Epoch [69/100], Step [500/17448], Loss: 6.8126\n",
      "Epoch [69/100], Step [600/17448], Loss: 8.6563\n",
      "Epoch [69/100], Step [700/17448], Loss: 9.3906\n",
      "Epoch [69/100], Step [800/17448], Loss: 6.6719\n",
      "Epoch [69/100], Step [900/17448], Loss: 9.1094\n",
      "Epoch [69/100], Step [1000/17448], Loss: 6.9376\n",
      "Epoch [69/100], Step [1100/17448], Loss: 8.6563\n",
      "Epoch [69/100], Step [1200/17448], Loss: 9.4063\n",
      "Epoch [69/100], Step [1300/17448], Loss: 3.8283\n",
      "Epoch [69/100], Step [1400/17448], Loss: 4.8750\n",
      "Epoch [69/100], Step [1500/17448], Loss: 9.7345\n",
      "Epoch [69/100], Step [1600/17448], Loss: 9.4688\n",
      "Epoch [69/100], Step [1700/17448], Loss: 9.0156\n",
      "Epoch [69/100], Step [1800/17448], Loss: 6.0782\n",
      "Epoch [69/100], Step [1900/17448], Loss: 9.3126\n",
      "Epoch [69/100], Step [2000/17448], Loss: 9.6406\n",
      "Epoch [69/100], Step [2100/17448], Loss: 6.9063\n",
      "Epoch [69/100], Step [2200/17448], Loss: 4.8282\n",
      "Epoch [69/100], Step [2300/17448], Loss: 7.9219\n",
      "Epoch [69/100], Step [2400/17448], Loss: 7.3438\n",
      "Epoch [69/100], Step [2500/17448], Loss: 10.0000\n",
      "Epoch [69/100], Step [2600/17448], Loss: 7.1250\n",
      "Epoch [69/100], Step [2700/17448], Loss: 8.0001\n",
      "Epoch [69/100], Step [2800/17448], Loss: 9.1094\n",
      "Epoch [69/100], Step [2900/17448], Loss: 6.0313\n",
      "Epoch [69/100], Step [3000/17448], Loss: 9.5156\n",
      "Epoch [69/100], Step [3100/17448], Loss: 11.0001\n",
      "Epoch [69/100], Step [3200/17448], Loss: 6.0469\n",
      "Epoch [69/100], Step [3300/17448], Loss: 3.3907\n",
      "Epoch [69/100], Step [3400/17448], Loss: 6.0940\n",
      "Epoch [69/100], Step [3500/17448], Loss: 9.2031\n",
      "Epoch [69/100], Step [3600/17448], Loss: 6.8283\n",
      "Epoch [69/100], Step [3700/17448], Loss: 8.6094\n",
      "Epoch [69/100], Step [3800/17448], Loss: 7.9688\n",
      "Epoch [69/100], Step [3900/17448], Loss: 9.5784\n",
      "Epoch [69/100], Step [4000/17448], Loss: 8.4219\n",
      "Epoch [69/100], Step [4100/17448], Loss: 7.0313\n",
      "Epoch [69/100], Step [4200/17448], Loss: 4.7813\n",
      "Epoch [69/100], Step [4300/17448], Loss: 10.4688\n",
      "Epoch [69/100], Step [4400/17448], Loss: 8.2344\n",
      "Epoch [69/100], Step [4500/17448], Loss: 8.1408\n",
      "Epoch [69/100], Step [4600/17448], Loss: 10.5002\n",
      "Epoch [69/100], Step [4700/17448], Loss: 4.7657\n",
      "Epoch [69/100], Step [4800/17448], Loss: 7.5158\n",
      "Epoch [69/100], Step [4900/17448], Loss: 4.6407\n",
      "Epoch [69/100], Step [5000/17448], Loss: 5.2970\n",
      "Epoch [69/100], Step [5100/17448], Loss: 10.6095\n",
      "Epoch [69/100], Step [5200/17448], Loss: 7.6720\n",
      "Epoch [69/100], Step [5300/17448], Loss: 4.6095\n",
      "Epoch [69/100], Step [5400/17448], Loss: 5.0313\n",
      "Epoch [69/100], Step [5500/17448], Loss: 8.3125\n",
      "Epoch [69/100], Step [5600/17448], Loss: 5.4531\n",
      "Epoch [69/100], Step [5700/17448], Loss: 7.1563\n",
      "Epoch [69/100], Step [5800/17448], Loss: 4.4688\n",
      "Epoch [69/100], Step [5900/17448], Loss: 5.0471\n",
      "Epoch [69/100], Step [6000/17448], Loss: 4.4219\n",
      "Epoch [69/100], Step [6100/17448], Loss: 8.3439\n",
      "Epoch [69/100], Step [6200/17448], Loss: 4.7969\n",
      "Epoch [69/100], Step [6300/17448], Loss: 8.5625\n",
      "Epoch [69/100], Step [6400/17448], Loss: 8.5313\n",
      "Epoch [69/100], Step [6500/17448], Loss: 9.4375\n",
      "Epoch [69/100], Step [6600/17448], Loss: 8.3906\n",
      "Epoch [69/100], Step [6700/17448], Loss: 10.6250\n",
      "Epoch [69/100], Step [6800/17448], Loss: 7.4844\n",
      "Epoch [69/100], Step [6900/17448], Loss: 7.8438\n",
      "Epoch [69/100], Step [7000/17448], Loss: 9.6094\n",
      "Epoch [69/100], Step [7100/17448], Loss: 6.4375\n",
      "Epoch [69/100], Step [7200/17448], Loss: 9.5156\n",
      "Epoch [69/100], Step [7300/17448], Loss: 5.8125\n",
      "Epoch [69/100], Step [7400/17448], Loss: 6.2032\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [70/100], Step [100/17448], Loss: 6.8594\n",
      "Epoch [70/100], Step [200/17448], Loss: 7.7032\n",
      "Epoch [70/100], Step [300/17448], Loss: 6.6094\n",
      "Epoch [70/100], Step [400/17448], Loss: 8.0157\n",
      "Epoch [70/100], Step [500/17448], Loss: 6.1408\n",
      "Epoch [70/100], Step [600/17448], Loss: 6.7657\n",
      "Epoch [70/100], Step [700/17448], Loss: 7.0782\n",
      "Epoch [70/100], Step [800/17448], Loss: 8.7188\n",
      "Epoch [70/100], Step [900/17448], Loss: 9.2344\n",
      "Epoch [70/100], Step [1000/17448], Loss: 5.1406\n",
      "Epoch [70/100], Step [1100/17448], Loss: 7.5625\n",
      "Epoch [70/100], Step [1200/17448], Loss: 7.2502\n",
      "Epoch [70/100], Step [1300/17448], Loss: 8.4063\n",
      "Epoch [70/100], Step [1400/17448], Loss: 6.9063\n",
      "Epoch [70/100], Step [1500/17448], Loss: 5.8127\n",
      "Epoch [70/100], Step [1600/17448], Loss: 6.8907\n",
      "Epoch [70/100], Step [1700/17448], Loss: 7.3438\n",
      "Epoch [70/100], Step [1800/17448], Loss: 8.3594\n",
      "Epoch [70/100], Step [1900/17448], Loss: 6.4063\n",
      "Epoch [70/100], Step [2000/17448], Loss: 7.9063\n",
      "Epoch [70/100], Step [2100/17448], Loss: 7.2188\n",
      "Epoch [70/100], Step [2200/17448], Loss: 9.7344\n",
      "Epoch [70/100], Step [2300/17448], Loss: 7.0156\n",
      "Epoch [70/100], Step [2400/17448], Loss: 6.8750\n",
      "Epoch [70/100], Step [2500/17448], Loss: 5.0001\n",
      "Epoch [70/100], Step [2600/17448], Loss: 7.4064\n",
      "Epoch [70/100], Step [2700/17448], Loss: 7.5157\n",
      "Epoch [70/100], Step [2800/17448], Loss: 10.1094\n",
      "Epoch [70/100], Step [2900/17448], Loss: 9.6250\n",
      "Epoch [70/100], Step [3000/17448], Loss: 10.0313\n",
      "Epoch [70/100], Step [3100/17448], Loss: 6.1407\n",
      "Epoch [70/100], Step [3200/17448], Loss: 8.7813\n",
      "Epoch [70/100], Step [3300/17448], Loss: 7.9844\n",
      "Epoch [70/100], Step [3400/17448], Loss: 6.9219\n",
      "Epoch [70/100], Step [3500/17448], Loss: 7.9063\n",
      "Epoch [70/100], Step [3600/17448], Loss: 8.5626\n",
      "Epoch [70/100], Step [3700/17448], Loss: 5.8751\n",
      "Epoch [70/100], Step [3800/17448], Loss: 5.5313\n",
      "Epoch [70/100], Step [3900/17448], Loss: 9.6094\n",
      "Epoch [70/100], Step [4000/17448], Loss: 7.7813\n",
      "Epoch [70/100], Step [4100/17448], Loss: 7.0469\n",
      "Epoch [70/100], Step [4200/17448], Loss: 7.6563\n",
      "Epoch [70/100], Step [4300/17448], Loss: 10.7500\n",
      "Epoch [70/100], Step [4400/17448], Loss: 8.0313\n",
      "Epoch [70/100], Step [4500/17448], Loss: 7.1876\n",
      "Epoch [70/100], Step [4600/17448], Loss: 6.4376\n",
      "Epoch [70/100], Step [4700/17448], Loss: 5.4688\n",
      "Epoch [70/100], Step [4800/17448], Loss: 6.0313\n",
      "Epoch [70/100], Step [4900/17448], Loss: 9.7657\n",
      "Epoch [70/100], Step [5000/17448], Loss: 4.8438\n",
      "Epoch [70/100], Step [5100/17448], Loss: 5.2500\n",
      "Epoch [70/100], Step [5200/17448], Loss: 10.7344\n",
      "Epoch [70/100], Step [5300/17448], Loss: 5.7813\n",
      "Epoch [70/100], Step [5400/17448], Loss: 6.0001\n",
      "Epoch [70/100], Step [5500/17448], Loss: 8.3438\n",
      "Epoch [70/100], Step [5600/17448], Loss: 5.7344\n",
      "Epoch [70/100], Step [5700/17448], Loss: 13.2500\n",
      "Epoch [70/100], Step [5800/17448], Loss: 9.1095\n",
      "Epoch [70/100], Step [5900/17448], Loss: 5.3438\n",
      "Epoch [70/100], Step [6000/17448], Loss: 10.6250\n",
      "Epoch [70/100], Step [6100/17448], Loss: 7.4063\n",
      "Epoch [70/100], Step [6200/17448], Loss: 8.6094\n",
      "Epoch [70/100], Step [6300/17448], Loss: 10.7814\n",
      "Epoch [70/100], Step [6400/17448], Loss: 7.1406\n",
      "Epoch [70/100], Step [6500/17448], Loss: 7.2656\n",
      "Epoch [70/100], Step [6600/17448], Loss: 6.4532\n",
      "Epoch [70/100], Step [6700/17448], Loss: 6.6563\n",
      "Epoch [70/100], Step [6800/17448], Loss: 7.2500\n",
      "Epoch [70/100], Step [6900/17448], Loss: 7.0157\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [71/100], Step [100/17448], Loss: 7.9063\n",
      "Epoch [71/100], Step [200/17448], Loss: 9.5001\n",
      "Epoch [71/100], Step [300/17448], Loss: 6.4220\n",
      "Epoch [71/100], Step [400/17448], Loss: 5.8439\n",
      "Epoch [71/100], Step [500/17448], Loss: 5.8282\n",
      "Epoch [71/100], Step [600/17448], Loss: 9.8751\n",
      "Epoch [71/100], Step [700/17448], Loss: 9.4063\n",
      "Epoch [71/100], Step [800/17448], Loss: 9.7189\n",
      "Epoch [71/100], Step [900/17448], Loss: 4.4219\n",
      "Epoch [71/100], Step [1000/17448], Loss: 5.9219\n",
      "Epoch [71/100], Step [1100/17448], Loss: 6.3907\n",
      "Epoch [71/100], Step [1200/17448], Loss: 5.9375\n",
      "Epoch [71/100], Step [1300/17448], Loss: 6.8594\n",
      "Epoch [71/100], Step [1400/17448], Loss: 10.3750\n",
      "Epoch [71/100], Step [1500/17448], Loss: 8.0469\n",
      "Epoch [71/100], Step [1600/17448], Loss: 6.6719\n",
      "Epoch [71/100], Step [1700/17448], Loss: 5.7971\n",
      "Epoch [71/100], Step [1800/17448], Loss: 7.6094\n",
      "Epoch [71/100], Step [1900/17448], Loss: 8.0938\n",
      "Epoch [71/100], Step [2000/17448], Loss: 10.8282\n",
      "Epoch [71/100], Step [2100/17448], Loss: 13.0626\n",
      "Epoch [71/100], Step [2200/17448], Loss: 6.6719\n",
      "Epoch [71/100], Step [2300/17448], Loss: 10.1719\n",
      "Epoch [71/100], Step [2400/17448], Loss: 8.2969\n",
      "Epoch [71/100], Step [2500/17448], Loss: 10.1563\n",
      "Epoch [71/100], Step [2600/17448], Loss: 6.6563\n",
      "Epoch [71/100], Step [2700/17448], Loss: 5.9219\n",
      "Epoch [71/100], Step [2800/17448], Loss: 6.2813\n",
      "Epoch [71/100], Step [2900/17448], Loss: 11.5938\n",
      "Epoch [71/100], Step [3000/17448], Loss: 8.0470\n",
      "Epoch [71/100], Step [3100/17448], Loss: 7.1563\n",
      "Epoch [71/100], Step [3200/17448], Loss: 10.5001\n",
      "Epoch [71/100], Step [3300/17448], Loss: 11.8907\n",
      "Epoch [71/100], Step [3400/17448], Loss: 6.8906\n",
      "Epoch [71/100], Step [3500/17448], Loss: 5.5782\n",
      "Epoch [71/100], Step [3600/17448], Loss: 9.5313\n",
      "Epoch [71/100], Step [3700/17448], Loss: 7.3125\n",
      "Epoch [71/100], Step [3800/17448], Loss: 7.6563\n",
      "Epoch [71/100], Step [3900/17448], Loss: 8.6406\n",
      "Epoch [71/100], Step [4000/17448], Loss: 7.9689\n",
      "Epoch [71/100], Step [4100/17448], Loss: 6.9845\n",
      "Epoch [71/100], Step [4200/17448], Loss: 6.5314\n",
      "Epoch [71/100], Step [4300/17448], Loss: 10.0781\n",
      "Epoch [71/100], Step [4400/17448], Loss: 8.9064\n",
      "Epoch [71/100], Step [4500/17448], Loss: 7.5626\n",
      "Epoch [71/100], Step [4600/17448], Loss: 6.4376\n",
      "Epoch [71/100], Step [4700/17448], Loss: 6.5938\n",
      "Epoch [71/100], Step [4800/17448], Loss: 6.7970\n",
      "Epoch [71/100], Step [4900/17448], Loss: 7.1251\n",
      "Epoch [71/100], Step [5000/17448], Loss: 4.8907\n",
      "Epoch [71/100], Step [5100/17448], Loss: 4.5157\n",
      "Epoch [71/100], Step [5200/17448], Loss: 6.9376\n",
      "Epoch [71/100], Step [5300/17448], Loss: 11.1094\n",
      "Epoch [71/100], Step [5400/17448], Loss: 8.0156\n",
      "Epoch [71/100], Step [5500/17448], Loss: 4.7031\n",
      "Epoch [71/100], Step [5600/17448], Loss: 8.2346\n",
      "Epoch [71/100], Step [5700/17448], Loss: 9.1250\n",
      "Epoch [71/100], Step [5800/17448], Loss: 7.2501\n",
      "Epoch [71/100], Step [5900/17448], Loss: 8.8906\n",
      "Epoch [71/100], Step [6000/17448], Loss: 9.2032\n",
      "Epoch [71/100], Step [6100/17448], Loss: 9.4844\n",
      "Epoch [71/100], Step [6200/17448], Loss: 8.1875\n",
      "Epoch [71/100], Step [6300/17448], Loss: 6.0000\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [72/100], Step [100/17448], Loss: 7.0938\n",
      "Epoch [72/100], Step [200/17448], Loss: 9.6094\n",
      "Epoch [72/100], Step [300/17448], Loss: 7.4063\n",
      "Epoch [72/100], Step [400/17448], Loss: 5.7344\n",
      "Epoch [72/100], Step [500/17448], Loss: 5.5157\n",
      "Epoch [72/100], Step [600/17448], Loss: 10.7344\n",
      "Epoch [72/100], Step [700/17448], Loss: 8.6719\n",
      "Epoch [72/100], Step [800/17448], Loss: 8.9375\n",
      "Epoch [72/100], Step [900/17448], Loss: 7.2815\n",
      "Epoch [72/100], Step [1000/17448], Loss: 9.3126\n",
      "Epoch [72/100], Step [1100/17448], Loss: 6.7188\n",
      "Epoch [72/100], Step [1200/17448], Loss: 6.7813\n",
      "Epoch [72/100], Step [1300/17448], Loss: 7.1250\n",
      "Epoch [72/100], Step [1400/17448], Loss: 6.6563\n",
      "Epoch [72/100], Step [1500/17448], Loss: 3.5784\n",
      "Epoch [72/100], Step [1600/17448], Loss: 6.3750\n",
      "Epoch [72/100], Step [1700/17448], Loss: 7.2657\n",
      "Epoch [72/100], Step [1800/17448], Loss: 8.2189\n",
      "Epoch [72/100], Step [1900/17448], Loss: 9.6406\n",
      "Epoch [72/100], Step [2000/17448], Loss: 6.9063\n",
      "Epoch [72/100], Step [2100/17448], Loss: 11.7344\n",
      "Epoch [72/100], Step [2200/17448], Loss: 6.2969\n",
      "Epoch [72/100], Step [2300/17448], Loss: 7.6719\n",
      "Epoch [72/100], Step [2400/17448], Loss: 5.8907\n",
      "Epoch [72/100], Step [2500/17448], Loss: 6.6720\n",
      "Epoch [72/100], Step [2600/17448], Loss: 7.0157\n",
      "Epoch [72/100], Step [2700/17448], Loss: 8.6251\n",
      "Epoch [72/100], Step [2800/17448], Loss: 6.9220\n",
      "Epoch [72/100], Step [2900/17448], Loss: 8.3750\n",
      "Epoch [72/100], Step [3000/17448], Loss: 7.1250\n",
      "Epoch [72/100], Step [3100/17448], Loss: 9.5313\n",
      "Epoch [72/100], Step [3200/17448], Loss: 8.8438\n",
      "Epoch [72/100], Step [3300/17448], Loss: 5.2969\n",
      "Epoch [72/100], Step [3400/17448], Loss: 5.8282\n",
      "Epoch [72/100], Step [3500/17448], Loss: 9.5782\n",
      "Epoch [72/100], Step [3600/17448], Loss: 4.1719\n",
      "Epoch [72/100], Step [3700/17448], Loss: 8.3907\n",
      "Epoch [72/100], Step [3800/17448], Loss: 5.8438\n",
      "Epoch [72/100], Step [3900/17448], Loss: 8.9531\n",
      "Epoch [72/100], Step [4000/17448], Loss: 6.2031\n",
      "Epoch [72/100], Step [4100/17448], Loss: 7.9844\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [73/100], Step [100/17448], Loss: 7.9065\n",
      "Epoch [73/100], Step [200/17448], Loss: 7.4847\n",
      "Epoch [73/100], Step [300/17448], Loss: 7.9690\n",
      "Epoch [73/100], Step [400/17448], Loss: 5.2970\n",
      "Epoch [73/100], Step [500/17448], Loss: 9.2344\n",
      "Epoch [73/100], Step [600/17448], Loss: 6.2656\n",
      "Epoch [73/100], Step [700/17448], Loss: 5.3594\n",
      "Epoch [73/100], Step [800/17448], Loss: 6.7344\n",
      "Epoch [73/100], Step [900/17448], Loss: 8.4845\n",
      "Epoch [73/100], Step [1000/17448], Loss: 6.2657\n",
      "Epoch [73/100], Step [1100/17448], Loss: 8.4844\n",
      "Epoch [73/100], Step [1200/17448], Loss: 7.1407\n",
      "Epoch [73/100], Step [1300/17448], Loss: 6.6407\n",
      "Epoch [73/100], Step [1400/17448], Loss: 8.1875\n",
      "Epoch [73/100], Step [1500/17448], Loss: 6.1720\n",
      "Epoch [73/100], Step [1600/17448], Loss: 8.3594\n",
      "Epoch [73/100], Step [1700/17448], Loss: 8.0159\n",
      "Epoch [73/100], Step [1800/17448], Loss: 8.1875\n",
      "Epoch [73/100], Step [1900/17448], Loss: 8.5938\n",
      "Epoch [73/100], Step [2000/17448], Loss: 7.9844\n",
      "Epoch [73/100], Step [2100/17448], Loss: 12.0313\n",
      "Epoch [73/100], Step [2200/17448], Loss: 7.3750\n",
      "Epoch [73/100], Step [2300/17448], Loss: 8.8282\n",
      "Epoch [73/100], Step [2400/17448], Loss: 7.1719\n",
      "Epoch [73/100], Step [2500/17448], Loss: 6.0001\n",
      "Epoch [73/100], Step [2600/17448], Loss: 7.3283\n",
      "Epoch [73/100], Step [2700/17448], Loss: 6.2500\n",
      "Epoch [73/100], Step [2800/17448], Loss: 7.7501\n",
      "Epoch [73/100], Step [2900/17448], Loss: 5.2502\n",
      "Epoch [73/100], Step [3000/17448], Loss: 10.5313\n",
      "Epoch [73/100], Step [3100/17448], Loss: 6.6250\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [74/100], Step [100/17448], Loss: 8.1719\n",
      "Epoch [74/100], Step [200/17448], Loss: 8.3126\n",
      "Epoch [74/100], Step [300/17448], Loss: 5.7656\n",
      "Epoch [74/100], Step [400/17448], Loss: 8.5000\n",
      "Epoch [74/100], Step [500/17448], Loss: 5.3282\n",
      "Epoch [74/100], Step [600/17448], Loss: 6.6406\n",
      "Epoch [74/100], Step [700/17448], Loss: 7.0313\n",
      "Epoch [74/100], Step [800/17448], Loss: 7.0782\n",
      "Epoch [74/100], Step [900/17448], Loss: 5.8906\n",
      "Epoch [74/100], Step [1000/17448], Loss: 6.2656\n",
      "Epoch [74/100], Step [1100/17448], Loss: 9.2188\n",
      "Epoch [74/100], Step [1200/17448], Loss: 8.0782\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [75/100], Step [100/17448], Loss: 7.5938\n",
      "Epoch [75/100], Step [200/17448], Loss: 6.7813\n",
      "Epoch [75/100], Step [300/17448], Loss: 8.1406\n",
      "Epoch [75/100], Step [400/17448], Loss: 6.7969\n",
      "Epoch [75/100], Step [500/17448], Loss: 6.7970\n",
      "Epoch [75/100], Step [600/17448], Loss: 8.9220\n",
      "Epoch [75/100], Step [700/17448], Loss: 9.9063\n",
      "Epoch [75/100], Step [800/17448], Loss: 9.2501\n",
      "Epoch [75/100], Step [900/17448], Loss: 6.9844\n",
      "Epoch [75/100], Step [1000/17448], Loss: 6.2033\n",
      "Epoch [75/100], Step [1100/17448], Loss: 7.7656\n",
      "Epoch [75/100], Step [1200/17448], Loss: 7.5313\n",
      "Epoch [75/100], Step [1300/17448], Loss: 5.0469\n",
      "Epoch [75/100], Step [1400/17448], Loss: 7.1094\n",
      "Epoch [75/100], Step [1500/17448], Loss: 6.8594\n",
      "Epoch [75/100], Step [1600/17448], Loss: 5.4532\n",
      "Epoch [75/100], Step [1700/17448], Loss: 7.5157\n",
      "Epoch [75/100], Step [1800/17448], Loss: 9.9844\n",
      "Epoch [75/100], Step [1900/17448], Loss: 8.0157\n",
      "Epoch [75/100], Step [2000/17448], Loss: 10.0001\n",
      "Epoch [75/100], Step [2100/17448], Loss: 9.6094\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [76/100], Step [100/17448], Loss: 7.7656\n",
      "Epoch [76/100], Step [200/17448], Loss: 9.6719\n",
      "Epoch [76/100], Step [300/17448], Loss: 5.3907\n",
      "Epoch [76/100], Step [400/17448], Loss: 8.9531\n",
      "Epoch [76/100], Step [500/17448], Loss: 4.9844\n",
      "Epoch [76/100], Step [600/17448], Loss: 8.6250\n",
      "Epoch [76/100], Step [700/17448], Loss: 6.6564\n",
      "Epoch [76/100], Step [800/17448], Loss: 7.3750\n",
      "Epoch [76/100], Step [900/17448], Loss: 10.7813\n",
      "Epoch [76/100], Step [1000/17448], Loss: 7.5781\n",
      "Epoch [76/100], Step [1100/17448], Loss: 6.4063\n",
      "Epoch [76/100], Step [1200/17448], Loss: 9.4063\n",
      "Epoch [76/100], Step [1300/17448], Loss: 10.9688\n",
      "Epoch [76/100], Step [1400/17448], Loss: 5.1408\n",
      "Epoch [76/100], Step [1500/17448], Loss: 6.3750\n",
      "Epoch [76/100], Step [1600/17448], Loss: 10.7188\n",
      "Epoch [76/100], Step [1700/17448], Loss: 5.6563\n",
      "Epoch [76/100], Step [1800/17448], Loss: 12.8438\n",
      "Epoch [76/100], Step [1900/17448], Loss: 6.0781\n",
      "Epoch [76/100], Step [2000/17448], Loss: 11.1095\n",
      "Epoch [76/100], Step [2100/17448], Loss: 5.7658\n",
      "Epoch [76/100], Step [2200/17448], Loss: 8.1094\n",
      "Epoch [76/100], Step [2300/17448], Loss: 11.2501\n",
      "Epoch [76/100], Step [2400/17448], Loss: 7.0000\n",
      "Epoch [76/100], Step [2500/17448], Loss: 5.3907\n",
      "Epoch [76/100], Step [2600/17448], Loss: 6.7344\n",
      "Epoch [76/100], Step [2700/17448], Loss: 7.6875\n",
      "Epoch [76/100], Step [2800/17448], Loss: 9.8126\n",
      "Epoch [76/100], Step [2900/17448], Loss: 8.4688\n",
      "Epoch [76/100], Step [3000/17448], Loss: 10.0940\n",
      "Epoch [76/100], Step [3100/17448], Loss: 9.7346\n",
      "Epoch [76/100], Step [3200/17448], Loss: 4.7500\n",
      "Epoch [76/100], Step [3300/17448], Loss: 10.0156\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [77/100], Step [100/17448], Loss: 6.3594\n",
      "Epoch [77/100], Step [200/17448], Loss: 5.3125\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [78/100], Step [100/17448], Loss: 8.3596\n",
      "Epoch [78/100], Step [200/17448], Loss: 8.0156\n",
      "Epoch [78/100], Step [300/17448], Loss: 8.3281\n",
      "Epoch [78/100], Step [400/17448], Loss: 6.9688\n",
      "Epoch [78/100], Step [500/17448], Loss: 8.0938\n",
      "Epoch [78/100], Step [600/17448], Loss: 8.0781\n",
      "Epoch [78/100], Step [700/17448], Loss: 5.4531\n",
      "Epoch [78/100], Step [800/17448], Loss: 8.2345\n",
      "Epoch [78/100], Step [900/17448], Loss: 5.8909\n",
      "Epoch [78/100], Step [1000/17448], Loss: 7.7969\n",
      "Epoch [78/100], Step [1100/17448], Loss: 7.5625\n",
      "Epoch [78/100], Step [1200/17448], Loss: 9.3281\n",
      "Epoch [78/100], Step [1300/17448], Loss: 5.0157\n",
      "Epoch [78/100], Step [1400/17448], Loss: 8.3594\n",
      "Epoch [78/100], Step [1500/17448], Loss: 6.7657\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [79/100], Step [100/17448], Loss: 7.1563\n",
      "Epoch [79/100], Step [200/17448], Loss: 6.3282\n",
      "Epoch [79/100], Step [300/17448], Loss: 5.1251\n",
      "Epoch [79/100], Step [400/17448], Loss: 5.3126\n",
      "Epoch [79/100], Step [500/17448], Loss: 5.4532\n",
      "Epoch [79/100], Step [600/17448], Loss: 5.2501\n",
      "Epoch [79/100], Step [700/17448], Loss: 8.4688\n",
      "Epoch [79/100], Step [800/17448], Loss: 11.8750\n",
      "Epoch [79/100], Step [900/17448], Loss: 4.9375\n",
      "Epoch [79/100], Step [1000/17448], Loss: 7.1407\n",
      "Epoch [79/100], Step [1100/17448], Loss: 6.3751\n",
      "Epoch [79/100], Step [1200/17448], Loss: 8.0157\n",
      "Epoch [79/100], Step [1300/17448], Loss: 8.9219\n",
      "Epoch [79/100], Step [1400/17448], Loss: 9.8751\n",
      "Epoch [79/100], Step [1500/17448], Loss: 9.0313\n",
      "Epoch [79/100], Step [1600/17448], Loss: 4.8907\n",
      "Epoch [79/100], Step [1700/17448], Loss: 9.0469\n",
      "Epoch [79/100], Step [1800/17448], Loss: 6.2188\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [80/100], Step [100/17448], Loss: 7.4688\n",
      "Epoch [80/100], Step [200/17448], Loss: 6.1719\n",
      "Epoch [80/100], Step [300/17448], Loss: 6.0625\n",
      "Epoch [80/100], Step [400/17448], Loss: 10.5938\n",
      "Epoch [80/100], Step [500/17448], Loss: 7.1094\n",
      "Epoch [80/100], Step [600/17448], Loss: 5.8126\n",
      "Epoch [80/100], Step [700/17448], Loss: 6.4219\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [81/100], Step [100/17448], Loss: 8.2969\n",
      "Epoch [81/100], Step [200/17448], Loss: 6.1563\n",
      "Epoch [81/100], Step [300/17448], Loss: 6.9376\n",
      "Epoch [81/100], Step [400/17448], Loss: 7.6096\n",
      "Epoch [81/100], Step [500/17448], Loss: 8.9845\n",
      "Epoch [81/100], Step [600/17448], Loss: 5.3282\n",
      "Epoch [81/100], Step [700/17448], Loss: 7.3281\n",
      "Epoch [81/100], Step [800/17448], Loss: 8.2344\n",
      "Epoch [81/100], Step [900/17448], Loss: 8.2813\n",
      "Epoch [81/100], Step [1000/17448], Loss: 9.9376\n",
      "Epoch [81/100], Step [1100/17448], Loss: 8.0625\n",
      "Epoch [81/100], Step [1200/17448], Loss: 4.5314\n",
      "Epoch [81/100], Step [1300/17448], Loss: 9.0627\n",
      "Epoch [81/100], Step [1400/17448], Loss: 7.6719\n",
      "Epoch [81/100], Step [1500/17448], Loss: 7.8125\n",
      "Epoch [81/100], Step [1600/17448], Loss: 7.1876\n",
      "Epoch [81/100], Step [1700/17448], Loss: 6.9219\n",
      "Epoch [81/100], Step [1800/17448], Loss: 5.7345\n",
      "Epoch [81/100], Step [1900/17448], Loss: 7.3126\n",
      "Epoch [81/100], Step [2000/17448], Loss: 8.2188\n",
      "Epoch [81/100], Step [2100/17448], Loss: 10.7969\n",
      "Epoch [81/100], Step [2200/17448], Loss: 6.7344\n",
      "Epoch [81/100], Step [2300/17448], Loss: 8.7969\n",
      "Epoch [81/100], Step [2400/17448], Loss: 8.3282\n",
      "Epoch [81/100], Step [2500/17448], Loss: 8.0781\n",
      "Epoch [81/100], Step [2600/17448], Loss: 10.3125\n",
      "Epoch [81/100], Step [2700/17448], Loss: 6.7032\n",
      "Epoch [81/100], Step [2800/17448], Loss: 7.1250\n",
      "Epoch [81/100], Step [2900/17448], Loss: 7.2500\n",
      "Epoch [81/100], Step [3000/17448], Loss: 4.6095\n",
      "Epoch [81/100], Step [3100/17448], Loss: 6.4531\n",
      "Epoch [81/100], Step [3200/17448], Loss: 6.6250\n",
      "Epoch [81/100], Step [3300/17448], Loss: 7.7345\n",
      "Epoch [81/100], Step [3400/17448], Loss: 9.1719\n",
      "Epoch [81/100], Step [3500/17448], Loss: 7.7188\n",
      "Epoch [81/100], Step [3600/17448], Loss: 6.4063\n",
      "Epoch [81/100], Step [3700/17448], Loss: 7.7813\n",
      "Epoch [81/100], Step [3800/17448], Loss: 6.1250\n",
      "Epoch [81/100], Step [3900/17448], Loss: 8.6407\n",
      "Epoch [81/100], Step [4000/17448], Loss: 7.5625\n",
      "Epoch [81/100], Step [4100/17448], Loss: 6.0158\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [82/100], Step [100/17448], Loss: 8.5156\n",
      "Epoch [82/100], Step [200/17448], Loss: 4.6094\n",
      "Epoch [82/100], Step [300/17448], Loss: 8.4688\n",
      "Epoch [82/100], Step [400/17448], Loss: 12.7969\n",
      "Epoch [82/100], Step [500/17448], Loss: 8.8907\n",
      "Epoch [82/100], Step [600/17448], Loss: 5.2344\n",
      "Epoch [82/100], Step [700/17448], Loss: 7.9375\n",
      "Epoch [82/100], Step [800/17448], Loss: 9.8282\n",
      "Epoch [82/100], Step [900/17448], Loss: 5.6407\n",
      "Epoch [82/100], Step [1000/17448], Loss: 5.2658\n",
      "Epoch [82/100], Step [1100/17448], Loss: 8.5625\n",
      "Epoch [82/100], Step [1200/17448], Loss: 8.5000\n",
      "Epoch [82/100], Step [1300/17448], Loss: 7.3751\n",
      "Epoch [82/100], Step [1400/17448], Loss: 6.9844\n",
      "Epoch [82/100], Step [1500/17448], Loss: 6.8750\n",
      "Epoch [82/100], Step [1600/17448], Loss: 8.0001\n",
      "Epoch [82/100], Step [1700/17448], Loss: 7.7188\n",
      "Epoch [82/100], Step [1800/17448], Loss: 11.1250\n",
      "Epoch [82/100], Step [1900/17448], Loss: 7.7656\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [83/100], Step [100/17448], Loss: 6.9689\n",
      "Epoch [83/100], Step [200/17448], Loss: 7.5001\n",
      "Epoch [83/100], Step [300/17448], Loss: 7.2032\n",
      "Epoch [83/100], Step [400/17448], Loss: 6.0156\n",
      "Epoch [83/100], Step [500/17448], Loss: 11.0158\n",
      "Epoch [83/100], Step [600/17448], Loss: 8.2031\n",
      "Epoch [83/100], Step [700/17448], Loss: 7.8438\n",
      "Epoch [83/100], Step [800/17448], Loss: 9.0937\n",
      "Epoch [83/100], Step [900/17448], Loss: 6.3752\n",
      "Epoch [83/100], Step [1000/17448], Loss: 6.2031\n",
      "Epoch [83/100], Step [1100/17448], Loss: 7.6250\n",
      "Epoch [83/100], Step [1200/17448], Loss: 6.3282\n",
      "Epoch [83/100], Step [1300/17448], Loss: 6.4845\n",
      "Epoch [83/100], Step [1400/17448], Loss: 7.6094\n",
      "Epoch [83/100], Step [1500/17448], Loss: 5.8126\n",
      "Epoch [83/100], Step [1600/17448], Loss: 4.9531\n",
      "Epoch [83/100], Step [1700/17448], Loss: 5.6252\n",
      "Epoch [83/100], Step [1800/17448], Loss: 8.5469\n",
      "Epoch [83/100], Step [1900/17448], Loss: 6.5938\n",
      "Epoch [83/100], Step [2000/17448], Loss: 7.5939\n",
      "Epoch [83/100], Step [2100/17448], Loss: 6.9533\n",
      "Epoch [83/100], Step [2200/17448], Loss: 7.9688\n",
      "Epoch [83/100], Step [2300/17448], Loss: 7.1875\n",
      "Epoch [83/100], Step [2400/17448], Loss: 6.6094\n",
      "Epoch [83/100], Step [2500/17448], Loss: 7.9688\n",
      "Epoch [83/100], Step [2600/17448], Loss: 10.0157\n",
      "Epoch [83/100], Step [2700/17448], Loss: 9.4220\n",
      "Epoch [83/100], Step [2800/17448], Loss: 4.7969\n",
      "Epoch [83/100], Step [2900/17448], Loss: 7.8906\n",
      "Epoch [83/100], Step [3000/17448], Loss: 12.0313\n",
      "Epoch [83/100], Step [3100/17448], Loss: 7.0313\n",
      "Epoch [83/100], Step [3200/17448], Loss: 9.2969\n",
      "Epoch [83/100], Step [3300/17448], Loss: 8.6094\n",
      "Epoch [83/100], Step [3400/17448], Loss: 9.2969\n",
      "Epoch [83/100], Step [3500/17448], Loss: 13.1407\n",
      "Epoch [83/100], Step [3600/17448], Loss: 6.2656\n",
      "Epoch [83/100], Step [3700/17448], Loss: 4.9064\n",
      "Epoch [83/100], Step [3800/17448], Loss: 6.1407\n",
      "Epoch [83/100], Step [3900/17448], Loss: 10.3282\n",
      "Epoch [83/100], Step [4000/17448], Loss: 6.9220\n",
      "Epoch [83/100], Step [4100/17448], Loss: 10.7344\n",
      "Epoch [83/100], Step [4200/17448], Loss: 8.4844\n",
      "Epoch [83/100], Step [4300/17448], Loss: 7.4063\n",
      "Epoch [83/100], Step [4400/17448], Loss: 8.2032\n",
      "Epoch [83/100], Step [4500/17448], Loss: 6.8438\n",
      "Epoch [83/100], Step [4600/17448], Loss: 10.0625\n",
      "Epoch [83/100], Step [4700/17448], Loss: 13.8438\n",
      "Epoch [83/100], Step [4800/17448], Loss: 6.5938\n",
      "Epoch [83/100], Step [4900/17448], Loss: 6.9220\n",
      "Epoch [83/100], Step [5000/17448], Loss: 7.6875\n",
      "Epoch [83/100], Step [5100/17448], Loss: 7.9375\n",
      "Epoch [83/100], Step [5200/17448], Loss: 8.9219\n",
      "Epoch [83/100], Step [5300/17448], Loss: 6.8126\n",
      "Epoch [83/100], Step [5400/17448], Loss: 9.1876\n",
      "Epoch [83/100], Step [5500/17448], Loss: 7.3906\n",
      "Epoch [83/100], Step [5600/17448], Loss: 4.2032\n",
      "Epoch [83/100], Step [5700/17448], Loss: 7.3282\n",
      "Epoch [83/100], Step [5800/17448], Loss: 10.3126\n",
      "Epoch [83/100], Step [5900/17448], Loss: 7.4532\n",
      "Epoch [83/100], Step [6000/17448], Loss: 6.6094\n",
      "Epoch [83/100], Step [6100/17448], Loss: 10.4063\n",
      "Epoch [83/100], Step [6200/17448], Loss: 7.6094\n",
      "Epoch [83/100], Step [6300/17448], Loss: 5.9375\n",
      "Epoch [83/100], Step [6400/17448], Loss: 9.1875\n",
      "Epoch [83/100], Step [6500/17448], Loss: 7.4063\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [84/100], Step [100/17448], Loss: 11.0468\n",
      "Epoch [84/100], Step [200/17448], Loss: 9.0469\n",
      "Epoch [84/100], Step [300/17448], Loss: 7.3125\n",
      "Epoch [84/100], Step [400/17448], Loss: 6.8750\n",
      "Epoch [84/100], Step [500/17448], Loss: 7.9376\n",
      "Epoch [84/100], Step [600/17448], Loss: 7.7500\n",
      "Epoch [84/100], Step [700/17448], Loss: 8.6875\n",
      "Epoch [84/100], Step [800/17448], Loss: 5.9375\n",
      "Epoch [84/100], Step [900/17448], Loss: 9.6250\n",
      "Epoch [84/100], Step [1000/17448], Loss: 7.9844\n",
      "Epoch [84/100], Step [1100/17448], Loss: 10.4375\n",
      "Epoch [84/100], Step [1200/17448], Loss: 8.7813\n",
      "Epoch [84/100], Step [1300/17448], Loss: 6.8126\n",
      "Epoch [84/100], Step [1400/17448], Loss: 5.6407\n",
      "Epoch [84/100], Step [1500/17448], Loss: 5.1563\n",
      "Epoch [84/100], Step [1600/17448], Loss: 8.6875\n",
      "Epoch [84/100], Step [1700/17448], Loss: 11.9532\n",
      "Epoch [84/100], Step [1800/17448], Loss: 7.2501\n",
      "Epoch [84/100], Step [1900/17448], Loss: 8.3595\n",
      "Epoch [84/100], Step [2000/17448], Loss: 4.7032\n",
      "Epoch [84/100], Step [2100/17448], Loss: 6.9844\n",
      "Epoch [84/100], Step [2200/17448], Loss: 10.0313\n",
      "Epoch [84/100], Step [2300/17448], Loss: 10.2344\n",
      "Epoch [84/100], Step [2400/17448], Loss: 7.1250\n",
      "Epoch [84/100], Step [2500/17448], Loss: 5.8751\n",
      "Epoch [84/100], Step [2600/17448], Loss: 9.1408\n",
      "Epoch [84/100], Step [2700/17448], Loss: 8.0471\n",
      "Epoch [84/100], Step [2800/17448], Loss: 6.0782\n",
      "Epoch [84/100], Step [2900/17448], Loss: 7.7814\n",
      "Epoch [84/100], Step [3000/17448], Loss: 5.5000\n",
      "Epoch [84/100], Step [3100/17448], Loss: 11.1563\n",
      "Epoch [84/100], Step [3200/17448], Loss: 9.2813\n",
      "Epoch [84/100], Step [3300/17448], Loss: 8.5938\n",
      "Epoch [84/100], Step [3400/17448], Loss: 6.2970\n",
      "Epoch [84/100], Step [3500/17448], Loss: 6.6251\n",
      "Epoch [84/100], Step [3600/17448], Loss: 8.6250\n",
      "Epoch [84/100], Step [3700/17448], Loss: 8.2500\n",
      "Epoch [84/100], Step [3800/17448], Loss: 7.4532\n",
      "Epoch [84/100], Step [3900/17448], Loss: 4.4532\n",
      "Epoch [84/100], Step [4000/17448], Loss: 8.1094\n",
      "Epoch [84/100], Step [4100/17448], Loss: 8.5938\n",
      "Epoch [84/100], Step [4200/17448], Loss: 8.3908\n",
      "Epoch [84/100], Step [4300/17448], Loss: 6.8908\n",
      "Epoch [84/100], Step [4400/17448], Loss: 6.6875\n",
      "Epoch [84/100], Step [4500/17448], Loss: 8.8594\n",
      "Epoch [84/100], Step [4600/17448], Loss: 7.1250\n",
      "Epoch [84/100], Step [4700/17448], Loss: 12.7500\n",
      "Epoch [84/100], Step [4800/17448], Loss: 7.2188\n",
      "Epoch [84/100], Step [4900/17448], Loss: 6.2969\n",
      "Epoch [84/100], Step [5000/17448], Loss: 7.9219\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [85/100], Step [100/17448], Loss: 9.1407\n",
      "Epoch [85/100], Step [200/17448], Loss: 9.0001\n",
      "Epoch [85/100], Step [300/17448], Loss: 8.0469\n",
      "Epoch [85/100], Step [400/17448], Loss: 6.5313\n",
      "Epoch [85/100], Step [500/17448], Loss: 12.0000\n",
      "Epoch [85/100], Step [600/17448], Loss: 6.5938\n",
      "Epoch [85/100], Step [700/17448], Loss: 4.9063\n",
      "Epoch [85/100], Step [800/17448], Loss: 6.3594\n",
      "Epoch [85/100], Step [900/17448], Loss: 6.7969\n",
      "Epoch [85/100], Step [1000/17448], Loss: 6.7032\n",
      "Epoch [85/100], Step [1100/17448], Loss: 9.8126\n",
      "Epoch [85/100], Step [1200/17448], Loss: 8.0469\n",
      "Epoch [85/100], Step [1300/17448], Loss: 10.5157\n",
      "Epoch [85/100], Step [1400/17448], Loss: 8.7970\n",
      "Epoch [85/100], Step [1500/17448], Loss: 7.1875\n",
      "Epoch [85/100], Step [1600/17448], Loss: 8.9531\n",
      "Epoch [85/100], Step [1700/17448], Loss: 8.5313\n",
      "Epoch [85/100], Step [1800/17448], Loss: 8.7191\n",
      "Epoch [85/100], Step [1900/17448], Loss: 5.9065\n",
      "Epoch [85/100], Step [2000/17448], Loss: 6.2031\n",
      "Epoch [85/100], Step [2100/17448], Loss: 7.7188\n",
      "Epoch [85/100], Step [2200/17448], Loss: 6.9531\n",
      "Epoch [85/100], Step [2300/17448], Loss: 9.2344\n",
      "Epoch [85/100], Step [2400/17448], Loss: 9.3438\n",
      "Epoch [85/100], Step [2500/17448], Loss: 4.3282\n",
      "Epoch [85/100], Step [2600/17448], Loss: 6.0157\n",
      "Epoch [85/100], Step [2700/17448], Loss: 9.1719\n",
      "Epoch [85/100], Step [2800/17448], Loss: 9.8281\n",
      "Epoch [85/100], Step [2900/17448], Loss: 8.1877\n",
      "Epoch [85/100], Step [3000/17448], Loss: 9.8438\n",
      "Epoch [85/100], Step [3100/17448], Loss: 7.6407\n",
      "Epoch [85/100], Step [3200/17448], Loss: 6.2969\n",
      "Epoch [85/100], Step [3300/17448], Loss: 7.8594\n",
      "Epoch [85/100], Step [3400/17448], Loss: 10.9219\n",
      "Epoch [85/100], Step [3500/17448], Loss: 9.8282\n",
      "Epoch [85/100], Step [3600/17448], Loss: 8.4531\n",
      "NaN loss encountered. Exiting training loop.\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [87/100], Step [100/17448], Loss: 9.4844\n",
      "Epoch [87/100], Step [200/17448], Loss: 10.0938\n",
      "Epoch [87/100], Step [300/17448], Loss: 9.6876\n",
      "Epoch [87/100], Step [400/17448], Loss: 4.0938\n",
      "Epoch [87/100], Step [500/17448], Loss: 8.0938\n",
      "Epoch [87/100], Step [600/17448], Loss: 8.4532\n",
      "Epoch [87/100], Step [700/17448], Loss: 9.0313\n",
      "Epoch [87/100], Step [800/17448], Loss: 6.0313\n",
      "Epoch [87/100], Step [900/17448], Loss: 7.8282\n",
      "Epoch [87/100], Step [1000/17448], Loss: 6.5939\n",
      "Epoch [87/100], Step [1100/17448], Loss: 4.8594\n",
      "Epoch [87/100], Step [1200/17448], Loss: 6.1563\n",
      "Epoch [87/100], Step [1300/17448], Loss: 6.3125\n",
      "Epoch [87/100], Step [1400/17448], Loss: 6.1250\n",
      "Epoch [87/100], Step [1500/17448], Loss: 8.6250\n",
      "Epoch [87/100], Step [1600/17448], Loss: 7.9219\n",
      "Epoch [87/100], Step [1700/17448], Loss: 6.2188\n",
      "Epoch [87/100], Step [1800/17448], Loss: 8.1407\n",
      "Epoch [87/100], Step [1900/17448], Loss: 6.1094\n",
      "Epoch [87/100], Step [2000/17448], Loss: 6.7033\n",
      "Epoch [87/100], Step [2100/17448], Loss: 6.1407\n",
      "Epoch [87/100], Step [2200/17448], Loss: 8.7031\n",
      "Epoch [87/100], Step [2300/17448], Loss: 7.7033\n",
      "Epoch [87/100], Step [2400/17448], Loss: 6.2503\n",
      "Epoch [87/100], Step [2500/17448], Loss: 6.3751\n",
      "Epoch [87/100], Step [2600/17448], Loss: 7.1563\n",
      "Epoch [87/100], Step [2700/17448], Loss: 8.0000\n",
      "Epoch [87/100], Step [2800/17448], Loss: 8.8751\n",
      "Epoch [87/100], Step [2900/17448], Loss: 6.3595\n",
      "Epoch [87/100], Step [3000/17448], Loss: 8.6251\n",
      "Epoch [87/100], Step [3100/17448], Loss: 9.7813\n",
      "Epoch [87/100], Step [3200/17448], Loss: 9.6562\n",
      "Epoch [87/100], Step [3300/17448], Loss: 5.4845\n",
      "Epoch [87/100], Step [3400/17448], Loss: 10.4220\n",
      "Epoch [87/100], Step [3500/17448], Loss: 6.8126\n",
      "Epoch [87/100], Step [3600/17448], Loss: 7.4219\n",
      "Epoch [87/100], Step [3700/17448], Loss: 6.6406\n",
      "Epoch [87/100], Step [3800/17448], Loss: 5.3751\n",
      "Epoch [87/100], Step [3900/17448], Loss: 8.6875\n",
      "Epoch [87/100], Step [4000/17448], Loss: 10.6563\n",
      "Epoch [87/100], Step [4100/17448], Loss: 6.2970\n",
      "Epoch [87/100], Step [4200/17448], Loss: 11.4844\n",
      "Epoch [87/100], Step [4300/17448], Loss: 8.5938\n",
      "Epoch [87/100], Step [4400/17448], Loss: 7.1719\n",
      "Epoch [87/100], Step [4500/17448], Loss: 7.2501\n",
      "Epoch [87/100], Step [4600/17448], Loss: 8.0470\n",
      "Epoch [87/100], Step [4700/17448], Loss: 9.6875\n",
      "Epoch [87/100], Step [4800/17448], Loss: 8.7188\n",
      "Epoch [87/100], Step [4900/17448], Loss: 4.8594\n",
      "Epoch [87/100], Step [5000/17448], Loss: 5.6563\n",
      "Epoch [87/100], Step [5100/17448], Loss: 7.3438\n",
      "Epoch [87/100], Step [5200/17448], Loss: 8.2500\n",
      "Epoch [87/100], Step [5300/17448], Loss: 7.5157\n",
      "Epoch [87/100], Step [5400/17448], Loss: 8.8594\n",
      "Epoch [87/100], Step [5500/17448], Loss: 7.6094\n",
      "Epoch [87/100], Step [5600/17448], Loss: 8.3907\n",
      "Epoch [87/100], Step [5700/17448], Loss: 7.5782\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [88/100], Step [100/17448], Loss: 9.6719\n",
      "Epoch [88/100], Step [200/17448], Loss: 7.6563\n",
      "Epoch [88/100], Step [300/17448], Loss: 8.5782\n",
      "Epoch [88/100], Step [400/17448], Loss: 8.8127\n",
      "Epoch [88/100], Step [500/17448], Loss: 10.2032\n",
      "Epoch [88/100], Step [600/17448], Loss: 5.0469\n",
      "Epoch [88/100], Step [700/17448], Loss: 9.2188\n",
      "Epoch [88/100], Step [800/17448], Loss: 8.1406\n",
      "Epoch [88/100], Step [900/17448], Loss: 9.2345\n",
      "Epoch [88/100], Step [1000/17448], Loss: 6.4375\n",
      "Epoch [88/100], Step [1100/17448], Loss: 6.1719\n",
      "Epoch [88/100], Step [1200/17448], Loss: 8.9844\n",
      "Epoch [88/100], Step [1300/17448], Loss: 10.1563\n",
      "Epoch [88/100], Step [1400/17448], Loss: 7.3438\n",
      "Epoch [88/100], Step [1500/17448], Loss: 4.7813\n",
      "Epoch [88/100], Step [1600/17448], Loss: 5.8438\n",
      "Epoch [88/100], Step [1700/17448], Loss: 7.6252\n",
      "Epoch [88/100], Step [1800/17448], Loss: 8.2813\n",
      "Epoch [88/100], Step [1900/17448], Loss: 4.9219\n",
      "Epoch [88/100], Step [2000/17448], Loss: 7.0315\n",
      "Epoch [88/100], Step [2100/17448], Loss: 8.8125\n",
      "Epoch [88/100], Step [2200/17448], Loss: 7.5939\n",
      "Epoch [88/100], Step [2300/17448], Loss: 7.4844\n",
      "Epoch [88/100], Step [2400/17448], Loss: 8.0314\n",
      "Epoch [88/100], Step [2500/17448], Loss: 9.0313\n",
      "Epoch [88/100], Step [2600/17448], Loss: 5.6251\n",
      "Epoch [88/100], Step [2700/17448], Loss: 6.3596\n",
      "Epoch [88/100], Step [2800/17448], Loss: 8.7188\n",
      "Epoch [88/100], Step [2900/17448], Loss: 10.2656\n",
      "Epoch [88/100], Step [3000/17448], Loss: 7.2188\n",
      "Epoch [88/100], Step [3100/17448], Loss: 7.8594\n",
      "Epoch [88/100], Step [3200/17448], Loss: 9.8438\n",
      "Epoch [88/100], Step [3300/17448], Loss: 7.4375\n",
      "Epoch [88/100], Step [3400/17448], Loss: 9.2188\n",
      "Epoch [88/100], Step [3500/17448], Loss: 7.1406\n",
      "Epoch [88/100], Step [3600/17448], Loss: 6.8594\n",
      "Epoch [88/100], Step [3700/17448], Loss: 6.3907\n",
      "Epoch [88/100], Step [3800/17448], Loss: 7.5313\n",
      "Epoch [88/100], Step [3900/17448], Loss: 7.0625\n",
      "Epoch [88/100], Step [4000/17448], Loss: 6.1719\n",
      "Epoch [88/100], Step [4100/17448], Loss: 7.9063\n",
      "Epoch [88/100], Step [4200/17448], Loss: 5.2813\n",
      "Epoch [88/100], Step [4300/17448], Loss: 5.7032\n",
      "Epoch [88/100], Step [4400/17448], Loss: 7.1563\n",
      "Epoch [88/100], Step [4500/17448], Loss: 7.6094\n",
      "Epoch [88/100], Step [4600/17448], Loss: 6.0156\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [89/100], Step [100/17448], Loss: 7.1407\n",
      "Epoch [89/100], Step [200/17448], Loss: 7.9065\n",
      "Epoch [89/100], Step [300/17448], Loss: 10.0313\n",
      "Epoch [89/100], Step [400/17448], Loss: 5.5938\n",
      "Epoch [89/100], Step [500/17448], Loss: 5.1876\n",
      "Epoch [89/100], Step [600/17448], Loss: 5.6407\n",
      "Epoch [89/100], Step [700/17448], Loss: 3.8126\n",
      "Epoch [89/100], Step [800/17448], Loss: 4.5157\n",
      "Epoch [89/100], Step [900/17448], Loss: 5.2501\n",
      "Epoch [89/100], Step [1000/17448], Loss: 5.7032\n",
      "Epoch [89/100], Step [1100/17448], Loss: 8.2969\n",
      "Epoch [89/100], Step [1200/17448], Loss: 7.8906\n",
      "Epoch [89/100], Step [1300/17448], Loss: 8.8282\n",
      "Epoch [89/100], Step [1400/17448], Loss: 7.8594\n",
      "Epoch [89/100], Step [1500/17448], Loss: 6.0472\n",
      "Epoch [89/100], Step [1600/17448], Loss: 9.7969\n",
      "Epoch [89/100], Step [1700/17448], Loss: 8.2813\n",
      "Epoch [89/100], Step [1800/17448], Loss: 6.9220\n",
      "Epoch [89/100], Step [1900/17448], Loss: 9.8907\n",
      "Epoch [89/100], Step [2000/17448], Loss: 7.2188\n",
      "Epoch [89/100], Step [2100/17448], Loss: 6.6250\n",
      "Epoch [89/100], Step [2200/17448], Loss: 6.8282\n",
      "Epoch [89/100], Step [2300/17448], Loss: 8.8126\n",
      "Epoch [89/100], Step [2400/17448], Loss: 8.7813\n",
      "Epoch [89/100], Step [2500/17448], Loss: 8.1094\n",
      "Epoch [89/100], Step [2600/17448], Loss: 8.1564\n",
      "Epoch [89/100], Step [2700/17448], Loss: 7.9688\n",
      "Epoch [89/100], Step [2800/17448], Loss: 5.4375\n",
      "Epoch [89/100], Step [2900/17448], Loss: 7.8750\n",
      "Epoch [89/100], Step [3000/17448], Loss: 5.7813\n",
      "Epoch [89/100], Step [3100/17448], Loss: 8.6094\n",
      "Epoch [89/100], Step [3200/17448], Loss: 6.3751\n",
      "Epoch [89/100], Step [3300/17448], Loss: 8.0001\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [90/100], Step [100/17448], Loss: 9.2188\n",
      "Epoch [90/100], Step [200/17448], Loss: 10.0313\n",
      "Epoch [90/100], Step [300/17448], Loss: 5.5157\n",
      "Epoch [90/100], Step [400/17448], Loss: 5.6094\n",
      "Epoch [90/100], Step [500/17448], Loss: 6.2345\n",
      "Epoch [90/100], Step [600/17448], Loss: 7.9063\n",
      "Epoch [90/100], Step [700/17448], Loss: 6.9844\n",
      "Epoch [90/100], Step [800/17448], Loss: 9.7657\n",
      "Epoch [90/100], Step [900/17448], Loss: 10.9063\n",
      "Epoch [90/100], Step [1000/17448], Loss: 4.8439\n",
      "Epoch [90/100], Step [1100/17448], Loss: 5.9531\n",
      "Epoch [90/100], Step [1200/17448], Loss: 10.4063\n",
      "Epoch [90/100], Step [1300/17448], Loss: 9.2813\n",
      "Epoch [90/100], Step [1400/17448], Loss: 5.8907\n",
      "Epoch [90/100], Step [1500/17448], Loss: 7.0469\n",
      "Epoch [90/100], Step [1600/17448], Loss: 6.7969\n",
      "Epoch [90/100], Step [1700/17448], Loss: 7.0157\n",
      "Epoch [90/100], Step [1800/17448], Loss: 9.1876\n",
      "Epoch [90/100], Step [1900/17448], Loss: 8.3125\n",
      "Epoch [90/100], Step [2000/17448], Loss: 8.6564\n",
      "Epoch [90/100], Step [2100/17448], Loss: 7.1563\n",
      "Epoch [90/100], Step [2200/17448], Loss: 9.9532\n",
      "Epoch [90/100], Step [2300/17448], Loss: 8.6250\n",
      "Epoch [90/100], Step [2400/17448], Loss: 7.1563\n",
      "Epoch [90/100], Step [2500/17448], Loss: 8.9375\n",
      "Epoch [90/100], Step [2600/17448], Loss: 6.9844\n",
      "Epoch [90/100], Step [2700/17448], Loss: 7.1407\n",
      "Epoch [90/100], Step [2800/17448], Loss: 8.2188\n",
      "Epoch [90/100], Step [2900/17448], Loss: 8.7344\n",
      "Epoch [90/100], Step [3000/17448], Loss: 7.9376\n",
      "Epoch [90/100], Step [3100/17448], Loss: 5.9532\n",
      "Epoch [90/100], Step [3200/17448], Loss: 5.1097\n",
      "Epoch [90/100], Step [3300/17448], Loss: 7.3908\n",
      "Epoch [90/100], Step [3400/17448], Loss: 8.0157\n",
      "Epoch [90/100], Step [3500/17448], Loss: 8.3281\n",
      "Epoch [90/100], Step [3600/17448], Loss: 6.8906\n",
      "Epoch [90/100], Step [3700/17448], Loss: 9.9375\n",
      "Epoch [90/100], Step [3800/17448], Loss: 9.1094\n",
      "Epoch [90/100], Step [3900/17448], Loss: 7.3907\n",
      "Epoch [90/100], Step [4000/17448], Loss: 6.9688\n",
      "Epoch [90/100], Step [4100/17448], Loss: 6.2813\n",
      "Epoch [90/100], Step [4200/17448], Loss: 9.9376\n",
      "Epoch [90/100], Step [4300/17448], Loss: 7.6250\n",
      "Epoch [90/100], Step [4400/17448], Loss: 8.0781\n",
      "Epoch [90/100], Step [4500/17448], Loss: 9.2657\n",
      "Epoch [90/100], Step [4600/17448], Loss: 8.2031\n",
      "Epoch [90/100], Step [4700/17448], Loss: 8.5469\n",
      "Epoch [90/100], Step [4800/17448], Loss: 14.5938\n",
      "Epoch [90/100], Step [4900/17448], Loss: 10.8125\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [91/100], Step [100/17448], Loss: 10.9688\n",
      "Epoch [91/100], Step [200/17448], Loss: 8.7345\n",
      "Epoch [91/100], Step [300/17448], Loss: 5.3281\n",
      "Epoch [91/100], Step [400/17448], Loss: 7.8125\n",
      "Epoch [91/100], Step [500/17448], Loss: 8.1251\n",
      "Epoch [91/100], Step [600/17448], Loss: 5.5625\n",
      "Epoch [91/100], Step [700/17448], Loss: 10.1406\n",
      "Epoch [91/100], Step [800/17448], Loss: 6.9845\n",
      "Epoch [91/100], Step [900/17448], Loss: 10.9844\n",
      "Epoch [91/100], Step [1000/17448], Loss: 7.8125\n",
      "Epoch [91/100], Step [1100/17448], Loss: 8.3906\n",
      "Epoch [91/100], Step [1200/17448], Loss: 9.0469\n",
      "Epoch [91/100], Step [1300/17448], Loss: 8.7971\n",
      "Epoch [91/100], Step [1400/17448], Loss: 7.4688\n",
      "Epoch [91/100], Step [1500/17448], Loss: 11.4062\n",
      "Epoch [91/100], Step [1600/17448], Loss: 8.1563\n",
      "Epoch [91/100], Step [1700/17448], Loss: 7.2813\n",
      "Epoch [91/100], Step [1800/17448], Loss: 7.5469\n",
      "Epoch [91/100], Step [1900/17448], Loss: 6.0938\n",
      "Epoch [91/100], Step [2000/17448], Loss: 8.8282\n",
      "Epoch [91/100], Step [2100/17448], Loss: 9.3750\n",
      "Epoch [91/100], Step [2200/17448], Loss: 6.7657\n",
      "Epoch [91/100], Step [2300/17448], Loss: 7.1094\n",
      "Epoch [91/100], Step [2400/17448], Loss: 6.4844\n",
      "Epoch [91/100], Step [2500/17448], Loss: 9.0939\n",
      "Epoch [91/100], Step [2600/17448], Loss: 5.3439\n",
      "Epoch [91/100], Step [2700/17448], Loss: 9.3594\n",
      "Epoch [91/100], Step [2800/17448], Loss: 6.6407\n",
      "Epoch [91/100], Step [2900/17448], Loss: 7.9219\n",
      "Epoch [91/100], Step [3000/17448], Loss: 14.3438\n",
      "Epoch [91/100], Step [3100/17448], Loss: 8.9219\n",
      "Epoch [91/100], Step [3200/17448], Loss: 6.3281\n",
      "Epoch [91/100], Step [3300/17448], Loss: 7.6094\n",
      "Epoch [91/100], Step [3400/17448], Loss: 7.1250\n",
      "Epoch [91/100], Step [3500/17448], Loss: 8.8125\n",
      "Epoch [91/100], Step [3600/17448], Loss: 7.1877\n",
      "Epoch [91/100], Step [3700/17448], Loss: 5.2969\n",
      "Epoch [91/100], Step [3800/17448], Loss: 6.7658\n",
      "Epoch [91/100], Step [3900/17448], Loss: 8.7345\n",
      "Epoch [91/100], Step [4000/17448], Loss: 7.1251\n",
      "Epoch [91/100], Step [4100/17448], Loss: 8.9532\n",
      "Epoch [91/100], Step [4200/17448], Loss: 10.0938\n",
      "Epoch [91/100], Step [4300/17448], Loss: 7.2344\n",
      "Epoch [91/100], Step [4400/17448], Loss: 6.6875\n",
      "Epoch [91/100], Step [4500/17448], Loss: 6.1250\n",
      "Epoch [91/100], Step [4600/17448], Loss: 5.0469\n",
      "Epoch [91/100], Step [4700/17448], Loss: 12.7031\n",
      "Epoch [91/100], Step [4800/17448], Loss: 7.1563\n",
      "Epoch [91/100], Step [4900/17448], Loss: 6.4063\n",
      "Epoch [91/100], Step [5000/17448], Loss: 8.1251\n",
      "Epoch [91/100], Step [5100/17448], Loss: 4.0469\n",
      "Epoch [91/100], Step [5200/17448], Loss: 5.4531\n",
      "Epoch [91/100], Step [5300/17448], Loss: 8.5313\n",
      "Epoch [91/100], Step [5400/17448], Loss: 8.0157\n",
      "Epoch [91/100], Step [5500/17448], Loss: 5.3906\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [92/100], Step [100/17448], Loss: 7.1563\n",
      "Epoch [92/100], Step [200/17448], Loss: 11.7656\n",
      "Epoch [92/100], Step [300/17448], Loss: 7.8595\n",
      "Epoch [92/100], Step [400/17448], Loss: 7.8438\n",
      "Epoch [92/100], Step [500/17448], Loss: 7.1094\n",
      "Epoch [92/100], Step [600/17448], Loss: 5.8906\n",
      "Epoch [92/100], Step [700/17448], Loss: 5.9375\n",
      "Epoch [92/100], Step [800/17448], Loss: 8.3282\n",
      "Epoch [92/100], Step [900/17448], Loss: 10.3594\n",
      "Epoch [92/100], Step [1000/17448], Loss: 6.8439\n",
      "Epoch [92/100], Step [1100/17448], Loss: 10.5157\n",
      "Epoch [92/100], Step [1200/17448], Loss: 9.6563\n",
      "Epoch [92/100], Step [1300/17448], Loss: 5.8281\n",
      "Epoch [92/100], Step [1400/17448], Loss: 8.5625\n",
      "Epoch [92/100], Step [1500/17448], Loss: 8.5000\n",
      "Epoch [92/100], Step [1600/17448], Loss: 8.3594\n",
      "Epoch [92/100], Step [1700/17448], Loss: 7.8907\n",
      "Epoch [92/100], Step [1800/17448], Loss: 5.7970\n",
      "Epoch [92/100], Step [1900/17448], Loss: 7.8438\n",
      "Epoch [92/100], Step [2000/17448], Loss: 6.7188\n",
      "Epoch [92/100], Step [2100/17448], Loss: 6.3595\n",
      "Epoch [92/100], Step [2200/17448], Loss: 10.5469\n",
      "Epoch [92/100], Step [2300/17448], Loss: 4.8595\n",
      "Epoch [92/100], Step [2400/17448], Loss: 7.9532\n",
      "Epoch [92/100], Step [2500/17448], Loss: 5.7969\n",
      "Epoch [92/100], Step [2600/17448], Loss: 7.5625\n",
      "Epoch [92/100], Step [2700/17448], Loss: 4.4063\n",
      "Epoch [92/100], Step [2800/17448], Loss: 8.5625\n",
      "Epoch [92/100], Step [2900/17448], Loss: 11.0469\n",
      "Epoch [92/100], Step [3000/17448], Loss: 10.5781\n",
      "Epoch [92/100], Step [3100/17448], Loss: 11.4532\n",
      "Epoch [92/100], Step [3200/17448], Loss: 6.0628\n",
      "Epoch [92/100], Step [3300/17448], Loss: 6.0469\n",
      "Epoch [92/100], Step [3400/17448], Loss: 6.9219\n",
      "Epoch [92/100], Step [3500/17448], Loss: 11.1563\n",
      "Epoch [92/100], Step [3600/17448], Loss: 6.4844\n",
      "Epoch [92/100], Step [3700/17448], Loss: 8.3751\n",
      "Epoch [92/100], Step [3800/17448], Loss: 9.9846\n",
      "Epoch [92/100], Step [3900/17448], Loss: 7.4220\n",
      "Epoch [92/100], Step [4000/17448], Loss: 7.0314\n",
      "Epoch [92/100], Step [4100/17448], Loss: 8.6095\n",
      "Epoch [92/100], Step [4200/17448], Loss: 6.5938\n",
      "Epoch [92/100], Step [4300/17448], Loss: 4.7344\n",
      "Epoch [92/100], Step [4400/17448], Loss: 7.6563\n",
      "Epoch [92/100], Step [4500/17448], Loss: 11.0313\n",
      "Epoch [92/100], Step [4600/17448], Loss: 8.7188\n",
      "Epoch [92/100], Step [4700/17448], Loss: 7.8908\n",
      "Epoch [92/100], Step [4800/17448], Loss: 9.1097\n",
      "Epoch [92/100], Step [4900/17448], Loss: 5.6719\n",
      "Epoch [92/100], Step [5000/17448], Loss: 7.9219\n",
      "Epoch [92/100], Step [5100/17448], Loss: 11.4063\n",
      "Epoch [92/100], Step [5200/17448], Loss: 10.0157\n",
      "Epoch [92/100], Step [5300/17448], Loss: 7.5939\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [93/100], Step [100/17448], Loss: 8.6563\n",
      "Epoch [93/100], Step [200/17448], Loss: 6.1721\n",
      "Epoch [93/100], Step [300/17448], Loss: 7.0001\n",
      "Epoch [93/100], Step [400/17448], Loss: 7.0626\n",
      "Epoch [93/100], Step [500/17448], Loss: 11.9375\n",
      "Epoch [93/100], Step [600/17448], Loss: 8.4688\n",
      "Epoch [93/100], Step [700/17448], Loss: 7.1719\n",
      "Epoch [93/100], Step [800/17448], Loss: 4.7189\n",
      "Epoch [93/100], Step [900/17448], Loss: 7.3125\n",
      "Epoch [93/100], Step [1000/17448], Loss: 11.6250\n",
      "Epoch [93/100], Step [1100/17448], Loss: 7.2344\n",
      "Epoch [93/100], Step [1200/17448], Loss: 8.0938\n",
      "Epoch [93/100], Step [1300/17448], Loss: 8.1094\n",
      "Epoch [93/100], Step [1400/17448], Loss: 9.9532\n",
      "Epoch [93/100], Step [1500/17448], Loss: 7.8907\n",
      "Epoch [93/100], Step [1600/17448], Loss: 7.7657\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [94/100], Step [100/17448], Loss: 8.1094\n",
      "Epoch [94/100], Step [200/17448], Loss: 4.9688\n",
      "Epoch [94/100], Step [300/17448], Loss: 7.3438\n",
      "Epoch [94/100], Step [400/17448], Loss: 8.7656\n",
      "Epoch [94/100], Step [500/17448], Loss: 5.5470\n",
      "Epoch [94/100], Step [600/17448], Loss: 8.6719\n",
      "Epoch [94/100], Step [700/17448], Loss: 12.9844\n",
      "Epoch [94/100], Step [800/17448], Loss: 5.5627\n",
      "Epoch [94/100], Step [900/17448], Loss: 6.0001\n",
      "Epoch [94/100], Step [1000/17448], Loss: 5.1875\n",
      "Epoch [94/100], Step [1100/17448], Loss: 7.0001\n",
      "Epoch [94/100], Step [1200/17448], Loss: 5.9375\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [95/100], Step [100/17448], Loss: 8.7502\n",
      "Epoch [95/100], Step [200/17448], Loss: 8.1719\n",
      "Epoch [95/100], Step [300/17448], Loss: 8.9375\n",
      "Epoch [95/100], Step [400/17448], Loss: 8.5938\n",
      "Epoch [95/100], Step [500/17448], Loss: 7.3282\n",
      "Epoch [95/100], Step [600/17448], Loss: 8.6094\n",
      "Epoch [95/100], Step [700/17448], Loss: 7.1251\n",
      "Epoch [95/100], Step [800/17448], Loss: 6.5938\n",
      "Epoch [95/100], Step [900/17448], Loss: 7.7031\n",
      "Epoch [95/100], Step [1000/17448], Loss: 9.1406\n",
      "Epoch [95/100], Step [1100/17448], Loss: 6.2188\n",
      "Epoch [95/100], Step [1200/17448], Loss: 9.0625\n",
      "Epoch [95/100], Step [1300/17448], Loss: 8.6094\n",
      "Epoch [95/100], Step [1400/17448], Loss: 6.6250\n",
      "Epoch [95/100], Step [1500/17448], Loss: 8.8594\n",
      "Epoch [95/100], Step [1600/17448], Loss: 7.3438\n",
      "Epoch [95/100], Step [1700/17448], Loss: 7.6251\n",
      "Epoch [95/100], Step [1800/17448], Loss: 9.7658\n",
      "Epoch [95/100], Step [1900/17448], Loss: 10.8750\n",
      "Epoch [95/100], Step [2000/17448], Loss: 7.4532\n",
      "Epoch [95/100], Step [2100/17448], Loss: 7.3751\n",
      "Epoch [95/100], Step [2200/17448], Loss: 6.2344\n",
      "Epoch [95/100], Step [2300/17448], Loss: 9.9064\n",
      "Epoch [95/100], Step [2400/17448], Loss: 6.5626\n",
      "Epoch [95/100], Step [2500/17448], Loss: 5.8594\n",
      "Epoch [95/100], Step [2600/17448], Loss: 8.3438\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [96/100], Step [100/17448], Loss: 8.4687\n",
      "Epoch [96/100], Step [200/17448], Loss: 10.8438\n",
      "Epoch [96/100], Step [300/17448], Loss: 7.6876\n",
      "Epoch [96/100], Step [400/17448], Loss: 7.9219\n",
      "Epoch [96/100], Step [500/17448], Loss: 7.0157\n",
      "Epoch [96/100], Step [600/17448], Loss: 6.9532\n",
      "Epoch [96/100], Step [700/17448], Loss: 7.7188\n",
      "Epoch [96/100], Step [800/17448], Loss: 8.1875\n",
      "Epoch [96/100], Step [900/17448], Loss: 7.3750\n",
      "Epoch [96/100], Step [1000/17448], Loss: 10.8126\n",
      "Epoch [96/100], Step [1100/17448], Loss: 7.6875\n",
      "Epoch [96/100], Step [1200/17448], Loss: 8.6406\n",
      "Epoch [96/100], Step [1300/17448], Loss: 9.6250\n",
      "Epoch [96/100], Step [1400/17448], Loss: 3.6563\n",
      "Epoch [96/100], Step [1500/17448], Loss: 8.9376\n",
      "Epoch [96/100], Step [1600/17448], Loss: 8.8909\n",
      "Epoch [96/100], Step [1700/17448], Loss: 7.7501\n",
      "Epoch [96/100], Step [1800/17448], Loss: 6.0000\n",
      "Epoch [96/100], Step [1900/17448], Loss: 9.6563\n",
      "Epoch [96/100], Step [2000/17448], Loss: 8.0000\n",
      "Epoch [96/100], Step [2100/17448], Loss: 5.3126\n",
      "Epoch [96/100], Step [2200/17448], Loss: 7.7031\n",
      "Epoch [96/100], Step [2300/17448], Loss: 7.2813\n",
      "Epoch [96/100], Step [2400/17448], Loss: 8.3594\n",
      "Epoch [96/100], Step [2500/17448], Loss: 7.5469\n",
      "Epoch [96/100], Step [2600/17448], Loss: 5.6719\n",
      "Epoch [96/100], Step [2700/17448], Loss: 6.1720\n",
      "Epoch [96/100], Step [2800/17448], Loss: 5.9219\n",
      "Epoch [96/100], Step [2900/17448], Loss: 7.6407\n",
      "Epoch [96/100], Step [3000/17448], Loss: 6.8438\n",
      "Epoch [96/100], Step [3100/17448], Loss: 7.1719\n",
      "Epoch [96/100], Step [3200/17448], Loss: 10.6876\n",
      "Epoch [96/100], Step [3300/17448], Loss: 5.0469\n",
      "Epoch [96/100], Step [3400/17448], Loss: 10.1564\n",
      "Epoch [96/100], Step [3500/17448], Loss: 8.4063\n",
      "Epoch [96/100], Step [3600/17448], Loss: 9.7501\n",
      "Epoch [96/100], Step [3700/17448], Loss: 6.2345\n",
      "Epoch [96/100], Step [3800/17448], Loss: 8.2033\n",
      "Epoch [96/100], Step [3900/17448], Loss: 8.1408\n",
      "Epoch [96/100], Step [4000/17448], Loss: 7.1094\n",
      "Epoch [96/100], Step [4100/17448], Loss: 5.2969\n",
      "Epoch [96/100], Step [4200/17448], Loss: 6.1408\n",
      "Epoch [96/100], Step [4300/17448], Loss: 7.3906\n",
      "Epoch [96/100], Step [4400/17448], Loss: 6.4689\n",
      "Epoch [96/100], Step [4500/17448], Loss: 11.9844\n",
      "Epoch [96/100], Step [4600/17448], Loss: 6.5001\n",
      "Epoch [96/100], Step [4700/17448], Loss: 6.9844\n",
      "Epoch [96/100], Step [4800/17448], Loss: 6.0629\n",
      "Epoch [96/100], Step [4900/17448], Loss: 10.5469\n",
      "Epoch [96/100], Step [5000/17448], Loss: 9.6875\n",
      "Epoch [96/100], Step [5100/17448], Loss: 7.0313\n",
      "Epoch [96/100], Step [5200/17448], Loss: 7.8282\n",
      "Epoch [96/100], Step [5300/17448], Loss: 5.5938\n",
      "Epoch [96/100], Step [5400/17448], Loss: 7.8751\n",
      "Epoch [96/100], Step [5500/17448], Loss: 8.8438\n",
      "Epoch [96/100], Step [5600/17448], Loss: 8.8750\n",
      "Epoch [96/100], Step [5700/17448], Loss: 6.5313\n",
      "Epoch [96/100], Step [5800/17448], Loss: 8.9688\n",
      "Epoch [96/100], Step [5900/17448], Loss: 4.5313\n",
      "Epoch [96/100], Step [6000/17448], Loss: 7.1876\n",
      "Epoch [96/100], Step [6100/17448], Loss: 7.5938\n",
      "Epoch [96/100], Step [6200/17448], Loss: 6.0781\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [97/100], Step [100/17448], Loss: 10.2188\n",
      "Epoch [97/100], Step [200/17448], Loss: 10.1719\n",
      "Epoch [97/100], Step [300/17448], Loss: 8.2813\n",
      "Epoch [97/100], Step [400/17448], Loss: 8.6251\n",
      "Epoch [97/100], Step [500/17448], Loss: 6.4689\n",
      "Epoch [97/100], Step [600/17448], Loss: 7.6094\n",
      "Epoch [97/100], Step [700/17448], Loss: 7.9219\n",
      "Epoch [97/100], Step [800/17448], Loss: 10.0938\n",
      "Epoch [97/100], Step [900/17448], Loss: 8.0626\n",
      "Epoch [97/100], Step [1000/17448], Loss: 8.5938\n",
      "Epoch [97/100], Step [1100/17448], Loss: 11.3594\n",
      "Epoch [97/100], Step [1200/17448], Loss: 8.3125\n",
      "Epoch [97/100], Step [1300/17448], Loss: 7.5000\n",
      "Epoch [97/100], Step [1400/17448], Loss: 6.2188\n",
      "Epoch [97/100], Step [1500/17448], Loss: 11.0938\n",
      "Epoch [97/100], Step [1600/17448], Loss: 6.3907\n",
      "Epoch [97/100], Step [1700/17448], Loss: 6.9533\n",
      "Epoch [97/100], Step [1800/17448], Loss: 5.5782\n",
      "Epoch [97/100], Step [1900/17448], Loss: 10.2032\n",
      "Epoch [97/100], Step [2000/17448], Loss: 7.6877\n",
      "Epoch [97/100], Step [2100/17448], Loss: 8.1563\n",
      "Epoch [97/100], Step [2200/17448], Loss: 8.1406\n",
      "Epoch [97/100], Step [2300/17448], Loss: 3.5782\n",
      "Epoch [97/100], Step [2400/17448], Loss: 7.8750\n",
      "Epoch [97/100], Step [2500/17448], Loss: 7.2970\n",
      "Epoch [97/100], Step [2600/17448], Loss: 7.6251\n",
      "Epoch [97/100], Step [2700/17448], Loss: 5.2188\n",
      "Epoch [97/100], Step [2800/17448], Loss: 6.2970\n",
      "Epoch [97/100], Step [2900/17448], Loss: 10.4531\n",
      "Epoch [97/100], Step [3000/17448], Loss: 7.5938\n",
      "Epoch [97/100], Step [3100/17448], Loss: 4.8282\n",
      "Epoch [97/100], Step [3200/17448], Loss: 10.2344\n",
      "Epoch [97/100], Step [3300/17448], Loss: 6.9376\n",
      "Epoch [97/100], Step [3400/17448], Loss: 8.8907\n",
      "Epoch [97/100], Step [3500/17448], Loss: 5.6563\n",
      "Epoch [97/100], Step [3600/17448], Loss: 7.4219\n",
      "Epoch [97/100], Step [3700/17448], Loss: 12.3438\n",
      "Epoch [97/100], Step [3800/17448], Loss: 8.2813\n",
      "Epoch [97/100], Step [3900/17448], Loss: 6.7344\n",
      "Epoch [97/100], Step [4000/17448], Loss: 6.8907\n",
      "Epoch [97/100], Step [4100/17448], Loss: 9.4219\n",
      "Epoch [97/100], Step [4200/17448], Loss: 7.3438\n",
      "Epoch [97/100], Step [4300/17448], Loss: 5.1719\n",
      "Epoch [97/100], Step [4400/17448], Loss: 7.9219\n",
      "Epoch [97/100], Step [4500/17448], Loss: 10.6719\n",
      "Epoch [97/100], Step [4600/17448], Loss: 8.3751\n",
      "Epoch [97/100], Step [4700/17448], Loss: 11.1094\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [98/100], Step [100/17448], Loss: 8.5313\n",
      "Epoch [98/100], Step [200/17448], Loss: 8.9688\n",
      "Epoch [98/100], Step [300/17448], Loss: 9.4844\n",
      "Epoch [98/100], Step [400/17448], Loss: 7.2969\n",
      "Epoch [98/100], Step [500/17448], Loss: 11.9688\n",
      "Epoch [98/100], Step [600/17448], Loss: 13.0938\n",
      "Epoch [98/100], Step [700/17448], Loss: 8.4063\n",
      "Epoch [98/100], Step [800/17448], Loss: 5.8438\n",
      "Epoch [98/100], Step [900/17448], Loss: 4.0157\n",
      "Epoch [98/100], Step [1000/17448], Loss: 6.4844\n",
      "Epoch [98/100], Step [1100/17448], Loss: 9.9377\n",
      "Epoch [98/100], Step [1200/17448], Loss: 10.2500\n",
      "Epoch [98/100], Step [1300/17448], Loss: 6.4688\n",
      "Epoch [98/100], Step [1400/17448], Loss: 4.9844\n",
      "Epoch [98/100], Step [1500/17448], Loss: 6.9219\n",
      "Epoch [98/100], Step [1600/17448], Loss: 8.4219\n",
      "Epoch [98/100], Step [1700/17448], Loss: 6.8125\n",
      "Epoch [98/100], Step [1800/17448], Loss: 5.3127\n",
      "Epoch [98/100], Step [1900/17448], Loss: 9.2344\n",
      "Epoch [98/100], Step [2000/17448], Loss: 8.0938\n",
      "Epoch [98/100], Step [2100/17448], Loss: 8.3438\n",
      "Epoch [98/100], Step [2200/17448], Loss: 9.7813\n",
      "Epoch [98/100], Step [2300/17448], Loss: 7.7656\n",
      "Epoch [98/100], Step [2400/17448], Loss: 11.2500\n",
      "Epoch [98/100], Step [2500/17448], Loss: 7.1719\n",
      "Epoch [98/100], Step [2600/17448], Loss: 7.7969\n",
      "Epoch [98/100], Step [2700/17448], Loss: 5.9532\n",
      "Epoch [98/100], Step [2800/17448], Loss: 4.7813\n",
      "Epoch [98/100], Step [2900/17448], Loss: 10.4688\n",
      "Epoch [98/100], Step [3000/17448], Loss: 5.6875\n",
      "Epoch [98/100], Step [3100/17448], Loss: 9.4219\n",
      "Epoch [98/100], Step [3200/17448], Loss: 9.6250\n",
      "Epoch [98/100], Step [3300/17448], Loss: 6.7656\n",
      "Epoch [98/100], Step [3400/17448], Loss: 5.6563\n",
      "Epoch [98/100], Step [3500/17448], Loss: 8.6250\n",
      "Epoch [98/100], Step [3600/17448], Loss: 7.3281\n",
      "Epoch [98/100], Step [3700/17448], Loss: 6.8126\n",
      "Epoch [98/100], Step [3800/17448], Loss: 7.0157\n",
      "Epoch [98/100], Step [3900/17448], Loss: 9.1720\n",
      "Epoch [98/100], Step [4000/17448], Loss: 7.0782\n",
      "Epoch [98/100], Step [4100/17448], Loss: 4.8282\n",
      "Epoch [98/100], Step [4200/17448], Loss: 7.7188\n",
      "Epoch [98/100], Step [4300/17448], Loss: 4.3750\n",
      "Epoch [98/100], Step [4400/17448], Loss: 6.7657\n",
      "Epoch [98/100], Step [4500/17448], Loss: 7.7969\n",
      "Epoch [98/100], Step [4600/17448], Loss: 9.2813\n",
      "Epoch [98/100], Step [4700/17448], Loss: 8.3907\n",
      "Epoch [98/100], Step [4800/17448], Loss: 7.9219\n",
      "Epoch [98/100], Step [4900/17448], Loss: 6.7657\n",
      "Epoch [98/100], Step [5000/17448], Loss: 4.2188\n",
      "Epoch [98/100], Step [5100/17448], Loss: 6.7656\n",
      "Epoch [98/100], Step [5200/17448], Loss: 8.3750\n",
      "Epoch [98/100], Step [5300/17448], Loss: 8.7813\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [99/100], Step [100/17448], Loss: 10.2188\n",
      "Epoch [99/100], Step [200/17448], Loss: 9.4220\n",
      "Epoch [99/100], Step [300/17448], Loss: 7.6719\n",
      "Epoch [99/100], Step [400/17448], Loss: 5.5156\n",
      "Epoch [99/100], Step [500/17448], Loss: 5.6719\n",
      "Epoch [99/100], Step [600/17448], Loss: 8.0469\n",
      "Epoch [99/100], Step [700/17448], Loss: 8.3750\n",
      "Epoch [99/100], Step [800/17448], Loss: 6.6719\n",
      "Epoch [99/100], Step [900/17448], Loss: 8.7656\n",
      "Epoch [99/100], Step [1000/17448], Loss: 6.6563\n",
      "Epoch [99/100], Step [1100/17448], Loss: 6.1095\n",
      "Epoch [99/100], Step [1200/17448], Loss: 6.2813\n",
      "Epoch [99/100], Step [1300/17448], Loss: 6.1719\n",
      "Epoch [99/100], Step [1400/17448], Loss: 7.7657\n",
      "Epoch [99/100], Step [1500/17448], Loss: 7.5782\n",
      "Epoch [99/100], Step [1600/17448], Loss: 8.7032\n",
      "Epoch [99/100], Step [1700/17448], Loss: 9.2032\n",
      "Epoch [99/100], Step [1800/17448], Loss: 8.8281\n",
      "Epoch [99/100], Step [1900/17448], Loss: 7.9063\n",
      "Epoch [99/100], Step [2000/17448], Loss: 4.9219\n",
      "Epoch [99/100], Step [2100/17448], Loss: 7.2813\n",
      "Epoch [99/100], Step [2200/17448], Loss: 9.1719\n",
      "Epoch [99/100], Step [2300/17448], Loss: 11.3750\n",
      "Epoch [99/100], Step [2400/17448], Loss: 7.3594\n",
      "Epoch [99/100], Step [2500/17448], Loss: 9.4219\n",
      "Epoch [99/100], Step [2600/17448], Loss: 7.2501\n",
      "Epoch [99/100], Step [2700/17448], Loss: 6.8127\n",
      "Epoch [99/100], Step [2800/17448], Loss: 7.4376\n",
      "Epoch [99/100], Step [2900/17448], Loss: 8.4063\n",
      "Epoch [99/100], Step [3000/17448], Loss: 6.8906\n",
      "Epoch [99/100], Step [3100/17448], Loss: 7.6719\n",
      "Epoch [99/100], Step [3200/17448], Loss: 7.5469\n",
      "Epoch [99/100], Step [3300/17448], Loss: 7.7500\n",
      "Epoch [99/100], Step [3400/17448], Loss: 7.3907\n",
      "Epoch [99/100], Step [3500/17448], Loss: 6.8908\n",
      "Epoch [99/100], Step [3600/17448], Loss: 6.6721\n",
      "Epoch [99/100], Step [3700/17448], Loss: 3.7188\n",
      "Epoch [99/100], Step [3800/17448], Loss: 7.8281\n",
      "Epoch [99/100], Step [3900/17448], Loss: 8.6251\n",
      "Epoch [99/100], Step [4000/17448], Loss: 8.9844\n",
      "Epoch [99/100], Step [4100/17448], Loss: 7.7031\n",
      "Epoch [99/100], Step [4200/17448], Loss: 7.9844\n",
      "Epoch [99/100], Step [4300/17448], Loss: 9.7188\n",
      "Epoch [99/100], Step [4400/17448], Loss: 8.3438\n",
      "Epoch [99/100], Step [4500/17448], Loss: 8.5001\n",
      "Epoch [99/100], Step [4600/17448], Loss: 6.8907\n",
      "Epoch [99/100], Step [4700/17448], Loss: 9.8438\n",
      "Epoch [99/100], Step [4800/17448], Loss: 7.4531\n",
      "Epoch [99/100], Step [4900/17448], Loss: 7.1563\n",
      "Epoch [99/100], Step [5000/17448], Loss: 9.7657\n",
      "Epoch [99/100], Step [5100/17448], Loss: 8.3281\n",
      "Epoch [99/100], Step [5200/17448], Loss: 6.6408\n",
      "Epoch [99/100], Step [5300/17448], Loss: 5.7188\n",
      "Epoch [99/100], Step [5400/17448], Loss: 9.9375\n",
      "Epoch [99/100], Step [5500/17448], Loss: 8.1562\n",
      "Epoch [99/100], Step [5600/17448], Loss: 6.6251\n",
      "Epoch [99/100], Step [5700/17448], Loss: 10.2345\n",
      "Epoch [99/100], Step [5800/17448], Loss: 5.1719\n",
      "Epoch [99/100], Step [5900/17448], Loss: 10.7344\n",
      "Epoch [99/100], Step [6000/17448], Loss: 8.1875\n",
      "Epoch [99/100], Step [6100/17448], Loss: 7.2658\n",
      "Epoch [99/100], Step [6200/17448], Loss: 6.8125\n",
      "Epoch [99/100], Step [6300/17448], Loss: 7.5000\n",
      "Epoch [99/100], Step [6400/17448], Loss: 6.8126\n",
      "Epoch [99/100], Step [6500/17448], Loss: 7.7969\n",
      "Epoch [99/100], Step [6600/17448], Loss: 4.7344\n",
      "Epoch [99/100], Step [6700/17448], Loss: 8.4375\n",
      "Epoch [99/100], Step [6800/17448], Loss: 7.4532\n",
      "Epoch [99/100], Step [6900/17448], Loss: 7.8907\n",
      "Epoch [99/100], Step [7000/17448], Loss: 8.6562\n",
      "Epoch [99/100], Step [7100/17448], Loss: 4.9689\n",
      "Epoch [99/100], Step [7200/17448], Loss: 7.1719\n",
      "Epoch [99/100], Step [7300/17448], Loss: 6.7657\n",
      "Epoch [99/100], Step [7400/17448], Loss: 4.2188\n",
      "Epoch [99/100], Step [7500/17448], Loss: 6.8438\n",
      "Epoch [99/100], Step [7600/17448], Loss: 6.6876\n",
      "Epoch [99/100], Step [7700/17448], Loss: 6.1407\n",
      "Epoch [99/100], Step [7800/17448], Loss: 7.7344\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [100/100], Step [100/17448], Loss: 7.5625\n",
      "Epoch [100/100], Step [200/17448], Loss: 12.6875\n",
      "Epoch [100/100], Step [300/17448], Loss: 9.0938\n",
      "Epoch [100/100], Step [400/17448], Loss: 10.3751\n",
      "Epoch [100/100], Step [500/17448], Loss: 6.6407\n",
      "Epoch [100/100], Step [600/17448], Loss: 5.4064\n",
      "Epoch [100/100], Step [700/17448], Loss: 6.0625\n",
      "Epoch [100/100], Step [800/17448], Loss: 7.7971\n",
      "Epoch [100/100], Step [900/17448], Loss: 7.6563\n",
      "Epoch [100/100], Step [1000/17448], Loss: 6.3282\n",
      "Epoch [100/100], Step [1100/17448], Loss: 5.9532\n",
      "Epoch [100/100], Step [1200/17448], Loss: 10.0000\n",
      "Epoch [100/100], Step [1300/17448], Loss: 5.0313\n",
      "Epoch [100/100], Step [1400/17448], Loss: 9.6252\n",
      "Epoch [100/100], Step [1500/17448], Loss: 9.9844\n",
      "Epoch [100/100], Step [1600/17448], Loss: 6.0938\n",
      "Epoch [100/100], Step [1700/17448], Loss: 6.9531\n",
      "Epoch [100/100], Step [1800/17448], Loss: 9.4844\n",
      "Epoch [100/100], Step [1900/17448], Loss: 8.2969\n",
      "Epoch [100/100], Step [2000/17448], Loss: 6.0938\n",
      "Epoch [100/100], Step [2100/17448], Loss: 7.6252\n",
      "Epoch [100/100], Step [2200/17448], Loss: 3.3438\n",
      "Epoch [100/100], Step [2300/17448], Loss: 9.2813\n",
      "Epoch [100/100], Step [2400/17448], Loss: 7.6250\n",
      "Epoch [100/100], Step [2500/17448], Loss: 9.5781\n",
      "Epoch [100/100], Step [2600/17448], Loss: 9.7656\n",
      "Epoch [100/100], Step [2700/17448], Loss: 9.2501\n",
      "NaN loss encountered. Exiting training loop.\n"
     ]
    }
   ],
   "source": [
    "# Train the model on GPU\n",
    "model.to(device)\n",
    "\n",
    "# Train the model in batches\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN loss encountered. Exiting training loop.\")\n",
    "            break\n",
    "\n",
    "        # loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Print the first 5 predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peirc\\anaconda3\\envs\\snowpark-ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\peirc\\anaconda3\\envs\\snowpark-ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peirc\\anaconda3\\envs\\snowpark-ml\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39ml1_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32mc:\\Users\\peirc\\anaconda3\\envs\\snowpark-ml\\Lib\\site-packages\\torch\\nn\\functional.py:3309\u001b[0m, in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3306\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m   3308\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m-> 3309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y.view(-1, 1))\n",
    "    print(f'Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Print the first 5 predictions\n",
    "    y_pred = y_pred.cpu()\n",
    "    y = y.cpu()\n",
    "    print(y_pred[:5].numpy().flatten())\n",
    "    print(y[:5].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
