{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML libraries\n",
    "import pycaret\n",
    "import xgboost\n",
    "\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.functions import datediff, to_date, col, expr\n",
    "\n",
    "# Import Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "# from pycaret.classification import setup, compare_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open(\"connection.json\"))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PURCHASE_DOCUMENT_ITEM_ID\"  |\"CREATE_DATE\"  |\"COMPANY_CODE_ID\"  |\"VENDOR_ID\"  |\"POSTAL_CD\"  |\"MATERIAL_ID\"  |\"SUB_COMMODITY_DESC\"                    |\"MRP_TYPE_ID\"  |\"PLANT_ID\"  |\"REQUESTED_DELIVERY_DATE\"  |\"INBOUND_DELIVERY_ID\"  |\"INBOUND_DELIVERY_ITEM_ID\"  |\"PLANNED_DELIVERY_DAYS\"  |\"FIRST_GR_POSTING_DATE\"  |\"TARGET_FEATURE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|10                           |20210210       |US39               |8010000984   |60045-5202   |NULL           |Tools                                   |NULL           |4120        |20210215                   |NULL                   |0                           |0.0                      |20210211                 |-4                |\n",
      "|20                           |20210210       |US10               |8010000984   |60045-5202   |NULL           |Machinery & Equipment                   |NULL           |4014        |20210214                   |185520728              |20                          |1.0                      |20210212                 |-2                |\n",
      "|20                           |20210210       |CA10               |V4138        |19973        |2100032775     |Tolling                                 |1              |4007        |20210216                   |185529650              |10                          |2.0                      |20210216                 |0                 |\n",
      "|10                           |20210210       |CA10               |8010002419   |H9J 4A1      |1100119629     |Valves                                  |1              |4036        |20210224                   |NULL                   |0                           |14.0                     |20210406                 |41                |\n",
      "|30                           |20210210       |CA10               |8010002454   |K7L 4Y5      |3100006053     |Tubes & Cores                           |1              |4036        |20210209                   |NULL                   |0                           |1.0                      |20210209                 |0                 |\n",
      "|10                           |20210210       |US10               |NULL         |NULL         |2100013224     |Additives, Colorants & Catalysts        |1              |4014        |20210211                   |185519500              |900002                      |1.0                      |20210216                 |5                 |\n",
      "|10                           |20210210       |US40               |8010099572   |53821        |NULL           |Transportation, Storage, Mail Services  |NULL           |4138        |20211231                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010005511   |77630-1818   |NULL           |Pumps & Compressors                     |NULL           |4120        |20210210                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010001167   |77705-1129   |NULL           |Maintenance Services                    |NULL           |4120        |20211129                   |NULL                   |0                           |0.0                      |0                        |NULL              |\n",
      "|10                           |20210210       |US39               |8010004468   |77507        |1100162838     |Piping & Tubing                         |1              |4120        |20210216                   |185562735              |10                          |6.0                      |20210305                 |17                |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PURCHASE_DOCUMENT_ITEM_ID\"  |\"CREATE_DATE\"  |\"COMPANY_CODE_ID\"  |\"VENDOR_ID\"  |\"POSTAL_CD\"  |\"MATERIAL_ID\"  |\"SUB_COMMODITY_DESC\"                    |\"MRP_TYPE_ID\"  |\"PLANT_ID\"  |\"REQUESTED_DELIVERY_DATE\"  |\"INBOUND_DELIVERY_ID\"  |\"INBOUND_DELIVERY_ITEM_ID\"  |\"PLANNED_DELIVERY_DAYS\"  |\"FIRST_GR_POSTING_DATE\"  |\"TARGET_FEATURE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|30                           |20210703       |CA10               |NULL         |NULL         |2100032311     |Tolling                                 |1              |4007        |20210703                   |185821762              |30                          |0.0                      |20210703                 |0                 |\n",
      "|30                           |20210703       |CA10               |NULL         |NULL         |2100032311     |Tolling                                 |1              |4007        |20210705                   |185821762              |30                          |0.0                      |20210703                 |-2                |\n",
      "|20                           |20210703       |GB10               |8010023829   |59229        |1100197903     |Spinning                                |1              |3009        |20210724                   |NULL                   |0                           |30.0                     |20210716                 |-8                |\n",
      "|10                           |20210703       |GB10               |NULL         |NULL         |2100015291     |Resins & Polymers                       |2              |3024        |20210705                   |185822217              |10                          |0.0                      |20210703                 |-2                |\n",
      "|10                           |20210703       |GB10               |8010023829   |59229        |1100197899     |Spinning                                |1              |3009        |20210714                   |NULL                   |0                           |28.0                     |20210716                 |2                 |\n",
      "|50                           |20210703       |CA10               |NULL         |NULL         |2100011410     |Tolling                                 |1              |4007        |20210706                   |185823356              |50                          |0.0                      |20210704                 |-2                |\n",
      "|10                           |20210703       |CA10               |8010001620   |M5W 1P1      |NULL           |Transportation, Storage, Mail Services  |NULL           |4036        |20210703                   |NULL                   |0                           |0.0                      |20210703                 |0                 |\n",
      "|50                           |20210703       |CA10               |NULL         |NULL         |2100021799     |Tolling                                 |1              |4007        |20210705                   |185821762              |50                          |0.0                      |20210703                 |-2                |\n",
      "|10                           |20210703       |GB10               |NULL         |NULL         |2100015294     |Resins & Polymers                       |2              |3024        |20210703                   |185822220              |10                          |0.0                      |20210703                 |0                 |\n",
      "|10                           |20210703       |CA10               |8010001620   |M5W 1P1      |NULL           |Transportation, Storage, Mail Services  |NULL           |4036        |20210703                   |NULL                   |0                           |0.0                      |20210703                 |0                 |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to the correct table\n",
    "tableName = 'PURCHASE_ORDER_HISTORY'\n",
    "dataframe = session.table(tableName)\n",
    "\n",
    "# Calculation to find the lag between Planned Delivery from Actual Delivery\n",
    "dataframe = dataframe.withColumn(\"target_feature\",\n",
    "                                    datediff('day', \n",
    "                                            col(\"DELIVERY_DATE_ML\"), \n",
    "                                            col(\"FIRST_GR_POSTING_DATE_ML\")))\n",
    "\n",
    "\n",
    "# Example: Selecting specific columns\n",
    "# This selects only a subset of columns. Adjust the column names as needed.\n",
    "filtered_dataframe = dataframe.select(\n",
    "    col(\"PURCHASE_DOCUMENT_ITEM_ID\"), # ID for purchase order\n",
    "    col(\"CREATE_DATE\"),            # day purchase order was created\n",
    "    col(\"COMPANY_CODE_ID\"),           # copmany w/in INVISTA making purchase\n",
    "    col(\"VENDOR_ID\"),                 # ID of the vendor \"we\" are purchasing from\n",
    "    col(\"POSTAL_CD\"),                 # postal code associated w company code ID\n",
    "    col(\"MATERIAL_ID\"),               # ID of material being purchase\n",
    "    col(\"SUB_COMMODITY_DESC\"),        # description of sub commodity\n",
    "    col(\"MRP_TYPE_ID\"),               # determined if material is reordered manually or automatically\n",
    "    col(\"PLANT_ID\"),                  # ID of plant making purchase\n",
    "    col(\"REQUESTED_DELIVERY_DATE\"),# delivery date from requisition\n",
    "    col(\"INBOUND_DELIVERY_ID\"),       # ID for delivery\n",
    "    col(\"INBOUND_DELIVERY_ITEM_ID\"),  # ID of item w/in delivery\n",
    "    col(\"PLANNED_DELIVERY_DAYS\"),     # Amount of days expected to take\n",
    "    col(\"FIRST_GR_POSTING_DATE\"),  # expected delivery date        \n",
    "    col(\"target_feature\")             # Lag between Planned Delivery from Actual Delivery \n",
    ")\n",
    "\n",
    "\n",
    "# Print a sample of the filtered dataframe to standard output.\n",
    "filtered_dataframe.show()\n",
    "\n",
    "# Optionally, you might want to filter rows based on some conditions\n",
    "# Example: Filtering out rows where FIRST_GR_POSTING_DATE_ML is NULL\n",
    "filtered_dataframe = filtered_dataframe.filter(col(\"FIRST_GR_POSTING_DATE\").is_not_null())\n",
    "\n",
    "# filtered_dataframe = filtered_dataframe[filtered_dataframe['PLANNED_DELIVERY_DAYS'] < 6]\n",
    "\n",
    "# Show the DataFrame after filtering\n",
    "filtered_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"COMPANY_CODE_ID\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"VENDOR_ID\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"POSTAL_CD\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"SUB_COMMODITY_DESC\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n",
      "Input value type doesn't match the target column data type, this replacement was skipped. Column Name: \"PLANNED_DELIVERY_DAYS\", Type: StringType(16777216), Input Value: 0, Type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'filtered_dataframe' is the DataFrame you've prepared in Snowflake\n",
    "# Convert the Snowpark DataFrame to a Pandas DataFrame with consideration for NULL values\n",
    "\n",
    "# Convert DataFrame to Pandas, handling NULL values by allowing float conversion\n",
    "df = filtered_dataframe.fillna(0).to_pandas()  # This replaces NULL with 0 before conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_DOCUMENT_ITEM_ID</th>\n",
       "      <th>CREATE_DATE</th>\n",
       "      <th>COMPANY_CODE_ID</th>\n",
       "      <th>VENDOR_ID</th>\n",
       "      <th>POSTAL_CD</th>\n",
       "      <th>MATERIAL_ID</th>\n",
       "      <th>SUB_COMMODITY_DESC</th>\n",
       "      <th>MRP_TYPE_ID</th>\n",
       "      <th>PLANT_ID</th>\n",
       "      <th>REQUESTED_DELIVERY_DATE</th>\n",
       "      <th>INBOUND_DELIVERY_ID</th>\n",
       "      <th>INBOUND_DELIVERY_ITEM_ID</th>\n",
       "      <th>PLANNED_DELIVERY_DAYS</th>\n",
       "      <th>FIRST_GR_POSTING_DATE</th>\n",
       "      <th>TARGET_FEATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CA10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2100021412</td>\n",
       "      <td>Tolling</td>\n",
       "      <td>1</td>\n",
       "      <td>4007</td>\n",
       "      <td>20210331</td>\n",
       "      <td>185610163</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210330</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN20</td>\n",
       "      <td>8010094262</td>\n",
       "      <td>201799</td>\n",
       "      <td>0</td>\n",
       "      <td>Telecommunications media services</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20210331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN20</td>\n",
       "      <td>8010098163</td>\n",
       "      <td>200333</td>\n",
       "      <td>0</td>\n",
       "      <td>Maintenance Services</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20210406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210511</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN16</td>\n",
       "      <td>8010019798</td>\n",
       "      <td>201604</td>\n",
       "      <td>0</td>\n",
       "      <td>Power Generation Equipment</td>\n",
       "      <td>0</td>\n",
       "      <td>1026</td>\n",
       "      <td>20210406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210409</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>US10</td>\n",
       "      <td>8010099718</td>\n",
       "      <td>30165</td>\n",
       "      <td>2300006415</td>\n",
       "      <td>Custom Manufacturing</td>\n",
       "      <td>1</td>\n",
       "      <td>4016</td>\n",
       "      <td>20210409</td>\n",
       "      <td>185639199</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20210413</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PURCHASE_DOCUMENT_ITEM_ID  CREATE_DATE COMPANY_CODE_ID   VENDOR_ID  \\\n",
       "0                         20     20210330            CA10        None   \n",
       "1                         10     20210330            CN20  8010094262   \n",
       "2                         20     20210330            CN20  8010098163   \n",
       "3                         10     20210330            CN16  8010019798   \n",
       "4                         10     20210330            US10  8010099718   \n",
       "\n",
       "  POSTAL_CD  MATERIAL_ID                 SUB_COMMODITY_DESC  MRP_TYPE_ID  \\\n",
       "0      None   2100021412                            Tolling            1   \n",
       "1    201799            0  Telecommunications media services            0   \n",
       "2    200333            0               Maintenance Services            0   \n",
       "3    201604            0         Power Generation Equipment            0   \n",
       "4     30165   2300006415               Custom Manufacturing            1   \n",
       "\n",
       "   PLANT_ID  REQUESTED_DELIVERY_DATE  INBOUND_DELIVERY_ID  \\\n",
       "0      4007                 20210331            185610163   \n",
       "1      1032                 20210331                    0   \n",
       "2      1032                 20210406                    0   \n",
       "3      1026                 20210406                    0   \n",
       "4      4016                 20210409            185639199   \n",
       "\n",
       "   INBOUND_DELIVERY_ITEM_ID PLANNED_DELIVERY_DAYS  FIRST_GR_POSTING_DATE  \\\n",
       "0                        20                   0.0               20210330   \n",
       "1                         0                   0.0                      0   \n",
       "2                         0                   0.0               20210511   \n",
       "3                         0                   0.0               20210409   \n",
       "4                        10                   5.0               20210413   \n",
       "\n",
       "   TARGET_FEATURE  \n",
       "0              -1  \n",
       "1               0  \n",
       "2              35  \n",
       "3               3  \n",
       "4               4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          -1\n",
       "1           0\n",
       "2          35\n",
       "3           3\n",
       "4           4\n",
       "           ..\n",
       "1139387    -2\n",
       "1139388     0\n",
       "1139389    -6\n",
       "1139390    29\n",
       "1139391    -1\n",
       "Name: TARGET_FEATURE, Length: 1139392, dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET_FEATURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_delivery_days(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove leading/trailing whitespace\n",
    "        value = value.strip() \n",
    "\n",
    "        # Check for timestamp format and handle separately\n",
    "        if re.match(r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\", value):  \n",
    "            return \"\"  # Replace timestamps with NA or another placeholder\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "df['PLANNED_DELIVERY_DAYS'] = df['PLANNED_DELIVERY_DAYS'].apply(clean_delivery_days)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'str'> '0'\n",
      "5 <class 'str'> '5'\n",
      "45 <class 'str'> '45'\n",
      "14 <class 'str'> '14'\n",
      "90 <class 'str'> '90'\n",
      "1 <class 'str'> '1'\n",
      "2 <class 'str'> '2'\n",
      "10 <class 'str'> '10'\n",
      "20 <class 'str'> '20'\n",
      "100 <class 'str'> '100'\n",
      "30 <class 'str'> '30'\n",
      "3 <class 'str'> '3'\n",
      "46 <class 'str'> '46'\n",
      "6 <class 'str'> '6'\n",
      "50 <class 'str'> '50'\n",
      "4 <class 'str'> '4'\n",
      "120 <class 'str'> '120'\n",
      "60 <class 'str'> '60'\n",
      "15 <class 'str'> '15'\n",
      "31 <class 'str'> '31'\n",
      "21 <class 'str'> '21'\n",
      "7 <class 'str'> '7'\n",
      "97 <class 'str'> '97'\n",
      "19 <class 'str'> '19'\n",
      "23 <class 'str'> '23'\n",
      "13 <class 'str'> '13'\n",
      "16 <class 'str'> '16'\n",
      "17 <class 'str'> '17'\n",
      "11 <class 'str'> '11'\n",
      "59 <class 'str'> '59'\n",
      "12 <class 'str'> '12'\n",
      "34 <class 'str'> '34'\n",
      "38 <class 'str'> '38'\n",
      "25 <class 'str'> '25'\n",
      "28 <class 'str'> '28'\n",
      "9 <class 'str'> '9'\n",
      "115 <class 'str'> '115'\n",
      "87 <class 'str'> '87'\n",
      "999 <class 'str'> '999'\n",
      "18 <class 'str'> '18'\n",
      "42 <class 'str'> '42'\n",
      "24 <class 'str'> '24'\n",
      "40 <class 'str'> '40'\n",
      "65 <class 'str'> '65'\n",
      "35 <class 'str'> '35'\n",
      "8 <class 'str'> '8'\n",
      "67 <class 'str'> '67'\n",
      "29 <class 'str'> '29'\n",
      "26 <class 'str'> '26'\n",
      "27 <class 'str'> '27'\n",
      "22 <class 'str'> '22'\n",
      "52 <class 'str'> '52'\n",
      "43 <class 'str'> '43'\n",
      "140 <class 'str'> '140'\n",
      "98 <class 'str'> '98'\n",
      "55 <class 'str'> '55'\n",
      "70 <class 'str'> '70'\n",
      "180 <class 'str'> '180'\n",
      "78 <class 'str'> '78'\n",
      "37 <class 'str'> '37'\n",
      "47 <class 'str'> '47'\n",
      "48 <class 'str'> '48'\n",
      "61 <class 'str'> '61'\n",
      "117 <class 'str'> '117'\n",
      "56 <class 'str'> '56'\n",
      "41 <class 'str'> '41'\n",
      "63 <class 'str'> '63'\n",
      "119 <class 'str'> '119'\n",
      "113 <class 'str'> '113'\n",
      "88 <class 'str'> '88'\n",
      "104 <class 'str'> '104'\n",
      "39 <class 'str'> '39'\n",
      "36 <class 'str'> '36'\n",
      "84 <class 'str'> '84'\n",
      "150 <class 'str'> '150'\n",
      "138 <class 'str'> '138'\n",
      "135 <class 'str'> '135'\n",
      "80 <class 'str'> '80'\n",
      "170 <class 'str'> '170'\n",
      "141 <class 'str'> '141'\n",
      "210 <class 'str'> '210'\n",
      "105 <class 'str'> '105'\n",
      "185 <class 'str'> '185'\n",
      "75 <class 'str'> '75'\n",
      "146 <class 'str'> '146'\n",
      "85 <class 'str'> '85'\n",
      "255 <class 'str'> '255'\n",
      "99 <class 'str'> '99'\n",
      "33 <class 'str'> '33'\n",
      "64 <class 'str'> '64'\n",
      "49 <class 'str'> '49'\n",
      "211 <class 'str'> '211'\n",
      "160 <class 'str'> '160'\n",
      "74 <class 'str'> '74'\n",
      "44 <class 'str'> '44'\n",
      "32 <class 'str'> '32'\n",
      "365 <class 'str'> '365'\n",
      "57 <class 'str'> '57'\n",
      "101 <class 'str'> '101'\n",
      "69 <class 'str'> '69'\n",
      "81 <class 'str'> '81'\n",
      "73 <class 'str'> '73'\n",
      "142 <class 'str'> '142'\n",
      "139 <class 'str'> '139'\n",
      "112 <class 'str'> '112'\n",
      "256 <class 'str'> '256'\n",
      "196 <class 'str'> '196'\n",
      "125 <class 'str'> '125'\n",
      "66 <class 'str'> '66'\n",
      "71 <class 'str'> '71'\n",
      "147 <class 'str'> '147'\n",
      "154 <class 'str'> '154'\n",
      "200 <class 'str'> '200'\n",
      "62 <class 'str'> '62'\n",
      "240 <class 'str'> '240'\n",
      "77 <class 'str'> '77'\n",
      "300 <class 'str'> '300'\n",
      "54 <class 'str'> '54'\n",
      "51 <class 'str'> '51'\n",
      "109 <class 'str'> '109'\n",
      "126 <class 'str'> '126'\n",
      "86 <class 'str'> '86'\n",
      "401 <class 'str'> '401'\n",
      "195 <class 'str'> '195'\n",
      "58 <class 'str'> '58'\n",
      "53 <class 'str'> '53'\n",
      "93 <class 'str'> '93'\n",
      "123 <class 'str'> '123'\n",
      "82 <class 'str'> '82'\n",
      "91 <class 'str'> '91'\n",
      "72 <class 'str'> '72'\n",
      "83 <class 'str'> '83'\n",
      "181 <class 'str'> '181'\n",
      "122 <class 'str'> '122'\n",
      "124 <class 'str'> '124'\n",
      "79 <class 'str'> '79'\n",
      "149 <class 'str'> '149'\n",
      "130 <class 'str'> '130'\n",
      "420 <class 'str'> '420'\n",
      "161 <class 'str'> '161'\n",
      "110 <class 'str'> '110'\n",
      "184 <class 'str'> '184'\n",
      "95 <class 'str'> '95'\n",
      "68 <class 'str'> '68'\n",
      "151 <class 'str'> '151'\n",
      "280 <class 'str'> '280'\n",
      "254 <class 'str'> '254'\n",
      "94 <class 'str'> '94'\n",
      "163 <class 'str'> '163'\n",
      "92 <class 'str'> '92'\n",
      "76 <class 'str'> '76'\n",
      "168 <class 'str'> '168'\n",
      "175 <class 'str'> '175'\n",
      "118 <class 'str'> '118'\n",
      "230 <class 'str'> '230'\n",
      "107 <class 'str'> '107'\n",
      "114 <class 'str'> '114'\n",
      "102 <class 'str'> '102'\n",
      "281 <class 'str'> '281'\n",
      "108 <class 'str'> '108'\n",
      "157 <class 'str'> '157'\n",
      "221 <class 'str'> '221'\n",
      " <class 'str'> ''\n",
      "96 <class 'str'> '96'\n",
      "103 <class 'str'> '103'\n",
      "153 <class 'str'> '153'\n",
      "158 <class 'str'> '158'\n",
      "89 <class 'str'> '89'\n",
      "225 <class 'str'> '225'\n",
      "152 <class 'str'> '152'\n",
      "350 <class 'str'> '350'\n",
      "270 <class 'str'> '270'\n",
      "192 <class 'str'> '192'\n",
      "106 <class 'str'> '106'\n",
      "337 <class 'str'> '337'\n",
      "134 <class 'str'> '134'\n",
      "155 <class 'str'> '155'\n",
      "250 <class 'str'> '250'\n",
      "136 <class 'str'> '136'\n",
      "143 <class 'str'> '143'\n",
      "209 <class 'str'> '209'\n",
      "219 <class 'str'> '219'\n",
      "190 <class 'str'> '190'\n",
      "128 <class 'str'> '128'\n",
      "182 <class 'str'> '182'\n",
      "585 <class 'str'> '585'\n",
      "220 <class 'str'> '220'\n",
      "207 <class 'str'> '207'\n",
      "267 <class 'str'> '267'\n",
      "224 <class 'str'> '224'\n",
      "121 <class 'str'> '121'\n",
      "390 <class 'str'> '390'\n",
      "395 <class 'str'> '395'\n",
      "212 <class 'str'> '212'\n",
      "165 <class 'str'> '165'\n",
      "223 <class 'str'> '223'\n",
      "116 <class 'str'> '116'\n",
      "189 <class 'str'> '189'\n",
      "127 <class 'str'> '127'\n",
      "238 <class 'str'> '238'\n",
      "177 <class 'str'> '177'\n",
      "144 <class 'str'> '144'\n",
      "111 <class 'str'> '111'\n",
      "252 <class 'str'> '252'\n",
      "173 <class 'str'> '173'\n",
      "178 <class 'str'> '178'\n",
      "137 <class 'str'> '137'\n",
      "169 <class 'str'> '169'\n",
      "253 <class 'str'> '253'\n",
      "145 <class 'str'> '145'\n",
      "166 <class 'str'> '166'\n",
      "203 <class 'str'> '203'\n",
      "217 <class 'str'> '217'\n",
      "317 <class 'str'> '317'\n",
      "260 <class 'str'> '260'\n",
      "167 <class 'str'> '167'\n",
      "133 <class 'str'> '133'\n",
      "235 <class 'str'> '235'\n",
      "340 <class 'str'> '340'\n",
      "198 <class 'str'> '198'\n",
      "380 <class 'str'> '380'\n",
      "129 <class 'str'> '129'\n",
      "265 <class 'str'> '265'\n",
      "245 <class 'str'> '245'\n",
      "305 <class 'str'> '305'\n",
      "355 <class 'str'> '355'\n",
      "228 <class 'str'> '228'\n",
      "164 <class 'str'> '164'\n",
      "171 <class 'str'> '171'\n",
      "435 <class 'str'> '435'\n",
      "289 <class 'str'> '289'\n",
      "188 <class 'str'> '188'\n",
      "172 <class 'str'> '172'\n",
      "295 <class 'str'> '295'\n",
      "820 <class 'str'> '820'\n",
      "234 <class 'str'> '234'\n",
      "205 <class 'str'> '205'\n",
      "297 <class 'str'> '297'\n",
      "148 <class 'str'> '148'\n",
      "294 <class 'str'> '294'\n",
      "186 <class 'str'> '186'\n",
      "162 <class 'str'> '162'\n",
      "208 <class 'str'> '208'\n",
      "462 <class 'str'> '462'\n",
      "392 <class 'str'> '392'\n",
      "308 <class 'str'> '308'\n",
      "500 <class 'str'> '500'\n",
      "204 <class 'str'> '204'\n",
      "197 <class 'str'> '197'\n",
      "131 <class 'str'> '131'\n",
      "156 <class 'str'> '156'\n",
      "183 <class 'str'> '183'\n",
      "304 <class 'str'> '304'\n",
      "259 <class 'str'> '259'\n",
      "132 <class 'str'> '132'\n",
      "206 <class 'str'> '206'\n",
      "216 <class 'str'> '216'\n",
      "310 <class 'str'> '310'\n",
      "320 <class 'str'> '320'\n",
      "179 <class 'str'> '179'\n"
     ]
    }
   ],
   "source": [
    "def remove_decimal(value):\n",
    "    return value.split('.')[0]  # Split by the decimal and keep the integer part\n",
    "\n",
    "df['PLANNED_DELIVERY_DAYS'] = df['PLANNED_DELIVERY_DAYS'].apply(remove_decimal)\n",
    "\n",
    "for value in df['PLANNED_DELIVERY_DAYS'].unique():\n",
    "    print(value, type(value), repr(value)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting column 'VENDOR_ID': Contains non-numeric values\n",
      "Error converting column 'POSTAL_CD': Contains non-numeric values\n"
     ]
    }
   ],
   "source": [
    "def convert_to_numeric(col):\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except ValueError:\n",
    "        print(f\"Error converting column '{col}': Contains non-numeric values\")\n",
    "\n",
    "convert_to_numeric('PURCHASE_DOCUMENT_ITEM_ID')\n",
    "convert_to_numeric('CREATE_DATE')\n",
    "convert_to_numeric('MATERIAL_ID')\n",
    "convert_to_numeric('MRP_TYPE_ID')\n",
    "convert_to_numeric('PLANT_ID')\n",
    "convert_to_numeric('INBOUND_DELIVERY_ID')\n",
    "convert_to_numeric('INBOUND_DELIVERY_ITEM_ID')\n",
    "convert_to_numeric('PLANNED_DELIVERY_DAYS')\n",
    "convert_to_numeric('FIRST_GR_POSTING_DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_DOCUMENT_ITEM_ID</th>\n",
       "      <th>CREATE_DATE</th>\n",
       "      <th>COMPANY_CODE_ID</th>\n",
       "      <th>VENDOR_ID</th>\n",
       "      <th>POSTAL_CD</th>\n",
       "      <th>MATERIAL_ID</th>\n",
       "      <th>SUB_COMMODITY_DESC</th>\n",
       "      <th>MRP_TYPE_ID</th>\n",
       "      <th>PLANT_ID</th>\n",
       "      <th>REQUESTED_DELIVERY_DATE</th>\n",
       "      <th>INBOUND_DELIVERY_ID</th>\n",
       "      <th>INBOUND_DELIVERY_ITEM_ID</th>\n",
       "      <th>PLANNED_DELIVERY_DAYS</th>\n",
       "      <th>FIRST_GR_POSTING_DATE</th>\n",
       "      <th>TARGET_FEATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CA10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2100021412</td>\n",
       "      <td>Tolling</td>\n",
       "      <td>1</td>\n",
       "      <td>4007</td>\n",
       "      <td>20210331</td>\n",
       "      <td>185610163</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210330</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN20</td>\n",
       "      <td>8010094262</td>\n",
       "      <td>201799</td>\n",
       "      <td>0</td>\n",
       "      <td>Telecommunications media services</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20210331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN20</td>\n",
       "      <td>8010098163</td>\n",
       "      <td>200333</td>\n",
       "      <td>0</td>\n",
       "      <td>Maintenance Services</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>20210406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210511</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>CN16</td>\n",
       "      <td>8010019798</td>\n",
       "      <td>201604</td>\n",
       "      <td>0</td>\n",
       "      <td>Power Generation Equipment</td>\n",
       "      <td>0</td>\n",
       "      <td>1026</td>\n",
       "      <td>20210406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20210409</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>20210330</td>\n",
       "      <td>US10</td>\n",
       "      <td>8010099718</td>\n",
       "      <td>30165</td>\n",
       "      <td>2300006415</td>\n",
       "      <td>Custom Manufacturing</td>\n",
       "      <td>1</td>\n",
       "      <td>4016</td>\n",
       "      <td>20210409</td>\n",
       "      <td>185639199</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20210413</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PURCHASE_DOCUMENT_ITEM_ID  CREATE_DATE COMPANY_CODE_ID   VENDOR_ID  \\\n",
       "0                         20     20210330            CA10        None   \n",
       "1                         10     20210330            CN20  8010094262   \n",
       "2                         20     20210330            CN20  8010098163   \n",
       "3                         10     20210330            CN16  8010019798   \n",
       "4                         10     20210330            US10  8010099718   \n",
       "\n",
       "  POSTAL_CD  MATERIAL_ID                 SUB_COMMODITY_DESC  MRP_TYPE_ID  \\\n",
       "0      None   2100021412                            Tolling            1   \n",
       "1    201799            0  Telecommunications media services            0   \n",
       "2    200333            0               Maintenance Services            0   \n",
       "3    201604            0         Power Generation Equipment            0   \n",
       "4     30165   2300006415               Custom Manufacturing            1   \n",
       "\n",
       "   PLANT_ID  REQUESTED_DELIVERY_DATE  INBOUND_DELIVERY_ID  \\\n",
       "0      4007                 20210331            185610163   \n",
       "1      1032                 20210331                    0   \n",
       "2      1032                 20210406                    0   \n",
       "3      1026                 20210406                    0   \n",
       "4      4016                 20210409            185639199   \n",
       "\n",
       "   INBOUND_DELIVERY_ITEM_ID  PLANNED_DELIVERY_DAYS  FIRST_GR_POSTING_DATE  \\\n",
       "0                        20                    0.0               20210330   \n",
       "1                         0                    0.0                      0   \n",
       "2                         0                    0.0               20210511   \n",
       "3                         0                    0.0               20210409   \n",
       "4                        10                    5.0               20210413   \n",
       "\n",
       "   TARGET_FEATURE  \n",
       "0              -1  \n",
       "1               0  \n",
       "2              35  \n",
       "3               3  \n",
       "4               4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1139392 entries, 0 to 1139391\n",
      "Data columns (total 15 columns):\n",
      " #   Column                     Non-Null Count    Dtype  \n",
      "---  ------                     --------------    -----  \n",
      " 0   PURCHASE_DOCUMENT_ITEM_ID  1139392 non-null  int16  \n",
      " 1   CREATE_DATE                1139392 non-null  int32  \n",
      " 2   COMPANY_CODE_ID            1139392 non-null  object \n",
      " 3   VENDOR_ID                  940583 non-null   object \n",
      " 4   POSTAL_CD                  939024 non-null   object \n",
      " 5   MATERIAL_ID                1139392 non-null  int64  \n",
      " 6   SUB_COMMODITY_DESC         1138367 non-null  object \n",
      " 7   MRP_TYPE_ID                1139392 non-null  int8   \n",
      " 8   PLANT_ID                   1139392 non-null  int16  \n",
      " 9   REQUESTED_DELIVERY_DATE    1139392 non-null  int32  \n",
      " 10  INBOUND_DELIVERY_ID        1139392 non-null  int32  \n",
      " 11  INBOUND_DELIVERY_ITEM_ID   1139392 non-null  int32  \n",
      " 12  PLANNED_DELIVERY_DAYS      1139388 non-null  float64\n",
      " 13  FIRST_GR_POSTING_DATE      1139392 non-null  int32  \n",
      " 14  TARGET_FEATURE             1139392 non-null  int32  \n",
      "dtypes: float64(1), int16(2), int32(6), int64(1), int8(1), object(4)\n",
      "memory usage: 83.7+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show types of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "df = df[(df['TARGET_FEATURE'] < 100) & (df['TARGET_FEATURE'] > -100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select relevant features\n",
    "X_categorical = df[['VENDOR_ID', 'SUB_COMMODITY_DESC', 'POSTAL_CD', 'COMPANY_CODE_ID']]\n",
    "X_numerical = df[['PURCHASE_DOCUMENT_ITEM_ID', 'CREATE_DATE', 'MATERIAL_ID', 'MRP_TYPE_ID', 'PLANT_ID', 'INBOUND_DELIVERY_ID', 'INBOUND_DELIVERY_ITEM_ID', 'PLANNED_DELIVERY_DAYS', 'FIRST_GR_POSTING_DATE']] \n",
    "y = df['TARGET_FEATURE']\n",
    "\n",
    "# Preprocessing categorical features (Assuming they are strings)\n",
    "X_categorical = X_categorical.applymap(str).apply(' '.join, axis=1)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocessing numerical features (Example)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# X_numerical['day_of_week'] = df['CREATE_DATE'].dt.dayofweek  # Extract day of week\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert data into PyTorch Tensors (assuming batching within the model)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X_categorical \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_categorical\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      6\u001b[0m X_numerical \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_numerical\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)) \n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Preprocessing numerical features (Example)\n",
    "# X_numerical['day_of_week'] = df['CREATE_DATE'].dt.dayofweek  # Extract day of week\n",
    "\n",
    "# Convert data into PyTorch Tensors (assuming batching within the model)\n",
    "X_categorical = torch.tensor(X_categorical.values.tolist())\n",
    "X_numerical = torch.tensor(X_numerical.values.astype(np.float32)) \n",
    "y = torch.tensor(y.values.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0000e+01,  2.0210e+07,  3.6584e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0000e+01,  2.0210e+07, -7.6471e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e+01,  2.0210e+07, -7.6471e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 1.0000e+01,  2.0201e+07, -4.3758e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0000e+01,  2.0201e+07,  2.4485e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.0000e+01,  2.0201e+07,  3.6584e+18,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create feature & target tensors on GPU\n",
    "features = df.drop('TARGET_FEATURE', axis=1)\n",
    "targets = df['TARGET_FEATURE']\n",
    "X = torch.tensor(features.values.astype(np.float32))\n",
    "y = torch.tensor(targets.values.astype(np.float32))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0., 35.,  ..., -6., 29., -1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Add training and test data to the device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RegressionNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size1 = 128  \n",
    "        self.hidden_size2 = 64  \n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, self.hidden_size1)\n",
    "        self.linear2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "\n",
    "        # Output layer for regression (no activation)\n",
    "        self.output_layer = nn.Linear(self.hidden_size2, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))  \n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x \n",
    "    \n",
    "# Create a better network\n",
    "class BetterNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size1 = 128  \n",
    "        self.hidden_size2 = 64  \n",
    "        self.hidden_size3 = 32  \n",
    "\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, self.hidden_size1)\n",
    "        self.linear2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "        self.linear3 = nn.Linear(self.hidden_size2, self.hidden_size3)\n",
    "\n",
    "        # Output layer for regression (no activation)\n",
    "        self.output_layer = nn.Linear(self.hidden_size3, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.linear1(x))\n",
    "        x = F.softmax(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# ... Data preprocessing ...\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_numerical_features, embedding_size):\n",
    "        super().__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(num_numerical_features + embedding_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output for regression\n",
    "        )\n",
    "\n",
    "    def forward(self, categorical_features, numerical_features):\n",
    "        # Encode categorical features using BERT\n",
    "        input_ids = self.tokenizer(categorical_features, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.bert_model(**input_ids).last_hidden_state[:, 0, :]\n",
    "\n",
    "         # Combine embeddings with numerical features\n",
    "        combined_features = torch.cat([numerical_features, embeddings], dim=1)\n",
    "\n",
    "        output = self.dense_layers(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "input_size = X.shape[1]  # Number of features\n",
    "\n",
    "# Create the model\n",
    "model = RegressionModel(input_size)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Mean Absolute Error (MAE) loss\n",
    "# loss_fn = nn.L1Loss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BetterNetwork(\n",
       "  (linear1): Linear(in_features=86, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data Preparation (If not using a DataLoader)\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# port to GPU\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Train the model in batches\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (inputs, targets) in enumerate(train_loader):\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "#         loss = loss_fn(outputs, targets)\n",
    "\n",
    "#         if torch.isnan(loss):\n",
    "#             print(\"NaN loss encountered. Exiting training loop.\")\n",
    "#             break\n",
    "\n",
    "#         # loss.backward()\n",
    "        \n",
    "#         # Gradient clipping\n",
    "#         # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (i+1) % 100 == 0:\n",
    "#             print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/13958], Loss: 180.5563\n",
      "Epoch [1/100], Step [200/13958], Loss: 235.9113\n",
      "Epoch [1/100], Step [300/13958], Loss: 181.9789\n",
      "Epoch [1/100], Step [400/13958], Loss: 89.9862\n",
      "Epoch [1/100], Step [500/13958], Loss: 119.7702\n",
      "Epoch [1/100], Step [600/13958], Loss: 313.4944\n",
      "Epoch [1/100], Step [700/13958], Loss: 339.9705\n",
      "Epoch [1/100], Step [800/13958], Loss: 202.5627\n",
      "Epoch [1/100], Step [900/13958], Loss: 272.8154\n",
      "Epoch [1/100], Step [1000/13958], Loss: 122.8910\n",
      "Epoch [1/100], Step [1100/13958], Loss: 313.5657\n",
      "Epoch [1/100], Step [1200/13958], Loss: 267.9989\n",
      "Epoch [1/100], Step [1300/13958], Loss: 287.0521\n",
      "Epoch [1/100], Step [1400/13958], Loss: 202.6155\n",
      "Epoch [1/100], Step [1500/13958], Loss: 275.1834\n",
      "Epoch [1/100], Step [1600/13958], Loss: 345.7057\n",
      "Epoch [1/100], Step [1700/13958], Loss: 435.5145\n",
      "Epoch [1/100], Step [1800/13958], Loss: 173.0294\n",
      "Epoch [1/100], Step [1900/13958], Loss: 128.4226\n",
      "Epoch [1/100], Step [2000/13958], Loss: 511.3183\n",
      "Epoch [1/100], Step [2100/13958], Loss: 246.5365\n",
      "Epoch [1/100], Step [2200/13958], Loss: 121.8797\n",
      "Epoch [1/100], Step [2300/13958], Loss: 126.6269\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [1/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [2/100], Step [100/13958], Loss: 317.1381\n",
      "Epoch [2/100], Step [200/13958], Loss: 143.7199\n",
      "Epoch [2/100], Step [300/13958], Loss: 432.0554\n",
      "Epoch [2/100], Step [400/13958], Loss: 496.3883\n",
      "Epoch [2/100], Step [500/13958], Loss: 531.2817\n",
      "Epoch [2/100], Step [600/13958], Loss: 303.7709\n",
      "Epoch [2/100], Step [700/13958], Loss: 275.6153\n",
      "Epoch [2/100], Step [800/13958], Loss: 252.5804\n",
      "Epoch [2/100], Step [900/13958], Loss: 251.4442\n",
      "Epoch [2/100], Step [1000/13958], Loss: 193.2789\n",
      "Epoch [2/100], Step [1100/13958], Loss: 77.9531\n",
      "Epoch [2/100], Step [1200/13958], Loss: 408.4790\n",
      "Epoch [2/100], Step [1300/13958], Loss: 267.7362\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [2/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [3/100], Step [100/13958], Loss: 238.4888\n",
      "Epoch [3/100], Step [200/13958], Loss: 193.5280\n",
      "Epoch [3/100], Step [300/13958], Loss: 298.0083\n",
      "Epoch [3/100], Step [400/13958], Loss: 321.1652\n",
      "Epoch [3/100], Step [500/13958], Loss: 246.5517\n",
      "Epoch [3/100], Step [600/13958], Loss: 229.3641\n",
      "Epoch [3/100], Step [700/13958], Loss: 214.9020\n",
      "Epoch [3/100], Step [800/13958], Loss: 115.7477\n",
      "Epoch [3/100], Step [900/13958], Loss: 267.3350\n",
      "Epoch [3/100], Step [1000/13958], Loss: 512.1538\n",
      "Epoch [3/100], Step [1100/13958], Loss: 139.6495\n",
      "Epoch [3/100], Step [1200/13958], Loss: 480.6235\n",
      "Epoch [3/100], Step [1300/13958], Loss: 156.7329\n",
      "Epoch [3/100], Step [1400/13958], Loss: 346.6464\n",
      "Epoch [3/100], Step [1500/13958], Loss: 329.2630\n",
      "Epoch [3/100], Step [1600/13958], Loss: 266.7042\n",
      "Epoch [3/100], Step [1700/13958], Loss: 325.9492\n",
      "Epoch [3/100], Step [1800/13958], Loss: 459.4567\n",
      "Epoch [3/100], Step [1900/13958], Loss: 399.9329\n",
      "Epoch [3/100], Step [2000/13958], Loss: 238.0101\n",
      "Epoch [3/100], Step [2100/13958], Loss: 238.6883\n",
      "Epoch [3/100], Step [2200/13958], Loss: 223.5986\n",
      "Epoch [3/100], Step [2300/13958], Loss: 315.7343\n",
      "Epoch [3/100], Step [2400/13958], Loss: 341.6970\n",
      "Epoch [3/100], Step [2500/13958], Loss: 351.5534\n",
      "Epoch [3/100], Step [2600/13958], Loss: 506.8578\n",
      "Epoch [3/100], Step [2700/13958], Loss: 240.4734\n",
      "Epoch [3/100], Step [2800/13958], Loss: 298.9149\n",
      "Epoch [3/100], Step [2900/13958], Loss: 383.0298\n",
      "Epoch [3/100], Step [3000/13958], Loss: 255.3101\n",
      "Epoch [3/100], Step [3100/13958], Loss: 139.9211\n",
      "Epoch [3/100], Step [3200/13958], Loss: 125.2632\n",
      "Epoch [3/100], Step [3300/13958], Loss: 136.4836\n",
      "Epoch [3/100], Step [3400/13958], Loss: 310.0039\n",
      "Epoch [3/100], Step [3500/13958], Loss: 465.6981\n",
      "Epoch [3/100], Step [3600/13958], Loss: 490.5647\n",
      "Epoch [3/100], Step [3700/13958], Loss: 239.3756\n",
      "Epoch [3/100], Step [3800/13958], Loss: 287.1353\n",
      "Epoch [3/100], Step [3900/13958], Loss: 191.8161\n",
      "Epoch [3/100], Step [4000/13958], Loss: 332.5299\n",
      "Epoch [3/100], Step [4100/13958], Loss: 233.9581\n",
      "Epoch [3/100], Step [4200/13958], Loss: 251.8646\n",
      "Epoch [3/100], Step [4300/13958], Loss: 289.1884\n",
      "Epoch [3/100], Step [4400/13958], Loss: 196.2568\n",
      "Epoch [3/100], Step [4500/13958], Loss: 107.2337\n",
      "Epoch [3/100], Step [4600/13958], Loss: 468.6200\n",
      "Epoch [3/100], Step [4700/13958], Loss: 79.4583\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [3/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [4/100], Step [100/13958], Loss: 241.4080\n",
      "Epoch [4/100], Step [200/13958], Loss: 206.0553\n",
      "Epoch [4/100], Step [300/13958], Loss: 428.2720\n",
      "Epoch [4/100], Step [400/13958], Loss: 446.5799\n",
      "Epoch [4/100], Step [500/13958], Loss: 302.2403\n",
      "Epoch [4/100], Step [600/13958], Loss: 142.5865\n",
      "Epoch [4/100], Step [700/13958], Loss: 360.5270\n",
      "Epoch [4/100], Step [800/13958], Loss: 583.7857\n",
      "Epoch [4/100], Step [900/13958], Loss: 305.6882\n",
      "Epoch [4/100], Step [1000/13958], Loss: 337.2873\n",
      "Epoch [4/100], Step [1100/13958], Loss: 452.7939\n",
      "Epoch [4/100], Step [1200/13958], Loss: 291.7057\n",
      "Epoch [4/100], Step [1300/13958], Loss: 371.8752\n",
      "Epoch [4/100], Step [1400/13958], Loss: 242.5682\n",
      "Epoch [4/100], Step [1500/13958], Loss: 213.7049\n",
      "Epoch [4/100], Step [1600/13958], Loss: 158.1593\n",
      "Epoch [4/100], Step [1700/13958], Loss: 171.6590\n",
      "Epoch [4/100], Step [1800/13958], Loss: 221.5152\n",
      "Epoch [4/100], Step [1900/13958], Loss: 239.2367\n",
      "Epoch [4/100], Step [2000/13958], Loss: 659.5793\n",
      "Epoch [4/100], Step [2100/13958], Loss: 466.8197\n",
      "Epoch [4/100], Step [2200/13958], Loss: 257.6797\n",
      "Epoch [4/100], Step [2300/13958], Loss: 473.3057\n",
      "Epoch [4/100], Step [2400/13958], Loss: 81.7937\n",
      "Epoch [4/100], Step [2500/13958], Loss: 447.4932\n",
      "Epoch [4/100], Step [2600/13958], Loss: 469.5312\n",
      "Epoch [4/100], Step [2700/13958], Loss: 65.9566\n",
      "Epoch [4/100], Step [2800/13958], Loss: 141.8773\n",
      "Epoch [4/100], Step [2900/13958], Loss: 206.5027\n",
      "Epoch [4/100], Step [3000/13958], Loss: 251.1226\n",
      "Epoch [4/100], Step [3100/13958], Loss: 210.6798\n",
      "Epoch [4/100], Step [3200/13958], Loss: 268.6907\n",
      "Epoch [4/100], Step [3300/13958], Loss: 308.5320\n",
      "Epoch [4/100], Step [3400/13958], Loss: 141.2841\n",
      "Epoch [4/100], Step [3500/13958], Loss: 534.1584\n",
      "Epoch [4/100], Step [3600/13958], Loss: 271.1640\n",
      "Epoch [4/100], Step [3700/13958], Loss: 452.3585\n",
      "Epoch [4/100], Step [3800/13958], Loss: 123.3180\n",
      "Epoch [4/100], Step [3900/13958], Loss: 411.5969\n",
      "Epoch [4/100], Step [4000/13958], Loss: 359.3690\n",
      "Epoch [4/100], Step [4100/13958], Loss: 229.9848\n",
      "Epoch [4/100], Step [4200/13958], Loss: 252.5148\n",
      "Epoch [4/100], Step [4300/13958], Loss: 245.5901\n",
      "Epoch [4/100], Step [4400/13958], Loss: 293.3841\n",
      "Epoch [4/100], Step [4500/13958], Loss: 173.9723\n",
      "Epoch [4/100], Step [4600/13958], Loss: 415.7459\n",
      "Epoch [4/100], Step [4700/13958], Loss: 288.8942\n",
      "Epoch [4/100], Step [4800/13958], Loss: 201.9775\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [4/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [5/100], Step [100/13958], Loss: 264.3141\n",
      "Epoch [5/100], Step [200/13958], Loss: 387.8656\n",
      "Epoch [5/100], Step [300/13958], Loss: 210.8545\n",
      "Epoch [5/100], Step [400/13958], Loss: 240.0349\n",
      "Epoch [5/100], Step [500/13958], Loss: 200.8167\n",
      "Epoch [5/100], Step [600/13958], Loss: 130.4948\n",
      "Epoch [5/100], Step [700/13958], Loss: 311.6560\n",
      "Epoch [5/100], Step [800/13958], Loss: 342.8235\n",
      "Epoch [5/100], Step [900/13958], Loss: 471.3594\n",
      "Epoch [5/100], Step [1000/13958], Loss: 137.9268\n",
      "Epoch [5/100], Step [1100/13958], Loss: 398.2043\n",
      "Epoch [5/100], Step [1200/13958], Loss: 386.8357\n",
      "Epoch [5/100], Step [1300/13958], Loss: 240.5637\n",
      "Epoch [5/100], Step [1400/13958], Loss: 517.6876\n",
      "Epoch [5/100], Step [1500/13958], Loss: 191.0301\n",
      "Epoch [5/100], Step [1600/13958], Loss: 86.2639\n",
      "Epoch [5/100], Step [1700/13958], Loss: 268.9138\n",
      "Epoch [5/100], Step [1800/13958], Loss: 238.0917\n",
      "Epoch [5/100], Step [1900/13958], Loss: 309.3012\n",
      "Epoch [5/100], Step [2000/13958], Loss: 356.7957\n",
      "Epoch [5/100], Step [2100/13958], Loss: 337.9500\n",
      "Epoch [5/100], Step [2200/13958], Loss: 113.1684\n",
      "Epoch [5/100], Step [2300/13958], Loss: 132.3711\n",
      "Epoch [5/100], Step [2400/13958], Loss: 388.6436\n",
      "Epoch [5/100], Step [2500/13958], Loss: 163.0042\n",
      "Epoch [5/100], Step [2600/13958], Loss: 299.9305\n",
      "Epoch [5/100], Step [2700/13958], Loss: 272.3780\n",
      "Epoch [5/100], Step [2800/13958], Loss: 257.4114\n",
      "Epoch [5/100], Step [2900/13958], Loss: 245.8710\n",
      "Epoch [5/100], Step [3000/13958], Loss: 594.6848\n",
      "Epoch [5/100], Step [3100/13958], Loss: 101.6724\n",
      "Epoch [5/100], Step [3200/13958], Loss: 332.9021\n",
      "Epoch [5/100], Step [3300/13958], Loss: 276.9408\n",
      "Epoch [5/100], Step [3400/13958], Loss: 169.1997\n",
      "Epoch [5/100], Step [3500/13958], Loss: 202.0163\n",
      "Epoch [5/100], Step [3600/13958], Loss: 409.2107\n",
      "Epoch [5/100], Step [3700/13958], Loss: 297.1819\n",
      "Epoch [5/100], Step [3800/13958], Loss: 215.1647\n",
      "Epoch [5/100], Step [3900/13958], Loss: 171.3256\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [5/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [6/100], Step [100/13958], Loss: 312.8355\n",
      "Epoch [6/100], Step [200/13958], Loss: 315.2794\n",
      "Epoch [6/100], Step [300/13958], Loss: 197.3259\n",
      "Epoch [6/100], Step [400/13958], Loss: 118.6988\n",
      "Epoch [6/100], Step [500/13958], Loss: 345.5562\n",
      "Epoch [6/100], Step [600/13958], Loss: 191.0698\n",
      "Epoch [6/100], Step [700/13958], Loss: 157.1745\n",
      "Epoch [6/100], Step [800/13958], Loss: 310.8330\n",
      "Epoch [6/100], Step [900/13958], Loss: 201.6817\n",
      "Epoch [6/100], Step [1000/13958], Loss: 774.0550\n",
      "Epoch [6/100], Step [1100/13958], Loss: 186.4626\n",
      "Epoch [6/100], Step [1200/13958], Loss: 263.4249\n",
      "Epoch [6/100], Step [1300/13958], Loss: 186.5759\n",
      "Epoch [6/100], Step [1400/13958], Loss: 169.6552\n",
      "Epoch [6/100], Step [1500/13958], Loss: 75.1040\n",
      "Epoch [6/100], Step [1600/13958], Loss: 453.1121\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [6/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [7/100], Step [100/13958], Loss: 213.0111\n",
      "Epoch [7/100], Step [200/13958], Loss: 389.0157\n",
      "Epoch [7/100], Step [300/13958], Loss: 122.1824\n",
      "Epoch [7/100], Step [400/13958], Loss: 195.3510\n",
      "Epoch [7/100], Step [500/13958], Loss: 500.2639\n",
      "Epoch [7/100], Step [600/13958], Loss: 148.8654\n",
      "Epoch [7/100], Step [700/13958], Loss: 322.5204\n",
      "Epoch [7/100], Step [800/13958], Loss: 197.8885\n",
      "Epoch [7/100], Step [900/13958], Loss: 304.9247\n",
      "Epoch [7/100], Step [1000/13958], Loss: 448.6229\n",
      "Epoch [7/100], Step [1100/13958], Loss: 244.4306\n",
      "Epoch [7/100], Step [1200/13958], Loss: 470.6453\n",
      "Epoch [7/100], Step [1300/13958], Loss: 131.3127\n",
      "Epoch [7/100], Step [1400/13958], Loss: 214.3305\n",
      "Epoch [7/100], Step [1500/13958], Loss: 469.7631\n",
      "Epoch [7/100], Step [1600/13958], Loss: 183.0214\n",
      "Epoch [7/100], Step [1700/13958], Loss: 242.6076\n",
      "Epoch [7/100], Step [1800/13958], Loss: 274.5160\n",
      "Epoch [7/100], Step [1900/13958], Loss: 122.2280\n",
      "Epoch [7/100], Step [2000/13958], Loss: 303.2423\n",
      "Epoch [7/100], Step [2100/13958], Loss: 193.5071\n",
      "Epoch [7/100], Step [2200/13958], Loss: 199.2709\n",
      "Epoch [7/100], Step [2300/13958], Loss: 193.0583\n",
      "Epoch [7/100], Step [2400/13958], Loss: 426.8748\n",
      "Epoch [7/100], Step [2500/13958], Loss: 230.0751\n",
      "Epoch [7/100], Step [2600/13958], Loss: 220.9240\n",
      "Epoch [7/100], Step [2700/13958], Loss: 214.5095\n",
      "Epoch [7/100], Step [2800/13958], Loss: 623.5055\n",
      "Epoch [7/100], Step [2900/13958], Loss: 350.5202\n",
      "Epoch [7/100], Step [3000/13958], Loss: 418.8840\n",
      "Epoch [7/100], Step [3100/13958], Loss: 233.7692\n",
      "Epoch [7/100], Step [3200/13958], Loss: 281.7822\n",
      "Epoch [7/100], Step [3300/13958], Loss: 484.3748\n",
      "Epoch [7/100], Step [3400/13958], Loss: 175.8006\n",
      "Epoch [7/100], Step [3500/13958], Loss: 217.2243\n",
      "Epoch [7/100], Step [3600/13958], Loss: 105.6037\n",
      "Epoch [7/100], Step [3700/13958], Loss: 188.2567\n",
      "Epoch [7/100], Step [3800/13958], Loss: 104.3278\n",
      "Epoch [7/100], Step [3900/13958], Loss: 452.9579\n",
      "Epoch [7/100], Step [4000/13958], Loss: 292.8792\n",
      "Epoch [7/100], Step [4100/13958], Loss: 194.8034\n",
      "Epoch [7/100], Step [4200/13958], Loss: 185.8244\n",
      "Epoch [7/100], Step [4300/13958], Loss: 215.5618\n",
      "Epoch [7/100], Step [4400/13958], Loss: 216.2364\n",
      "Epoch [7/100], Step [4500/13958], Loss: 205.6281\n",
      "Epoch [7/100], Step [4600/13958], Loss: 68.6194\n",
      "Epoch [7/100], Step [4700/13958], Loss: 89.6837\n",
      "Epoch [7/100], Step [4800/13958], Loss: 162.8174\n",
      "Epoch [7/100], Step [4900/13958], Loss: 183.2181\n",
      "Epoch [7/100], Step [5000/13958], Loss: 686.9784\n",
      "Epoch [7/100], Step [5100/13958], Loss: 225.4394\n",
      "Epoch [7/100], Step [5200/13958], Loss: 180.3042\n",
      "Epoch [7/100], Step [5300/13958], Loss: 185.8084\n",
      "Epoch [7/100], Step [5400/13958], Loss: 258.8808\n",
      "Epoch [7/100], Step [5500/13958], Loss: 330.6970\n",
      "Epoch [7/100], Step [5600/13958], Loss: 92.8820\n",
      "Epoch [7/100], Step [5700/13958], Loss: 180.0547\n",
      "Epoch [7/100], Step [5800/13958], Loss: 494.7734\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [7/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [8/100], Step [100/13958], Loss: 254.0144\n",
      "Epoch [8/100], Step [200/13958], Loss: 332.1485\n",
      "Epoch [8/100], Step [300/13958], Loss: 224.4027\n",
      "Epoch [8/100], Step [400/13958], Loss: 384.4609\n",
      "Epoch [8/100], Step [500/13958], Loss: 239.2417\n",
      "Epoch [8/100], Step [600/13958], Loss: 123.2067\n",
      "Epoch [8/100], Step [700/13958], Loss: 434.1765\n",
      "Epoch [8/100], Step [800/13958], Loss: 79.4571\n",
      "Epoch [8/100], Step [900/13958], Loss: 147.7575\n",
      "Epoch [8/100], Step [1000/13958], Loss: 300.8065\n",
      "Epoch [8/100], Step [1100/13958], Loss: 252.1633\n",
      "Epoch [8/100], Step [1200/13958], Loss: 353.1756\n",
      "Epoch [8/100], Step [1300/13958], Loss: 430.5118\n",
      "Epoch [8/100], Step [1400/13958], Loss: 158.5396\n",
      "Epoch [8/100], Step [1500/13958], Loss: 292.8777\n",
      "Epoch [8/100], Step [1600/13958], Loss: 285.9604\n",
      "Epoch [8/100], Step [1700/13958], Loss: 349.7672\n",
      "Epoch [8/100], Step [1800/13958], Loss: 326.5538\n",
      "Epoch [8/100], Step [1900/13958], Loss: 187.5762\n",
      "Epoch [8/100], Step [2000/13958], Loss: 177.3658\n",
      "Epoch [8/100], Step [2100/13958], Loss: 243.3255\n",
      "Epoch [8/100], Step [2200/13958], Loss: 51.9450\n",
      "Epoch [8/100], Step [2300/13958], Loss: 286.2506\n",
      "Epoch [8/100], Step [2400/13958], Loss: 178.0826\n",
      "Epoch [8/100], Step [2500/13958], Loss: 384.2260\n",
      "Epoch [8/100], Step [2600/13958], Loss: 462.1929\n",
      "Epoch [8/100], Step [2700/13958], Loss: 285.6246\n",
      "Epoch [8/100], Step [2800/13958], Loss: 134.5305\n",
      "Epoch [8/100], Step [2900/13958], Loss: 244.6604\n",
      "Epoch [8/100], Step [3000/13958], Loss: 257.0582\n",
      "Epoch [8/100], Step [3100/13958], Loss: 338.8933\n",
      "Epoch [8/100], Step [3200/13958], Loss: 310.8490\n",
      "Epoch [8/100], Step [3300/13958], Loss: 70.2143\n",
      "Epoch [8/100], Step [3400/13958], Loss: 295.2623\n",
      "Epoch [8/100], Step [3500/13958], Loss: 224.9515\n",
      "Epoch [8/100], Step [3600/13958], Loss: 164.9391\n",
      "Epoch [8/100], Step [3700/13958], Loss: 327.5061\n",
      "Epoch [8/100], Step [3800/13958], Loss: 197.0280\n",
      "Epoch [8/100], Step [3900/13958], Loss: 181.2785\n",
      "Epoch [8/100], Step [4000/13958], Loss: 202.5002\n",
      "Epoch [8/100], Step [4100/13958], Loss: 155.0567\n",
      "Epoch [8/100], Step [4200/13958], Loss: 233.7461\n",
      "Epoch [8/100], Step [4300/13958], Loss: 232.7662\n",
      "Epoch [8/100], Step [4400/13958], Loss: 285.0558\n",
      "Epoch [8/100], Step [4500/13958], Loss: 319.7045\n",
      "Epoch [8/100], Step [4600/13958], Loss: 593.9672\n",
      "Epoch [8/100], Step [4700/13958], Loss: 275.6497\n",
      "Epoch [8/100], Step [4800/13958], Loss: 248.1410\n",
      "Epoch [8/100], Step [4900/13958], Loss: 130.6274\n",
      "Epoch [8/100], Step [5000/13958], Loss: 242.7277\n",
      "Epoch [8/100], Step [5100/13958], Loss: 245.8861\n",
      "Epoch [8/100], Step [5200/13958], Loss: 218.1826\n",
      "Epoch [8/100], Step [5300/13958], Loss: 275.9203\n",
      "Epoch [8/100], Step [5400/13958], Loss: 353.4391\n",
      "Epoch [8/100], Step [5500/13958], Loss: 264.9574\n",
      "Epoch [8/100], Step [5600/13958], Loss: 336.6031\n",
      "Epoch [8/100], Step [5700/13958], Loss: 305.6840\n",
      "Epoch [8/100], Step [5800/13958], Loss: 216.2506\n",
      "Epoch [8/100], Step [5900/13958], Loss: 169.2616\n",
      "Epoch [8/100], Step [6000/13958], Loss: 242.2049\n",
      "Epoch [8/100], Step [6100/13958], Loss: 95.9319\n",
      "Epoch [8/100], Step [6200/13958], Loss: 479.6962\n",
      "Epoch [8/100], Step [6300/13958], Loss: 147.4793\n",
      "Epoch [8/100], Step [6400/13958], Loss: 74.0103\n",
      "Epoch [8/100], Step [6500/13958], Loss: 172.8036\n",
      "Epoch [8/100], Step [6600/13958], Loss: 394.7536\n",
      "Epoch [8/100], Step [6700/13958], Loss: 379.2028\n",
      "Epoch [8/100], Step [6800/13958], Loss: 475.5474\n",
      "Epoch [8/100], Step [6900/13958], Loss: 264.6978\n",
      "Epoch [8/100], Step [7000/13958], Loss: 322.6427\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [8/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [9/100], Step [100/13958], Loss: 187.2229\n",
      "Epoch [9/100], Step [200/13958], Loss: 223.5284\n",
      "Epoch [9/100], Step [300/13958], Loss: 441.2902\n",
      "Epoch [9/100], Step [400/13958], Loss: 144.8087\n",
      "Epoch [9/100], Step [500/13958], Loss: 487.6956\n",
      "Epoch [9/100], Step [600/13958], Loss: 217.6623\n",
      "Epoch [9/100], Step [700/13958], Loss: 193.1763\n",
      "Epoch [9/100], Step [800/13958], Loss: 363.6662\n",
      "Epoch [9/100], Step [900/13958], Loss: 327.8577\n",
      "Epoch [9/100], Step [1000/13958], Loss: 333.2123\n",
      "Epoch [9/100], Step [1100/13958], Loss: 303.3875\n",
      "Epoch [9/100], Step [1200/13958], Loss: 40.5826\n",
      "Epoch [9/100], Step [1300/13958], Loss: 289.1946\n",
      "Epoch [9/100], Step [1400/13958], Loss: 153.0769\n",
      "Epoch [9/100], Step [1500/13958], Loss: 134.6529\n",
      "Epoch [9/100], Step [1600/13958], Loss: 373.2454\n",
      "Epoch [9/100], Step [1700/13958], Loss: 354.7644\n",
      "Epoch [9/100], Step [1800/13958], Loss: 124.3420\n",
      "Epoch [9/100], Step [1900/13958], Loss: 334.3318\n",
      "Epoch [9/100], Step [2000/13958], Loss: 170.0495\n",
      "Epoch [9/100], Step [2100/13958], Loss: 204.1275\n",
      "Epoch [9/100], Step [2200/13958], Loss: 333.1827\n",
      "Epoch [9/100], Step [2300/13958], Loss: 397.9372\n",
      "Epoch [9/100], Step [2400/13958], Loss: 91.8663\n",
      "Epoch [9/100], Step [2500/13958], Loss: 137.6077\n",
      "Epoch [9/100], Step [2600/13958], Loss: 196.2718\n",
      "Epoch [9/100], Step [2700/13958], Loss: 215.8811\n",
      "Epoch [9/100], Step [2800/13958], Loss: 127.0767\n",
      "Epoch [9/100], Step [2900/13958], Loss: 121.6719\n",
      "Epoch [9/100], Step [3000/13958], Loss: 347.0079\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [9/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [10/100], Step [100/13958], Loss: 202.2953\n",
      "Epoch [10/100], Step [200/13958], Loss: 395.0695\n",
      "Epoch [10/100], Step [300/13958], Loss: 225.7383\n",
      "Epoch [10/100], Step [400/13958], Loss: 361.3520\n",
      "Epoch [10/100], Step [500/13958], Loss: 491.1395\n",
      "Epoch [10/100], Step [600/13958], Loss: 178.4122\n",
      "Epoch [10/100], Step [700/13958], Loss: 217.6083\n",
      "Epoch [10/100], Step [800/13958], Loss: 126.0232\n",
      "Epoch [10/100], Step [900/13958], Loss: 163.3870\n",
      "Epoch [10/100], Step [1000/13958], Loss: 259.8611\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [10/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [11/100], Step [100/13958], Loss: 418.6586\n",
      "Epoch [11/100], Step [200/13958], Loss: 161.3703\n",
      "Epoch [11/100], Step [300/13958], Loss: 241.1638\n",
      "Epoch [11/100], Step [400/13958], Loss: 427.6245\n",
      "Epoch [11/100], Step [500/13958], Loss: 304.4103\n",
      "Epoch [11/100], Step [600/13958], Loss: 171.4364\n",
      "Epoch [11/100], Step [700/13958], Loss: 116.6339\n",
      "Epoch [11/100], Step [800/13958], Loss: 255.3086\n",
      "Epoch [11/100], Step [900/13958], Loss: 163.1862\n",
      "Epoch [11/100], Step [1000/13958], Loss: 348.7167\n",
      "Epoch [11/100], Step [1100/13958], Loss: 151.3106\n",
      "Epoch [11/100], Step [1200/13958], Loss: 303.9203\n",
      "Epoch [11/100], Step [1300/13958], Loss: 216.9839\n",
      "Epoch [11/100], Step [1400/13958], Loss: 248.5401\n",
      "Epoch [11/100], Step [1500/13958], Loss: 209.5270\n",
      "Epoch [11/100], Step [1600/13958], Loss: 232.2276\n",
      "Epoch [11/100], Step [1700/13958], Loss: 87.9841\n",
      "Epoch [11/100], Step [1800/13958], Loss: 326.5778\n",
      "Epoch [11/100], Step [1900/13958], Loss: 121.5336\n",
      "Epoch [11/100], Step [2000/13958], Loss: 265.6282\n",
      "Epoch [11/100], Step [2100/13958], Loss: 379.9162\n",
      "Epoch [11/100], Step [2200/13958], Loss: 331.6324\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [11/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [12/100], Step [100/13958], Loss: 319.4615\n",
      "Epoch [12/100], Step [200/13958], Loss: 159.1452\n",
      "Epoch [12/100], Step [300/13958], Loss: 137.5950\n",
      "Epoch [12/100], Step [400/13958], Loss: 416.3587\n",
      "Epoch [12/100], Step [500/13958], Loss: 182.4579\n",
      "Epoch [12/100], Step [600/13958], Loss: 246.5495\n",
      "Epoch [12/100], Step [700/13958], Loss: 414.3589\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [12/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [13/100], Step [100/13958], Loss: 288.4338\n",
      "Epoch [13/100], Step [200/13958], Loss: 370.6307\n",
      "Epoch [13/100], Step [300/13958], Loss: 152.1432\n",
      "Epoch [13/100], Step [400/13958], Loss: 127.8382\n",
      "Epoch [13/100], Step [500/13958], Loss: 166.7717\n",
      "Epoch [13/100], Step [600/13958], Loss: 27.4099\n",
      "Epoch [13/100], Step [700/13958], Loss: 265.5341\n",
      "Epoch [13/100], Step [800/13958], Loss: 69.1283\n",
      "Epoch [13/100], Step [900/13958], Loss: 154.5269\n",
      "Epoch [13/100], Step [1000/13958], Loss: 216.5182\n",
      "Epoch [13/100], Step [1100/13958], Loss: 239.9405\n",
      "Epoch [13/100], Step [1200/13958], Loss: 154.9137\n",
      "Epoch [13/100], Step [1300/13958], Loss: 522.3409\n",
      "Epoch [13/100], Step [1400/13958], Loss: 153.8887\n",
      "Epoch [13/100], Step [1500/13958], Loss: 368.4649\n",
      "Epoch [13/100], Step [1600/13958], Loss: 247.4884\n",
      "Epoch [13/100], Step [1700/13958], Loss: 313.2657\n",
      "Epoch [13/100], Step [1800/13958], Loss: 117.0609\n",
      "Epoch [13/100], Step [1900/13958], Loss: 391.3072\n",
      "Epoch [13/100], Step [2000/13958], Loss: 127.9296\n",
      "Epoch [13/100], Step [2100/13958], Loss: 245.0455\n",
      "Epoch [13/100], Step [2200/13958], Loss: 365.3485\n",
      "Epoch [13/100], Step [2300/13958], Loss: 256.2101\n",
      "Epoch [13/100], Step [2400/13958], Loss: 317.9190\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [13/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [14/100], Step [100/13958], Loss: 178.1635\n",
      "Epoch [14/100], Step [200/13958], Loss: 178.8707\n",
      "Epoch [14/100], Step [300/13958], Loss: 300.4406\n",
      "Epoch [14/100], Step [400/13958], Loss: 203.1918\n",
      "Epoch [14/100], Step [500/13958], Loss: 245.0772\n",
      "Epoch [14/100], Step [600/13958], Loss: 309.3694\n",
      "Epoch [14/100], Step [700/13958], Loss: 379.8027\n",
      "Epoch [14/100], Step [800/13958], Loss: 225.6422\n",
      "Epoch [14/100], Step [900/13958], Loss: 241.1702\n",
      "Epoch [14/100], Step [1000/13958], Loss: 202.7473\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [14/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [15/100], Step [100/13958], Loss: 226.6532\n",
      "Epoch [15/100], Step [200/13958], Loss: 388.9955\n",
      "Epoch [15/100], Step [300/13958], Loss: 323.0446\n",
      "Epoch [15/100], Step [400/13958], Loss: 135.5116\n",
      "Epoch [15/100], Step [500/13958], Loss: 260.7563\n",
      "Epoch [15/100], Step [600/13958], Loss: 322.4759\n",
      "Epoch [15/100], Step [700/13958], Loss: 354.2906\n",
      "Epoch [15/100], Step [800/13958], Loss: 167.5281\n",
      "Epoch [15/100], Step [900/13958], Loss: 258.4720\n",
      "Epoch [15/100], Step [1000/13958], Loss: 172.1894\n",
      "Epoch [15/100], Step [1100/13958], Loss: 58.4459\n",
      "Epoch [15/100], Step [1200/13958], Loss: 299.6000\n",
      "Epoch [15/100], Step [1300/13958], Loss: 244.5387\n",
      "Epoch [15/100], Step [1400/13958], Loss: 274.5702\n",
      "Epoch [15/100], Step [1500/13958], Loss: 210.9930\n",
      "Epoch [15/100], Step [1600/13958], Loss: 147.6489\n",
      "Epoch [15/100], Step [1700/13958], Loss: 189.0404\n",
      "Epoch [15/100], Step [1800/13958], Loss: 186.2194\n",
      "Epoch [15/100], Step [1900/13958], Loss: 488.9103\n",
      "Epoch [15/100], Step [2000/13958], Loss: 291.8961\n",
      "Epoch [15/100], Step [2100/13958], Loss: 510.1720\n",
      "Epoch [15/100], Step [2200/13958], Loss: 306.2294\n",
      "Epoch [15/100], Step [2300/13958], Loss: 153.2861\n",
      "Epoch [15/100], Step [2400/13958], Loss: 246.7561\n",
      "Epoch [15/100], Step [2500/13958], Loss: 241.0470\n",
      "Epoch [15/100], Step [2600/13958], Loss: 263.7549\n",
      "Epoch [15/100], Step [2700/13958], Loss: 367.3246\n",
      "Epoch [15/100], Step [2800/13958], Loss: 223.7325\n",
      "Epoch [15/100], Step [2900/13958], Loss: 138.5896\n",
      "Epoch [15/100], Step [3000/13958], Loss: 93.6581\n",
      "Epoch [15/100], Step [3100/13958], Loss: 303.2832\n",
      "Epoch [15/100], Step [3200/13958], Loss: 294.1039\n",
      "Epoch [15/100], Step [3300/13958], Loss: 215.9467\n",
      "Epoch [15/100], Step [3400/13958], Loss: 220.4521\n",
      "Epoch [15/100], Step [3500/13958], Loss: 114.0301\n",
      "Epoch [15/100], Step [3600/13958], Loss: 429.6568\n",
      "Epoch [15/100], Step [3700/13958], Loss: 144.9613\n",
      "Epoch [15/100], Step [3800/13958], Loss: 159.3436\n",
      "Epoch [15/100], Step [3900/13958], Loss: 345.9242\n",
      "Epoch [15/100], Step [4000/13958], Loss: 287.8426\n",
      "Epoch [15/100], Step [4100/13958], Loss: 224.0091\n",
      "Epoch [15/100], Step [4200/13958], Loss: 94.8857\n",
      "Epoch [15/100], Step [4300/13958], Loss: 246.5574\n",
      "Epoch [15/100], Step [4400/13958], Loss: 167.6096\n",
      "Epoch [15/100], Step [4500/13958], Loss: 386.2245\n",
      "Epoch [15/100], Step [4600/13958], Loss: 516.2644\n",
      "Epoch [15/100], Step [4700/13958], Loss: 237.1287\n",
      "Epoch [15/100], Step [4800/13958], Loss: 353.6814\n",
      "Epoch [15/100], Step [4900/13958], Loss: 156.7390\n",
      "Epoch [15/100], Step [5000/13958], Loss: 116.7860\n",
      "Epoch [15/100], Step [5100/13958], Loss: 334.5269\n",
      "Epoch [15/100], Step [5200/13958], Loss: 230.5369\n",
      "Epoch [15/100], Step [5300/13958], Loss: 272.4418\n",
      "Epoch [15/100], Step [5400/13958], Loss: 500.4266\n",
      "Epoch [15/100], Step [5500/13958], Loss: 176.6755\n",
      "Epoch [15/100], Step [5600/13958], Loss: 108.6350\n",
      "Epoch [15/100], Step [5700/13958], Loss: 431.9931\n",
      "Epoch [15/100], Step [5800/13958], Loss: 259.5737\n",
      "Epoch [15/100], Step [5900/13958], Loss: 310.1353\n",
      "Epoch [15/100], Step [6000/13958], Loss: 436.2319\n",
      "Epoch [15/100], Step [6100/13958], Loss: 266.1500\n",
      "Epoch [15/100], Step [6200/13958], Loss: 59.5091\n",
      "Epoch [15/100], Step [6300/13958], Loss: 285.1220\n",
      "Epoch [15/100], Step [6400/13958], Loss: 574.2230\n",
      "Epoch [15/100], Step [6500/13958], Loss: 109.6808\n",
      "Epoch [15/100], Step [6600/13958], Loss: 121.9783\n",
      "Epoch [15/100], Step [6700/13958], Loss: 271.3717\n",
      "Epoch [15/100], Step [6800/13958], Loss: 141.1391\n",
      "Epoch [15/100], Step [6900/13958], Loss: 263.2537\n",
      "Epoch [15/100], Step [7000/13958], Loss: 146.1151\n",
      "Epoch [15/100], Step [7100/13958], Loss: 332.0595\n",
      "Epoch [15/100], Step [7200/13958], Loss: 196.1106\n",
      "Epoch [15/100], Step [7300/13958], Loss: 273.9752\n",
      "Epoch [15/100], Step [7400/13958], Loss: 634.0594\n",
      "Epoch [15/100], Step [7500/13958], Loss: 173.2569\n",
      "Epoch [15/100], Step [7600/13958], Loss: 343.3311\n",
      "Epoch [15/100], Step [7700/13958], Loss: 168.9912\n",
      "Epoch [15/100], Step [7800/13958], Loss: 73.4005\n",
      "Epoch [15/100], Step [7900/13958], Loss: 86.8691\n",
      "Epoch [15/100], Step [8000/13958], Loss: 212.8486\n",
      "Epoch [15/100], Step [8100/13958], Loss: 196.3522\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [15/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [16/100], Step [100/13958], Loss: 280.6676\n",
      "Epoch [16/100], Step [200/13958], Loss: 171.5836\n",
      "Epoch [16/100], Step [300/13958], Loss: 341.1010\n",
      "Epoch [16/100], Step [400/13958], Loss: 257.8057\n",
      "Epoch [16/100], Step [500/13958], Loss: 262.6312\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [16/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [17/100], Step [100/13958], Loss: 113.8089\n",
      "Epoch [17/100], Step [200/13958], Loss: 499.5764\n",
      "Epoch [17/100], Step [300/13958], Loss: 320.4910\n",
      "Epoch [17/100], Step [400/13958], Loss: 168.5758\n",
      "Epoch [17/100], Step [500/13958], Loss: 262.4745\n",
      "Epoch [17/100], Step [600/13958], Loss: 191.2592\n",
      "Epoch [17/100], Step [700/13958], Loss: 83.2282\n",
      "Epoch [17/100], Step [800/13958], Loss: 247.4163\n",
      "Epoch [17/100], Step [900/13958], Loss: 487.1828\n",
      "Epoch [17/100], Step [1000/13958], Loss: 305.5738\n",
      "Epoch [17/100], Step [1100/13958], Loss: 398.6936\n",
      "Epoch [17/100], Step [1200/13958], Loss: 367.8698\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [17/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [18/100], Step [100/13958], Loss: 239.2569\n",
      "Epoch [18/100], Step [200/13958], Loss: 208.6152\n",
      "Epoch [18/100], Step [300/13958], Loss: 338.0403\n",
      "Epoch [18/100], Step [400/13958], Loss: 527.5953\n",
      "Epoch [18/100], Step [500/13958], Loss: 269.5443\n",
      "Epoch [18/100], Step [600/13958], Loss: 460.3757\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [18/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [19/100], Step [100/13958], Loss: 292.8247\n",
      "Epoch [19/100], Step [200/13958], Loss: 297.3160\n",
      "Epoch [19/100], Step [300/13958], Loss: 293.9581\n",
      "Epoch [19/100], Step [400/13958], Loss: 387.9782\n",
      "Epoch [19/100], Step [500/13958], Loss: 261.3215\n",
      "Epoch [19/100], Step [600/13958], Loss: 257.8731\n",
      "Epoch [19/100], Step [700/13958], Loss: 329.7927\n",
      "Epoch [19/100], Step [800/13958], Loss: 357.8339\n",
      "Epoch [19/100], Step [900/13958], Loss: 511.1666\n",
      "Epoch [19/100], Step [1000/13958], Loss: 222.0473\n",
      "Epoch [19/100], Step [1100/13958], Loss: 351.8159\n",
      "Epoch [19/100], Step [1200/13958], Loss: 341.8989\n",
      "Epoch [19/100], Step [1300/13958], Loss: 286.1443\n",
      "Epoch [19/100], Step [1400/13958], Loss: 285.6613\n",
      "Epoch [19/100], Step [1500/13958], Loss: 278.8219\n",
      "Epoch [19/100], Step [1600/13958], Loss: 207.2501\n",
      "Epoch [19/100], Step [1700/13958], Loss: 308.0106\n",
      "Epoch [19/100], Step [1800/13958], Loss: 410.5057\n",
      "Epoch [19/100], Step [1900/13958], Loss: 185.8232\n",
      "Epoch [19/100], Step [2000/13958], Loss: 308.7537\n",
      "Epoch [19/100], Step [2100/13958], Loss: 310.4158\n",
      "Epoch [19/100], Step [2200/13958], Loss: 536.8284\n",
      "Epoch [19/100], Step [2300/13958], Loss: 225.1980\n",
      "Epoch [19/100], Step [2400/13958], Loss: 126.0977\n",
      "Epoch [19/100], Step [2500/13958], Loss: 103.9889\n",
      "Epoch [19/100], Step [2600/13958], Loss: 299.9861\n",
      "Epoch [19/100], Step [2700/13958], Loss: 231.8810\n",
      "Epoch [19/100], Step [2800/13958], Loss: 385.1054\n",
      "Epoch [19/100], Step [2900/13958], Loss: 248.1367\n",
      "Epoch [19/100], Step [3000/13958], Loss: 130.2608\n",
      "Epoch [19/100], Step [3100/13958], Loss: 469.9254\n",
      "Epoch [19/100], Step [3200/13958], Loss: 292.6187\n",
      "Epoch [19/100], Step [3300/13958], Loss: 417.2078\n",
      "Epoch [19/100], Step [3400/13958], Loss: 279.5896\n",
      "Epoch [19/100], Step [3500/13958], Loss: 612.0519\n",
      "Epoch [19/100], Step [3600/13958], Loss: 248.6106\n",
      "Epoch [19/100], Step [3700/13958], Loss: 161.4292\n",
      "Epoch [19/100], Step [3800/13958], Loss: 197.0367\n",
      "Epoch [19/100], Step [3900/13958], Loss: 235.7413\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [19/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [20/100], Step [100/13958], Loss: 200.2334\n",
      "Epoch [20/100], Step [200/13958], Loss: 169.7169\n",
      "Epoch [20/100], Step [300/13958], Loss: 246.9730\n",
      "Epoch [20/100], Step [400/13958], Loss: 314.9808\n",
      "Epoch [20/100], Step [500/13958], Loss: 229.8586\n",
      "Epoch [20/100], Step [600/13958], Loss: 120.9067\n",
      "Epoch [20/100], Step [700/13958], Loss: 157.0127\n",
      "Epoch [20/100], Step [800/13958], Loss: 231.6646\n",
      "Epoch [20/100], Step [900/13958], Loss: 511.1087\n",
      "Epoch [20/100], Step [1000/13958], Loss: 359.0750\n",
      "Epoch [20/100], Step [1100/13958], Loss: 316.4181\n",
      "Epoch [20/100], Step [1200/13958], Loss: 452.1396\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [20/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [21/100], Step [100/13958], Loss: 581.8197\n",
      "Epoch [21/100], Step [200/13958], Loss: 155.3785\n",
      "Epoch [21/100], Step [300/13958], Loss: 252.3919\n",
      "Epoch [21/100], Step [400/13958], Loss: 359.2995\n",
      "Epoch [21/100], Step [500/13958], Loss: 248.7936\n",
      "Epoch [21/100], Step [600/13958], Loss: 165.0157\n",
      "Epoch [21/100], Step [700/13958], Loss: 222.6416\n",
      "Epoch [21/100], Step [800/13958], Loss: 598.7116\n",
      "Epoch [21/100], Step [900/13958], Loss: 340.8811\n",
      "Epoch [21/100], Step [1000/13958], Loss: 367.4780\n",
      "Epoch [21/100], Step [1100/13958], Loss: 501.9603\n",
      "Epoch [21/100], Step [1200/13958], Loss: 338.9185\n",
      "Epoch [21/100], Step [1300/13958], Loss: 298.0472\n",
      "Epoch [21/100], Step [1400/13958], Loss: 360.2019\n",
      "Epoch [21/100], Step [1500/13958], Loss: 276.4768\n",
      "Epoch [21/100], Step [1600/13958], Loss: 430.7739\n",
      "Epoch [21/100], Step [1700/13958], Loss: 334.5619\n",
      "Epoch [21/100], Step [1800/13958], Loss: 540.8228\n",
      "Epoch [21/100], Step [1900/13958], Loss: 292.5262\n",
      "Epoch [21/100], Step [2000/13958], Loss: 405.4738\n",
      "Epoch [21/100], Step [2100/13958], Loss: 144.4595\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [21/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [22/100], Step [100/13958], Loss: 515.4194\n",
      "Epoch [22/100], Step [200/13958], Loss: 215.4721\n",
      "Epoch [22/100], Step [300/13958], Loss: 145.5489\n",
      "Epoch [22/100], Step [400/13958], Loss: 415.9492\n",
      "Epoch [22/100], Step [500/13958], Loss: 252.1038\n",
      "Epoch [22/100], Step [600/13958], Loss: 266.3673\n",
      "Epoch [22/100], Step [700/13958], Loss: 223.6304\n",
      "Epoch [22/100], Step [800/13958], Loss: 256.8250\n",
      "Epoch [22/100], Step [900/13958], Loss: 478.2799\n",
      "Epoch [22/100], Step [1000/13958], Loss: 286.9350\n",
      "Epoch [22/100], Step [1100/13958], Loss: 135.0108\n",
      "Epoch [22/100], Step [1200/13958], Loss: 205.3572\n",
      "Epoch [22/100], Step [1300/13958], Loss: 162.3974\n",
      "Epoch [22/100], Step [1400/13958], Loss: 136.8307\n",
      "Epoch [22/100], Step [1500/13958], Loss: 486.3081\n",
      "Epoch [22/100], Step [1600/13958], Loss: 178.5652\n",
      "Epoch [22/100], Step [1700/13958], Loss: 320.0195\n",
      "Epoch [22/100], Step [1800/13958], Loss: 283.4344\n",
      "Epoch [22/100], Step [1900/13958], Loss: 361.0302\n",
      "Epoch [22/100], Step [2000/13958], Loss: 295.9348\n",
      "Epoch [22/100], Step [2100/13958], Loss: 341.1393\n",
      "Epoch [22/100], Step [2200/13958], Loss: 115.9672\n",
      "Epoch [22/100], Step [2300/13958], Loss: 403.5386\n",
      "Epoch [22/100], Step [2400/13958], Loss: 399.8531\n",
      "Epoch [22/100], Step [2500/13958], Loss: 543.4244\n",
      "Epoch [22/100], Step [2600/13958], Loss: 147.4604\n",
      "Epoch [22/100], Step [2700/13958], Loss: 488.9774\n",
      "Epoch [22/100], Step [2800/13958], Loss: 180.9992\n",
      "Epoch [22/100], Step [2900/13958], Loss: 327.2317\n",
      "Epoch [22/100], Step [3000/13958], Loss: 375.7964\n",
      "Epoch [22/100], Step [3100/13958], Loss: 312.9634\n",
      "Epoch [22/100], Step [3200/13958], Loss: 285.0483\n",
      "Epoch [22/100], Step [3300/13958], Loss: 192.6866\n",
      "Epoch [22/100], Step [3400/13958], Loss: 418.2458\n",
      "Epoch [22/100], Step [3500/13958], Loss: 137.8604\n",
      "Epoch [22/100], Step [3600/13958], Loss: 176.6972\n",
      "Epoch [22/100], Step [3700/13958], Loss: 163.5774\n",
      "Epoch [22/100], Step [3800/13958], Loss: 513.7945\n",
      "Epoch [22/100], Step [3900/13958], Loss: 60.7055\n",
      "Epoch [22/100], Step [4000/13958], Loss: 34.4638\n",
      "Epoch [22/100], Step [4100/13958], Loss: 442.3358\n",
      "Epoch [22/100], Step [4200/13958], Loss: 384.7639\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [22/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [23/100], Step [100/13958], Loss: 217.2283\n",
      "Epoch [23/100], Step [200/13958], Loss: 261.3109\n",
      "Epoch [23/100], Step [300/13958], Loss: 36.0080\n",
      "Epoch [23/100], Step [400/13958], Loss: 85.4163\n",
      "Epoch [23/100], Step [500/13958], Loss: 253.2481\n",
      "Epoch [23/100], Step [600/13958], Loss: 47.6028\n",
      "Epoch [23/100], Step [700/13958], Loss: 326.3670\n",
      "Epoch [23/100], Step [800/13958], Loss: 274.3156\n",
      "Epoch [23/100], Step [900/13958], Loss: 278.9065\n",
      "Epoch [23/100], Step [1000/13958], Loss: 133.3517\n",
      "Epoch [23/100], Step [1100/13958], Loss: 188.0611\n",
      "Epoch [23/100], Step [1200/13958], Loss: 143.6254\n",
      "Epoch [23/100], Step [1300/13958], Loss: 312.3110\n",
      "Epoch [23/100], Step [1400/13958], Loss: 263.5150\n",
      "Epoch [23/100], Step [1500/13958], Loss: 419.8276\n",
      "Epoch [23/100], Step [1600/13958], Loss: 167.6939\n",
      "Epoch [23/100], Step [1700/13958], Loss: 379.6035\n",
      "Epoch [23/100], Step [1800/13958], Loss: 378.1359\n",
      "Epoch [23/100], Step [1900/13958], Loss: 212.3898\n",
      "Epoch [23/100], Step [2000/13958], Loss: 308.2763\n",
      "Epoch [23/100], Step [2100/13958], Loss: 199.9257\n",
      "Epoch [23/100], Step [2200/13958], Loss: 105.4875\n",
      "Epoch [23/100], Step [2300/13958], Loss: 311.9972\n",
      "Epoch [23/100], Step [2400/13958], Loss: 233.6228\n",
      "Epoch [23/100], Step [2500/13958], Loss: 272.6396\n",
      "Epoch [23/100], Step [2600/13958], Loss: 317.0136\n",
      "Epoch [23/100], Step [2700/13958], Loss: 429.7102\n",
      "Epoch [23/100], Step [2800/13958], Loss: 202.5308\n",
      "Epoch [23/100], Step [2900/13958], Loss: 454.1756\n",
      "Epoch [23/100], Step [3000/13958], Loss: 460.9663\n",
      "Epoch [23/100], Step [3100/13958], Loss: 185.4887\n",
      "Epoch [23/100], Step [3200/13958], Loss: 313.0541\n",
      "Epoch [23/100], Step [3300/13958], Loss: 233.8910\n",
      "Epoch [23/100], Step [3400/13958], Loss: 92.9928\n",
      "Epoch [23/100], Step [3500/13958], Loss: 153.6308\n",
      "Epoch [23/100], Step [3600/13958], Loss: 211.7970\n",
      "Epoch [23/100], Step [3700/13958], Loss: 191.3152\n",
      "Epoch [23/100], Step [3800/13958], Loss: 340.3708\n",
      "Epoch [23/100], Step [3900/13958], Loss: 175.6100\n",
      "Epoch [23/100], Step [4000/13958], Loss: 169.8704\n",
      "Epoch [23/100], Step [4100/13958], Loss: 298.4607\n",
      "Epoch [23/100], Step [4200/13958], Loss: 182.3624\n",
      "Epoch [23/100], Step [4300/13958], Loss: 299.2598\n",
      "Epoch [23/100], Step [4400/13958], Loss: 194.9049\n",
      "Epoch [23/100], Step [4500/13958], Loss: 220.1339\n",
      "Epoch [23/100], Step [4600/13958], Loss: 466.6049\n",
      "Epoch [23/100], Step [4700/13958], Loss: 118.1191\n",
      "Epoch [23/100], Step [4800/13958], Loss: 299.4296\n",
      "Epoch [23/100], Step [4900/13958], Loss: 246.0035\n",
      "Epoch [23/100], Step [5000/13958], Loss: 543.4037\n",
      "Epoch [23/100], Step [5100/13958], Loss: 248.9591\n",
      "Epoch [23/100], Step [5200/13958], Loss: 69.9204\n",
      "Epoch [23/100], Step [5300/13958], Loss: 290.0845\n",
      "Epoch [23/100], Step [5400/13958], Loss: 415.1755\n",
      "Epoch [23/100], Step [5500/13958], Loss: 243.6835\n",
      "Epoch [23/100], Step [5600/13958], Loss: 206.1296\n",
      "Epoch [23/100], Step [5700/13958], Loss: 198.2148\n",
      "Epoch [23/100], Step [5800/13958], Loss: 247.3714\n",
      "Epoch [23/100], Step [5900/13958], Loss: 234.3375\n",
      "Epoch [23/100], Step [6000/13958], Loss: 487.6928\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [23/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [24/100], Step [100/13958], Loss: 426.4851\n",
      "Epoch [24/100], Step [200/13958], Loss: 196.1211\n",
      "Epoch [24/100], Step [300/13958], Loss: 141.2447\n",
      "Epoch [24/100], Step [400/13958], Loss: 227.2130\n",
      "Epoch [24/100], Step [500/13958], Loss: 59.7017\n",
      "Epoch [24/100], Step [600/13958], Loss: 134.5475\n",
      "Epoch [24/100], Step [700/13958], Loss: 297.2053\n",
      "Epoch [24/100], Step [800/13958], Loss: 433.6035\n",
      "Epoch [24/100], Step [900/13958], Loss: 368.9099\n",
      "Epoch [24/100], Step [1000/13958], Loss: 130.0325\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [24/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [25/100], Step [100/13958], Loss: 191.7827\n",
      "Epoch [25/100], Step [200/13958], Loss: 366.1040\n",
      "Epoch [25/100], Step [300/13958], Loss: 114.0647\n",
      "Epoch [25/100], Step [400/13958], Loss: 149.8890\n",
      "Epoch [25/100], Step [500/13958], Loss: 344.7709\n",
      "Epoch [25/100], Step [600/13958], Loss: 117.1921\n",
      "Epoch [25/100], Step [700/13958], Loss: 259.3996\n",
      "Epoch [25/100], Step [800/13958], Loss: 144.2542\n",
      "Epoch [25/100], Step [900/13958], Loss: 190.4285\n",
      "Epoch [25/100], Step [1000/13958], Loss: 275.5876\n",
      "Epoch [25/100], Step [1100/13958], Loss: 551.4993\n",
      "Epoch [25/100], Step [1200/13958], Loss: 132.3814\n",
      "Epoch [25/100], Step [1300/13958], Loss: 216.4683\n",
      "Epoch [25/100], Step [1400/13958], Loss: 183.8389\n",
      "Epoch [25/100], Step [1500/13958], Loss: 209.7937\n",
      "Epoch [25/100], Step [1600/13958], Loss: 206.5411\n",
      "Epoch [25/100], Step [1700/13958], Loss: 315.4745\n",
      "Epoch [25/100], Step [1800/13958], Loss: 280.4474\n",
      "Epoch [25/100], Step [1900/13958], Loss: 394.8276\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [25/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [26/100], Step [100/13958], Loss: 113.8981\n",
      "Epoch [26/100], Step [200/13958], Loss: 234.1454\n",
      "Epoch [26/100], Step [300/13958], Loss: 210.8136\n",
      "Epoch [26/100], Step [400/13958], Loss: 347.8310\n",
      "Epoch [26/100], Step [500/13958], Loss: 152.9374\n",
      "Epoch [26/100], Step [600/13958], Loss: 532.7384\n",
      "Epoch [26/100], Step [700/13958], Loss: 251.6530\n",
      "Epoch [26/100], Step [800/13958], Loss: 105.0945\n",
      "Epoch [26/100], Step [900/13958], Loss: 128.5034\n",
      "Epoch [26/100], Step [1000/13958], Loss: 122.5432\n",
      "Epoch [26/100], Step [1100/13958], Loss: 272.6628\n",
      "Epoch [26/100], Step [1200/13958], Loss: 290.1373\n",
      "Epoch [26/100], Step [1300/13958], Loss: 251.2635\n",
      "Epoch [26/100], Step [1400/13958], Loss: 90.4565\n",
      "Epoch [26/100], Step [1500/13958], Loss: 188.4347\n",
      "Epoch [26/100], Step [1600/13958], Loss: 107.5325\n",
      "Epoch [26/100], Step [1700/13958], Loss: 195.2025\n",
      "Epoch [26/100], Step [1800/13958], Loss: 208.5946\n",
      "Epoch [26/100], Step [1900/13958], Loss: 345.9545\n",
      "Epoch [26/100], Step [2000/13958], Loss: 258.1600\n",
      "Epoch [26/100], Step [2100/13958], Loss: 182.1291\n",
      "Epoch [26/100], Step [2200/13958], Loss: 183.8461\n",
      "Epoch [26/100], Step [2300/13958], Loss: 581.3121\n",
      "Epoch [26/100], Step [2400/13958], Loss: 232.8535\n",
      "Epoch [26/100], Step [2500/13958], Loss: 361.0220\n",
      "Epoch [26/100], Step [2600/13958], Loss: 409.2288\n",
      "Epoch [26/100], Step [2700/13958], Loss: 275.9551\n",
      "Epoch [26/100], Step [2800/13958], Loss: 310.0963\n",
      "Epoch [26/100], Step [2900/13958], Loss: 548.6124\n",
      "Epoch [26/100], Step [3000/13958], Loss: 177.3335\n",
      "Epoch [26/100], Step [3100/13958], Loss: 134.6113\n",
      "Epoch [26/100], Step [3200/13958], Loss: 126.5555\n",
      "Epoch [26/100], Step [3300/13958], Loss: 194.1076\n",
      "Epoch [26/100], Step [3400/13958], Loss: 221.0117\n",
      "Epoch [26/100], Step [3500/13958], Loss: 100.4225\n",
      "Epoch [26/100], Step [3600/13958], Loss: 176.5268\n",
      "Epoch [26/100], Step [3700/13958], Loss: 136.9083\n",
      "Epoch [26/100], Step [3800/13958], Loss: 455.0909\n",
      "Epoch [26/100], Step [3900/13958], Loss: 229.1152\n",
      "Epoch [26/100], Step [4000/13958], Loss: 200.4313\n",
      "Epoch [26/100], Step [4100/13958], Loss: 118.3641\n",
      "Epoch [26/100], Step [4200/13958], Loss: 181.0800\n",
      "Epoch [26/100], Step [4300/13958], Loss: 342.8157\n",
      "Epoch [26/100], Step [4400/13958], Loss: 211.6523\n",
      "Epoch [26/100], Step [4500/13958], Loss: 201.7312\n",
      "Epoch [26/100], Step [4600/13958], Loss: 137.1997\n",
      "Epoch [26/100], Step [4700/13958], Loss: 293.1797\n",
      "Epoch [26/100], Step [4800/13958], Loss: 327.0973\n",
      "Epoch [26/100], Step [4900/13958], Loss: 484.6075\n",
      "Epoch [26/100], Step [5000/13958], Loss: 229.6884\n",
      "Epoch [26/100], Step [5100/13958], Loss: 290.0972\n",
      "Epoch [26/100], Step [5200/13958], Loss: 355.4824\n",
      "Epoch [26/100], Step [5300/13958], Loss: 177.6614\n",
      "Epoch [26/100], Step [5400/13958], Loss: 476.2342\n",
      "Epoch [26/100], Step [5500/13958], Loss: 163.5119\n",
      "Epoch [26/100], Step [5600/13958], Loss: 165.2087\n",
      "Epoch [26/100], Step [5700/13958], Loss: 238.4671\n",
      "Epoch [26/100], Step [5800/13958], Loss: 110.4868\n",
      "Epoch [26/100], Step [5900/13958], Loss: 221.4526\n",
      "Epoch [26/100], Step [6000/13958], Loss: 404.0065\n",
      "Epoch [26/100], Step [6100/13958], Loss: 108.2583\n",
      "Epoch [26/100], Step [6200/13958], Loss: 34.7482\n",
      "Epoch [26/100], Step [6300/13958], Loss: 472.5639\n",
      "Epoch [26/100], Step [6400/13958], Loss: 227.1026\n",
      "Epoch [26/100], Step [6500/13958], Loss: 299.5349\n",
      "Epoch [26/100], Step [6600/13958], Loss: 269.2059\n",
      "Epoch [26/100], Step [6700/13958], Loss: 386.4570\n",
      "Epoch [26/100], Step [6800/13958], Loss: 192.9574\n",
      "Epoch [26/100], Step [6900/13958], Loss: 174.0001\n",
      "Epoch [26/100], Step [7000/13958], Loss: 596.1442\n",
      "Epoch [26/100], Step [7100/13958], Loss: 125.3459\n",
      "Epoch [26/100], Step [7200/13958], Loss: 68.6492\n",
      "Epoch [26/100], Step [7300/13958], Loss: 240.8831\n",
      "Epoch [26/100], Step [7400/13958], Loss: 180.9677\n",
      "Epoch [26/100], Step [7500/13958], Loss: 168.2740\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [26/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [27/100], Step [100/13958], Loss: 238.8605\n",
      "Epoch [27/100], Step [200/13958], Loss: 163.7729\n",
      "Epoch [27/100], Step [300/13958], Loss: 307.7035\n",
      "Epoch [27/100], Step [400/13958], Loss: 134.2245\n",
      "Epoch [27/100], Step [500/13958], Loss: 345.3040\n",
      "Epoch [27/100], Step [600/13958], Loss: 219.5688\n",
      "Epoch [27/100], Step [700/13958], Loss: 114.6850\n",
      "Epoch [27/100], Step [800/13958], Loss: 256.6817\n",
      "Epoch [27/100], Step [900/13958], Loss: 261.9418\n",
      "Epoch [27/100], Step [1000/13958], Loss: 351.9716\n",
      "Epoch [27/100], Step [1100/13958], Loss: 142.0683\n",
      "Epoch [27/100], Step [1200/13958], Loss: 88.4768\n",
      "Epoch [27/100], Step [1300/13958], Loss: 396.6282\n",
      "Epoch [27/100], Step [1400/13958], Loss: 180.7857\n",
      "Epoch [27/100], Step [1500/13958], Loss: 243.5186\n",
      "Epoch [27/100], Step [1600/13958], Loss: 103.5869\n",
      "Epoch [27/100], Step [1700/13958], Loss: 193.0560\n",
      "Epoch [27/100], Step [1800/13958], Loss: 350.3003\n",
      "Epoch [27/100], Step [1900/13958], Loss: 184.4859\n",
      "Epoch [27/100], Step [2000/13958], Loss: 456.0849\n",
      "Epoch [27/100], Step [2100/13958], Loss: 523.4229\n",
      "Epoch [27/100], Step [2200/13958], Loss: 216.0970\n",
      "Epoch [27/100], Step [2300/13958], Loss: 453.4675\n",
      "Epoch [27/100], Step [2400/13958], Loss: 330.7350\n",
      "Epoch [27/100], Step [2500/13958], Loss: 350.7529\n",
      "Epoch [27/100], Step [2600/13958], Loss: 307.4125\n",
      "Epoch [27/100], Step [2700/13958], Loss: 308.6824\n",
      "Epoch [27/100], Step [2800/13958], Loss: 252.9463\n",
      "Epoch [27/100], Step [2900/13958], Loss: 139.1908\n",
      "Epoch [27/100], Step [3000/13958], Loss: 136.3284\n",
      "Epoch [27/100], Step [3100/13958], Loss: 492.9190\n",
      "Epoch [27/100], Step [3200/13958], Loss: 538.3840\n",
      "Epoch [27/100], Step [3300/13958], Loss: 196.4776\n",
      "Epoch [27/100], Step [3400/13958], Loss: 216.8683\n",
      "Epoch [27/100], Step [3500/13958], Loss: 292.1180\n",
      "Epoch [27/100], Step [3600/13958], Loss: 135.8949\n",
      "Epoch [27/100], Step [3700/13958], Loss: 436.8985\n",
      "Epoch [27/100], Step [3800/13958], Loss: 372.8888\n",
      "Epoch [27/100], Step [3900/13958], Loss: 176.2966\n",
      "Epoch [27/100], Step [4000/13958], Loss: 428.7522\n",
      "Epoch [27/100], Step [4100/13958], Loss: 342.8681\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [27/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [28/100], Step [100/13958], Loss: 186.2953\n",
      "Epoch [28/100], Step [200/13958], Loss: 95.9009\n",
      "Epoch [28/100], Step [300/13958], Loss: 267.2528\n",
      "Epoch [28/100], Step [400/13958], Loss: 357.5690\n",
      "Epoch [28/100], Step [500/13958], Loss: 389.3510\n",
      "Epoch [28/100], Step [600/13958], Loss: 436.8352\n",
      "Epoch [28/100], Step [700/13958], Loss: 253.1731\n",
      "Epoch [28/100], Step [800/13958], Loss: 487.4483\n",
      "Epoch [28/100], Step [900/13958], Loss: 436.3464\n",
      "Epoch [28/100], Step [1000/13958], Loss: 170.1107\n",
      "Epoch [28/100], Step [1100/13958], Loss: 247.3013\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [28/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [29/100], Step [100/13958], Loss: 346.9761\n",
      "Epoch [29/100], Step [200/13958], Loss: 183.0087\n",
      "Epoch [29/100], Step [300/13958], Loss: 333.4290\n",
      "Epoch [29/100], Step [400/13958], Loss: 229.6182\n",
      "Epoch [29/100], Step [500/13958], Loss: 147.3634\n",
      "Epoch [29/100], Step [600/13958], Loss: 219.6443\n",
      "Epoch [29/100], Step [700/13958], Loss: 495.6834\n",
      "Epoch [29/100], Step [800/13958], Loss: 158.2364\n",
      "Epoch [29/100], Step [900/13958], Loss: 214.2012\n",
      "Epoch [29/100], Step [1000/13958], Loss: 445.9599\n",
      "Epoch [29/100], Step [1100/13958], Loss: 366.1971\n",
      "Epoch [29/100], Step [1200/13958], Loss: 201.6546\n",
      "Epoch [29/100], Step [1300/13958], Loss: 110.8466\n",
      "Epoch [29/100], Step [1400/13958], Loss: 229.4601\n",
      "Epoch [29/100], Step [1500/13958], Loss: 69.4002\n",
      "Epoch [29/100], Step [1600/13958], Loss: 236.9136\n",
      "Epoch [29/100], Step [1700/13958], Loss: 285.6480\n",
      "Epoch [29/100], Step [1800/13958], Loss: 175.8223\n",
      "Epoch [29/100], Step [1900/13958], Loss: 109.4323\n",
      "Epoch [29/100], Step [2000/13958], Loss: 181.8283\n",
      "Epoch [29/100], Step [2100/13958], Loss: 207.9064\n",
      "Epoch [29/100], Step [2200/13958], Loss: 67.3733\n",
      "Epoch [29/100], Step [2300/13958], Loss: 268.6844\n",
      "Epoch [29/100], Step [2400/13958], Loss: 92.4924\n",
      "Epoch [29/100], Step [2500/13958], Loss: 206.4382\n",
      "Epoch [29/100], Step [2600/13958], Loss: 44.3956\n",
      "Epoch [29/100], Step [2700/13958], Loss: 364.7153\n",
      "Epoch [29/100], Step [2800/13958], Loss: 168.7315\n",
      "Epoch [29/100], Step [2900/13958], Loss: 70.1614\n",
      "Epoch [29/100], Step [3000/13958], Loss: 335.1533\n",
      "Epoch [29/100], Step [3100/13958], Loss: 216.7575\n",
      "Epoch [29/100], Step [3200/13958], Loss: 168.0811\n",
      "Epoch [29/100], Step [3300/13958], Loss: 320.7649\n",
      "Epoch [29/100], Step [3400/13958], Loss: 418.2379\n",
      "Epoch [29/100], Step [3500/13958], Loss: 186.9939\n",
      "Epoch [29/100], Step [3600/13958], Loss: 252.0689\n",
      "Epoch [29/100], Step [3700/13958], Loss: 313.1438\n",
      "Epoch [29/100], Step [3800/13958], Loss: 181.8762\n",
      "Epoch [29/100], Step [3900/13958], Loss: 104.9507\n",
      "Epoch [29/100], Step [4000/13958], Loss: 153.0728\n",
      "Epoch [29/100], Step [4100/13958], Loss: 140.0042\n",
      "Epoch [29/100], Step [4200/13958], Loss: 443.3263\n",
      "Epoch [29/100], Step [4300/13958], Loss: 591.0599\n",
      "Epoch [29/100], Step [4400/13958], Loss: 368.5605\n",
      "Epoch [29/100], Step [4500/13958], Loss: 117.2504\n",
      "Epoch [29/100], Step [4600/13958], Loss: 393.6671\n",
      "Epoch [29/100], Step [4700/13958], Loss: 385.9728\n",
      "Epoch [29/100], Step [4800/13958], Loss: 238.3603\n",
      "Epoch [29/100], Step [4900/13958], Loss: 508.6023\n",
      "Epoch [29/100], Step [5000/13958], Loss: 313.3593\n",
      "Epoch [29/100], Step [5100/13958], Loss: 148.6764\n",
      "Epoch [29/100], Step [5200/13958], Loss: 116.6455\n",
      "Epoch [29/100], Step [5300/13958], Loss: 417.9125\n",
      "Epoch [29/100], Step [5400/13958], Loss: 261.2973\n",
      "Epoch [29/100], Step [5500/13958], Loss: 345.8676\n",
      "Epoch [29/100], Step [5600/13958], Loss: 411.6101\n",
      "Epoch [29/100], Step [5700/13958], Loss: 117.5355\n",
      "Epoch [29/100], Step [5800/13958], Loss: 157.1036\n",
      "Epoch [29/100], Step [5900/13958], Loss: 122.1428\n",
      "Epoch [29/100], Step [6000/13958], Loss: 272.3364\n",
      "Epoch [29/100], Step [6100/13958], Loss: 412.6003\n",
      "Epoch [29/100], Step [6200/13958], Loss: 170.5215\n",
      "Epoch [29/100], Step [6300/13958], Loss: 244.8737\n",
      "Epoch [29/100], Step [6400/13958], Loss: 358.8859\n",
      "Epoch [29/100], Step [6500/13958], Loss: 329.5244\n",
      "Epoch [29/100], Step [6600/13958], Loss: 265.9975\n",
      "Epoch [29/100], Step [6700/13958], Loss: 508.5338\n",
      "Epoch [29/100], Step [6800/13958], Loss: 165.7243\n",
      "Epoch [29/100], Step [6900/13958], Loss: 112.6095\n",
      "Epoch [29/100], Step [7000/13958], Loss: 387.4569\n",
      "Epoch [29/100], Step [7100/13958], Loss: 154.7489\n",
      "Epoch [29/100], Step [7200/13958], Loss: 130.3724\n",
      "Epoch [29/100], Step [7300/13958], Loss: 281.8545\n",
      "Epoch [29/100], Step [7400/13958], Loss: 483.6474\n",
      "Epoch [29/100], Step [7500/13958], Loss: 356.3122\n",
      "Epoch [29/100], Step [7600/13958], Loss: 155.6996\n",
      "Epoch [29/100], Step [7700/13958], Loss: 450.5436\n",
      "Epoch [29/100], Step [7800/13958], Loss: 173.8027\n",
      "Epoch [29/100], Step [7900/13958], Loss: 592.3638\n",
      "Epoch [29/100], Step [8000/13958], Loss: 272.5594\n",
      "Epoch [29/100], Step [8100/13958], Loss: 285.7651\n",
      "Epoch [29/100], Step [8200/13958], Loss: 255.0912\n",
      "Epoch [29/100], Step [8300/13958], Loss: 260.6241\n",
      "Epoch [29/100], Step [8400/13958], Loss: 152.1333\n",
      "Epoch [29/100], Step [8500/13958], Loss: 266.2569\n",
      "Epoch [29/100], Step [8600/13958], Loss: 293.4506\n",
      "Epoch [29/100], Step [8700/13958], Loss: 198.0935\n",
      "Epoch [29/100], Step [8800/13958], Loss: 174.5430\n",
      "Epoch [29/100], Step [8900/13958], Loss: 420.9442\n",
      "Epoch [29/100], Step [9000/13958], Loss: 351.4832\n",
      "Epoch [29/100], Step [9100/13958], Loss: 139.1762\n",
      "Epoch [29/100], Step [9200/13958], Loss: 208.2074\n",
      "Epoch [29/100], Step [9300/13958], Loss: 238.5274\n",
      "Epoch [29/100], Step [9400/13958], Loss: 265.9888\n",
      "Epoch [29/100], Step [9500/13958], Loss: 221.8905\n",
      "Epoch [29/100], Step [9600/13958], Loss: 533.8020\n",
      "Epoch [29/100], Step [9700/13958], Loss: 109.9239\n",
      "Epoch [29/100], Step [9800/13958], Loss: 332.2815\n",
      "Epoch [29/100], Step [9900/13958], Loss: 260.8583\n",
      "Epoch [29/100], Step [10000/13958], Loss: 103.7211\n",
      "Epoch [29/100], Step [10100/13958], Loss: 322.8500\n",
      "Epoch [29/100], Step [10200/13958], Loss: 211.5943\n",
      "Epoch [29/100], Step [10300/13958], Loss: 168.8453\n",
      "Epoch [29/100], Step [10400/13958], Loss: 652.4453\n",
      "Epoch [29/100], Step [10500/13958], Loss: 202.7384\n",
      "Epoch [29/100], Step [10600/13958], Loss: 336.7800\n",
      "Epoch [29/100], Step [10700/13958], Loss: 400.5594\n",
      "Epoch [29/100], Step [10800/13958], Loss: 81.7358\n",
      "Epoch [29/100], Step [10900/13958], Loss: 388.5574\n",
      "Epoch [29/100], Step [11000/13958], Loss: 132.4641\n",
      "Epoch [29/100], Step [11100/13958], Loss: 111.1205\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [29/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [30/100], Step [100/13958], Loss: 483.5005\n",
      "Epoch [30/100], Step [200/13958], Loss: 266.8094\n",
      "Epoch [30/100], Step [300/13958], Loss: 369.2985\n",
      "Epoch [30/100], Step [400/13958], Loss: 226.9924\n",
      "Epoch [30/100], Step [500/13958], Loss: 227.8101\n",
      "Epoch [30/100], Step [600/13958], Loss: 155.0427\n",
      "Epoch [30/100], Step [700/13958], Loss: 214.8975\n",
      "Epoch [30/100], Step [800/13958], Loss: 336.9682\n",
      "Epoch [30/100], Step [900/13958], Loss: 293.4553\n",
      "Epoch [30/100], Step [1000/13958], Loss: 295.8911\n",
      "Epoch [30/100], Step [1100/13958], Loss: 202.7972\n",
      "Epoch [30/100], Step [1200/13958], Loss: 118.6699\n",
      "Epoch [30/100], Step [1300/13958], Loss: 344.0086\n",
      "Epoch [30/100], Step [1400/13958], Loss: 261.9216\n",
      "Epoch [30/100], Step [1500/13958], Loss: 299.7713\n",
      "Epoch [30/100], Step [1600/13958], Loss: 726.4099\n",
      "Epoch [30/100], Step [1700/13958], Loss: 135.6573\n",
      "Epoch [30/100], Step [1800/13958], Loss: 250.9516\n",
      "Epoch [30/100], Step [1900/13958], Loss: 239.4003\n",
      "Epoch [30/100], Step [2000/13958], Loss: 84.3091\n",
      "Epoch [30/100], Step [2100/13958], Loss: 312.0704\n",
      "Epoch [30/100], Step [2200/13958], Loss: 337.0675\n",
      "Epoch [30/100], Step [2300/13958], Loss: 145.1569\n",
      "Epoch [30/100], Step [2400/13958], Loss: 75.1948\n",
      "Epoch [30/100], Step [2500/13958], Loss: 287.1728\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [30/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [31/100], Step [100/13958], Loss: 229.3931\n",
      "Epoch [31/100], Step [200/13958], Loss: 225.9601\n",
      "Epoch [31/100], Step [300/13958], Loss: 253.2865\n",
      "Epoch [31/100], Step [400/13958], Loss: 217.4020\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [31/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [32/100], Step [100/13958], Loss: 255.0959\n",
      "Epoch [32/100], Step [200/13958], Loss: 617.3544\n",
      "Epoch [32/100], Step [300/13958], Loss: 147.7447\n",
      "Epoch [32/100], Step [400/13958], Loss: 419.2622\n",
      "Epoch [32/100], Step [500/13958], Loss: 193.3727\n",
      "Epoch [32/100], Step [600/13958], Loss: 126.5084\n",
      "Epoch [32/100], Step [700/13958], Loss: 112.8808\n",
      "Epoch [32/100], Step [800/13958], Loss: 125.4909\n",
      "Epoch [32/100], Step [900/13958], Loss: 90.4745\n",
      "Epoch [32/100], Step [1000/13958], Loss: 130.9597\n",
      "Epoch [32/100], Step [1100/13958], Loss: 281.6075\n",
      "Epoch [32/100], Step [1200/13958], Loss: 382.9169\n",
      "Epoch [32/100], Step [1300/13958], Loss: 278.3516\n",
      "Epoch [32/100], Step [1400/13958], Loss: 171.0508\n",
      "Epoch [32/100], Step [1500/13958], Loss: 318.4841\n",
      "Epoch [32/100], Step [1600/13958], Loss: 473.7852\n",
      "Epoch [32/100], Step [1700/13958], Loss: 127.9030\n",
      "Epoch [32/100], Step [1800/13958], Loss: 132.9690\n",
      "Epoch [32/100], Step [1900/13958], Loss: 337.7197\n",
      "Epoch [32/100], Step [2000/13958], Loss: 219.4689\n",
      "Epoch [32/100], Step [2100/13958], Loss: 290.4596\n",
      "Epoch [32/100], Step [2200/13958], Loss: 446.2272\n",
      "Epoch [32/100], Step [2300/13958], Loss: 310.7735\n",
      "Epoch [32/100], Step [2400/13958], Loss: 57.9443\n",
      "Epoch [32/100], Step [2500/13958], Loss: 369.8807\n",
      "Epoch [32/100], Step [2600/13958], Loss: 508.4365\n",
      "Epoch [32/100], Step [2700/13958], Loss: 378.1143\n",
      "Epoch [32/100], Step [2800/13958], Loss: 157.1210\n",
      "Epoch [32/100], Step [2900/13958], Loss: 219.1141\n",
      "Epoch [32/100], Step [3000/13958], Loss: 251.3471\n",
      "Epoch [32/100], Step [3100/13958], Loss: 258.9201\n",
      "Epoch [32/100], Step [3200/13958], Loss: 328.0874\n",
      "Epoch [32/100], Step [3300/13958], Loss: 362.5086\n",
      "Epoch [32/100], Step [3400/13958], Loss: 466.6638\n",
      "Epoch [32/100], Step [3500/13958], Loss: 188.6943\n",
      "Epoch [32/100], Step [3600/13958], Loss: 427.7205\n",
      "Epoch [32/100], Step [3700/13958], Loss: 168.1233\n",
      "Epoch [32/100], Step [3800/13958], Loss: 356.5670\n",
      "Epoch [32/100], Step [3900/13958], Loss: 271.8361\n",
      "Epoch [32/100], Step [4000/13958], Loss: 257.0128\n",
      "Epoch [32/100], Step [4100/13958], Loss: 378.1995\n",
      "Epoch [32/100], Step [4200/13958], Loss: 468.3010\n",
      "Epoch [32/100], Step [4300/13958], Loss: 137.3129\n",
      "Epoch [32/100], Step [4400/13958], Loss: 646.3354\n",
      "Epoch [32/100], Step [4500/13958], Loss: 266.9634\n",
      "Epoch [32/100], Step [4600/13958], Loss: 307.6560\n",
      "Epoch [32/100], Step [4700/13958], Loss: 344.5807\n",
      "Epoch [32/100], Step [4800/13958], Loss: 510.8557\n",
      "Epoch [32/100], Step [4900/13958], Loss: 405.2707\n",
      "Epoch [32/100], Step [5000/13958], Loss: 68.2812\n",
      "Epoch [32/100], Step [5100/13958], Loss: 339.2741\n",
      "Epoch [32/100], Step [5200/13958], Loss: 160.5747\n",
      "Epoch [32/100], Step [5300/13958], Loss: 120.6494\n",
      "Epoch [32/100], Step [5400/13958], Loss: 336.3501\n",
      "Epoch [32/100], Step [5500/13958], Loss: 238.8393\n",
      "Epoch [32/100], Step [5600/13958], Loss: 195.2508\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [32/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [33/100], Step [100/13958], Loss: 496.9883\n",
      "Epoch [33/100], Step [200/13958], Loss: 184.6463\n",
      "Epoch [33/100], Step [300/13958], Loss: 315.6399\n",
      "Epoch [33/100], Step [400/13958], Loss: 331.5488\n",
      "Epoch [33/100], Step [500/13958], Loss: 265.7187\n",
      "Epoch [33/100], Step [600/13958], Loss: 220.9804\n",
      "Epoch [33/100], Step [700/13958], Loss: 234.1211\n",
      "Epoch [33/100], Step [800/13958], Loss: 321.0540\n",
      "Epoch [33/100], Step [900/13958], Loss: 88.8616\n",
      "Epoch [33/100], Step [1000/13958], Loss: 118.4804\n",
      "Epoch [33/100], Step [1100/13958], Loss: 107.3757\n",
      "Epoch [33/100], Step [1200/13958], Loss: 301.4774\n",
      "Epoch [33/100], Step [1300/13958], Loss: 86.0882\n",
      "Epoch [33/100], Step [1400/13958], Loss: 296.0646\n",
      "Epoch [33/100], Step [1500/13958], Loss: 248.7239\n",
      "Epoch [33/100], Step [1600/13958], Loss: 281.6235\n",
      "Epoch [33/100], Step [1700/13958], Loss: 162.7444\n",
      "Epoch [33/100], Step [1800/13958], Loss: 414.6566\n",
      "Epoch [33/100], Step [1900/13958], Loss: 141.3334\n",
      "Epoch [33/100], Step [2000/13958], Loss: 132.2066\n",
      "Epoch [33/100], Step [2100/13958], Loss: 356.4334\n",
      "Epoch [33/100], Step [2200/13958], Loss: 181.9421\n",
      "Epoch [33/100], Step [2300/13958], Loss: 416.2329\n",
      "Epoch [33/100], Step [2400/13958], Loss: 321.5634\n",
      "Epoch [33/100], Step [2500/13958], Loss: 286.5928\n",
      "Epoch [33/100], Step [2600/13958], Loss: 547.9105\n",
      "Epoch [33/100], Step [2700/13958], Loss: 187.9749\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [33/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [34/100], Step [100/13958], Loss: 154.2980\n",
      "Epoch [34/100], Step [200/13958], Loss: 348.1623\n",
      "Epoch [34/100], Step [300/13958], Loss: 279.4524\n",
      "Epoch [34/100], Step [400/13958], Loss: 265.4545\n",
      "Epoch [34/100], Step [500/13958], Loss: 222.0455\n",
      "Epoch [34/100], Step [600/13958], Loss: 229.8102\n",
      "Epoch [34/100], Step [700/13958], Loss: 234.8926\n",
      "Epoch [34/100], Step [800/13958], Loss: 168.7771\n",
      "Epoch [34/100], Step [900/13958], Loss: 286.5898\n",
      "Epoch [34/100], Step [1000/13958], Loss: 282.6095\n",
      "Epoch [34/100], Step [1100/13958], Loss: 259.0699\n",
      "Epoch [34/100], Step [1200/13958], Loss: 472.1732\n",
      "Epoch [34/100], Step [1300/13958], Loss: 68.8945\n",
      "Epoch [34/100], Step [1400/13958], Loss: 210.9437\n",
      "Epoch [34/100], Step [1500/13958], Loss: 251.6133\n",
      "Epoch [34/100], Step [1600/13958], Loss: 187.9075\n",
      "Epoch [34/100], Step [1700/13958], Loss: 236.6263\n",
      "Epoch [34/100], Step [1800/13958], Loss: 432.8922\n",
      "Epoch [34/100], Step [1900/13958], Loss: 185.2944\n",
      "Epoch [34/100], Step [2000/13958], Loss: 316.4684\n",
      "Epoch [34/100], Step [2100/13958], Loss: 285.7867\n",
      "Epoch [34/100], Step [2200/13958], Loss: 223.5029\n",
      "Epoch [34/100], Step [2300/13958], Loss: 192.6112\n",
      "Epoch [34/100], Step [2400/13958], Loss: 444.3926\n",
      "Epoch [34/100], Step [2500/13958], Loss: 282.0472\n",
      "Epoch [34/100], Step [2600/13958], Loss: 183.3381\n",
      "Epoch [34/100], Step [2700/13958], Loss: 194.3890\n",
      "Epoch [34/100], Step [2800/13958], Loss: 316.2172\n",
      "Epoch [34/100], Step [2900/13958], Loss: 241.9070\n",
      "Epoch [34/100], Step [3000/13958], Loss: 228.4237\n",
      "Epoch [34/100], Step [3100/13958], Loss: 453.0897\n",
      "Epoch [34/100], Step [3200/13958], Loss: 319.2981\n",
      "Epoch [34/100], Step [3300/13958], Loss: 107.4173\n",
      "Epoch [34/100], Step [3400/13958], Loss: 267.0826\n",
      "Epoch [34/100], Step [3500/13958], Loss: 165.6359\n",
      "Epoch [34/100], Step [3600/13958], Loss: 140.4467\n",
      "Epoch [34/100], Step [3700/13958], Loss: 172.2750\n",
      "Epoch [34/100], Step [3800/13958], Loss: 423.6521\n",
      "Epoch [34/100], Step [3900/13958], Loss: 285.2397\n",
      "Epoch [34/100], Step [4000/13958], Loss: 243.1830\n",
      "Epoch [34/100], Step [4100/13958], Loss: 121.0667\n",
      "Epoch [34/100], Step [4200/13958], Loss: 226.9577\n",
      "Epoch [34/100], Step [4300/13958], Loss: 212.8528\n",
      "Epoch [34/100], Step [4400/13958], Loss: 354.7332\n",
      "Epoch [34/100], Step [4500/13958], Loss: 244.9188\n",
      "Epoch [34/100], Step [4600/13958], Loss: 89.4141\n",
      "Epoch [34/100], Step [4700/13958], Loss: 205.6828\n",
      "Epoch [34/100], Step [4800/13958], Loss: 73.0183\n",
      "Epoch [34/100], Step [4900/13958], Loss: 232.3316\n",
      "Epoch [34/100], Step [5000/13958], Loss: 284.5591\n",
      "Epoch [34/100], Step [5100/13958], Loss: 162.0827\n",
      "Epoch [34/100], Step [5200/13958], Loss: 334.4783\n",
      "Epoch [34/100], Step [5300/13958], Loss: 362.0952\n",
      "Epoch [34/100], Step [5400/13958], Loss: 363.8309\n",
      "Epoch [34/100], Step [5500/13958], Loss: 371.4138\n",
      "Epoch [34/100], Step [5600/13958], Loss: 284.6938\n",
      "Epoch [34/100], Step [5700/13958], Loss: 244.3563\n",
      "Epoch [34/100], Step [5800/13958], Loss: 245.5492\n",
      "Epoch [34/100], Step [5900/13958], Loss: 435.4164\n",
      "Epoch [34/100], Step [6000/13958], Loss: 172.1182\n",
      "Epoch [34/100], Step [6100/13958], Loss: 115.2743\n",
      "Epoch [34/100], Step [6200/13958], Loss: 293.1755\n",
      "Epoch [34/100], Step [6300/13958], Loss: 324.6201\n",
      "Epoch [34/100], Step [6400/13958], Loss: 305.4885\n",
      "Epoch [34/100], Step [6500/13958], Loss: 522.8990\n",
      "Epoch [34/100], Step [6600/13958], Loss: 112.6830\n",
      "Epoch [34/100], Step [6700/13958], Loss: 163.2957\n",
      "Epoch [34/100], Step [6800/13958], Loss: 315.4639\n",
      "Epoch [34/100], Step [6900/13958], Loss: 331.5478\n",
      "Epoch [34/100], Step [7000/13958], Loss: 353.9954\n",
      "Epoch [34/100], Step [7100/13958], Loss: 329.8442\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [34/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [35/100], Step [100/13958], Loss: 209.1640\n",
      "Epoch [35/100], Step [200/13958], Loss: 332.9700\n",
      "Epoch [35/100], Step [300/13958], Loss: 253.8314\n",
      "Epoch [35/100], Step [400/13958], Loss: 454.8167\n",
      "Epoch [35/100], Step [500/13958], Loss: 239.4693\n",
      "Epoch [35/100], Step [600/13958], Loss: 105.4242\n",
      "Epoch [35/100], Step [700/13958], Loss: 49.0624\n",
      "Epoch [35/100], Step [800/13958], Loss: 106.3079\n",
      "Epoch [35/100], Step [900/13958], Loss: 184.7445\n",
      "Epoch [35/100], Step [1000/13958], Loss: 220.6260\n",
      "Epoch [35/100], Step [1100/13958], Loss: 309.1392\n",
      "Epoch [35/100], Step [1200/13958], Loss: 159.3670\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [35/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [36/100], Step [100/13958], Loss: 332.6917\n",
      "Epoch [36/100], Step [200/13958], Loss: 205.3695\n",
      "Epoch [36/100], Step [300/13958], Loss: 272.1089\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [36/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [37/100], Step [100/13958], Loss: 129.4698\n",
      "Epoch [37/100], Step [200/13958], Loss: 292.8544\n",
      "Epoch [37/100], Step [300/13958], Loss: 567.0863\n",
      "Epoch [37/100], Step [400/13958], Loss: 371.3162\n",
      "Epoch [37/100], Step [500/13958], Loss: 258.4927\n",
      "Epoch [37/100], Step [600/13958], Loss: 115.2378\n",
      "Epoch [37/100], Step [700/13958], Loss: 89.8403\n",
      "Epoch [37/100], Step [800/13958], Loss: 517.4263\n",
      "Epoch [37/100], Step [900/13958], Loss: 248.1554\n",
      "Epoch [37/100], Step [1000/13958], Loss: 433.1035\n",
      "Epoch [37/100], Step [1100/13958], Loss: 299.0914\n",
      "Epoch [37/100], Step [1200/13958], Loss: 389.4388\n",
      "Epoch [37/100], Step [1300/13958], Loss: 164.1427\n",
      "Epoch [37/100], Step [1400/13958], Loss: 175.6422\n",
      "Epoch [37/100], Step [1500/13958], Loss: 99.0330\n",
      "Epoch [37/100], Step [1600/13958], Loss: 277.6129\n",
      "Epoch [37/100], Step [1700/13958], Loss: 92.8113\n",
      "Epoch [37/100], Step [1800/13958], Loss: 91.6439\n",
      "Epoch [37/100], Step [1900/13958], Loss: 85.8690\n",
      "Epoch [37/100], Step [2000/13958], Loss: 356.7675\n",
      "Epoch [37/100], Step [2100/13958], Loss: 102.2657\n",
      "Epoch [37/100], Step [2200/13958], Loss: 597.5826\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [37/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [38/100], Step [100/13958], Loss: 153.0935\n",
      "Epoch [38/100], Step [200/13958], Loss: 198.7289\n",
      "Epoch [38/100], Step [300/13958], Loss: 169.8357\n",
      "Epoch [38/100], Step [400/13958], Loss: 263.6017\n",
      "Epoch [38/100], Step [500/13958], Loss: 233.6991\n",
      "Epoch [38/100], Step [600/13958], Loss: 166.8490\n",
      "Epoch [38/100], Step [700/13958], Loss: 181.8894\n",
      "Epoch [38/100], Step [800/13958], Loss: 133.8954\n",
      "Epoch [38/100], Step [900/13958], Loss: 226.1675\n",
      "Epoch [38/100], Step [1000/13958], Loss: 58.9246\n",
      "Epoch [38/100], Step [1100/13958], Loss: 204.1449\n",
      "Epoch [38/100], Step [1200/13958], Loss: 145.8380\n",
      "Epoch [38/100], Step [1300/13958], Loss: 163.5745\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [38/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [39/100], Step [100/13958], Loss: 392.7407\n",
      "Epoch [39/100], Step [200/13958], Loss: 230.5914\n",
      "Epoch [39/100], Step [300/13958], Loss: 268.8733\n",
      "Epoch [39/100], Step [400/13958], Loss: 377.8876\n",
      "Epoch [39/100], Step [500/13958], Loss: 239.1069\n",
      "Epoch [39/100], Step [600/13958], Loss: 434.1036\n",
      "Epoch [39/100], Step [700/13958], Loss: 345.1665\n",
      "Epoch [39/100], Step [800/13958], Loss: 85.2575\n",
      "Epoch [39/100], Step [900/13958], Loss: 348.5526\n",
      "Epoch [39/100], Step [1000/13958], Loss: 321.6273\n",
      "Epoch [39/100], Step [1100/13958], Loss: 231.5438\n",
      "Epoch [39/100], Step [1200/13958], Loss: 216.4607\n",
      "Epoch [39/100], Step [1300/13958], Loss: 170.9560\n",
      "Epoch [39/100], Step [1400/13958], Loss: 194.8182\n",
      "Epoch [39/100], Step [1500/13958], Loss: 242.0781\n",
      "Epoch [39/100], Step [1600/13958], Loss: 305.1988\n",
      "Epoch [39/100], Step [1700/13958], Loss: 217.6559\n",
      "Epoch [39/100], Step [1800/13958], Loss: 126.1512\n",
      "Epoch [39/100], Step [1900/13958], Loss: 368.6261\n",
      "Epoch [39/100], Step [2000/13958], Loss: 334.8221\n",
      "Epoch [39/100], Step [2100/13958], Loss: 206.2418\n",
      "Epoch [39/100], Step [2200/13958], Loss: 250.5706\n",
      "Epoch [39/100], Step [2300/13958], Loss: 130.2471\n",
      "Epoch [39/100], Step [2400/13958], Loss: 127.6657\n",
      "Epoch [39/100], Step [2500/13958], Loss: 53.3280\n",
      "Epoch [39/100], Step [2600/13958], Loss: 278.6400\n",
      "Epoch [39/100], Step [2700/13958], Loss: 156.4582\n",
      "Epoch [39/100], Step [2800/13958], Loss: 128.2391\n",
      "Epoch [39/100], Step [2900/13958], Loss: 219.5457\n",
      "Epoch [39/100], Step [3000/13958], Loss: 359.3792\n",
      "Epoch [39/100], Step [3100/13958], Loss: 86.3349\n",
      "Epoch [39/100], Step [3200/13958], Loss: 241.2534\n",
      "Epoch [39/100], Step [3300/13958], Loss: 496.8401\n",
      "Epoch [39/100], Step [3400/13958], Loss: 213.9669\n",
      "Epoch [39/100], Step [3500/13958], Loss: 272.1502\n",
      "Epoch [39/100], Step [3600/13958], Loss: 221.3434\n",
      "Epoch [39/100], Step [3700/13958], Loss: 450.6419\n",
      "Epoch [39/100], Step [3800/13958], Loss: 333.8327\n",
      "Epoch [39/100], Step [3900/13958], Loss: 230.6272\n",
      "Epoch [39/100], Step [4000/13958], Loss: 218.3395\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [39/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [40/100], Step [100/13958], Loss: 302.1779\n",
      "Epoch [40/100], Step [200/13958], Loss: 159.0981\n",
      "Epoch [40/100], Step [300/13958], Loss: 232.0604\n",
      "Epoch [40/100], Step [400/13958], Loss: 399.0046\n",
      "Epoch [40/100], Step [500/13958], Loss: 371.7272\n",
      "Epoch [40/100], Step [600/13958], Loss: 85.9696\n",
      "Epoch [40/100], Step [700/13958], Loss: 446.9302\n",
      "Epoch [40/100], Step [800/13958], Loss: 180.8575\n",
      "Epoch [40/100], Step [900/13958], Loss: 311.8608\n",
      "Epoch [40/100], Step [1000/13958], Loss: 162.4671\n",
      "Epoch [40/100], Step [1100/13958], Loss: 297.2200\n",
      "Epoch [40/100], Step [1200/13958], Loss: 579.9659\n",
      "Epoch [40/100], Step [1300/13958], Loss: 38.3222\n",
      "Epoch [40/100], Step [1400/13958], Loss: 145.6053\n",
      "Epoch [40/100], Step [1500/13958], Loss: 272.5800\n",
      "Epoch [40/100], Step [1600/13958], Loss: 203.5824\n",
      "Epoch [40/100], Step [1700/13958], Loss: 218.7577\n",
      "Epoch [40/100], Step [1800/13958], Loss: 96.5372\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [40/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [41/100], Step [100/13958], Loss: 133.8923\n",
      "Epoch [41/100], Step [200/13958], Loss: 315.7457\n",
      "Epoch [41/100], Step [300/13958], Loss: 140.4144\n",
      "Epoch [41/100], Step [400/13958], Loss: 316.7114\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [41/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [42/100], Step [100/13958], Loss: 75.0178\n",
      "Epoch [42/100], Step [200/13958], Loss: 407.1669\n",
      "Epoch [42/100], Step [300/13958], Loss: 127.6353\n",
      "Epoch [42/100], Step [400/13958], Loss: 325.8498\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [42/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [43/100], Step [100/13958], Loss: 417.9247\n",
      "Epoch [43/100], Step [200/13958], Loss: 365.0080\n",
      "Epoch [43/100], Step [300/13958], Loss: 182.8975\n",
      "Epoch [43/100], Step [400/13958], Loss: 310.9017\n",
      "Epoch [43/100], Step [500/13958], Loss: 119.7902\n",
      "Epoch [43/100], Step [600/13958], Loss: 145.3038\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [43/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [44/100], Step [100/13958], Loss: 90.6511\n",
      "Epoch [44/100], Step [200/13958], Loss: 124.8802\n",
      "Epoch [44/100], Step [300/13958], Loss: 504.8439\n",
      "Epoch [44/100], Step [400/13958], Loss: 196.6781\n",
      "Epoch [44/100], Step [500/13958], Loss: 164.8336\n",
      "Epoch [44/100], Step [600/13958], Loss: 290.9609\n",
      "Epoch [44/100], Step [700/13958], Loss: 188.9560\n",
      "Epoch [44/100], Step [800/13958], Loss: 327.5275\n",
      "Epoch [44/100], Step [900/13958], Loss: 473.3651\n",
      "Epoch [44/100], Step [1000/13958], Loss: 433.1678\n",
      "Epoch [44/100], Step [1100/13958], Loss: 367.5196\n",
      "Epoch [44/100], Step [1200/13958], Loss: 306.9011\n",
      "Epoch [44/100], Step [1300/13958], Loss: 241.3276\n",
      "Epoch [44/100], Step [1400/13958], Loss: 230.5432\n",
      "Epoch [44/100], Step [1500/13958], Loss: 435.0019\n",
      "Epoch [44/100], Step [1600/13958], Loss: 224.9255\n",
      "Epoch [44/100], Step [1700/13958], Loss: 506.9608\n",
      "Epoch [44/100], Step [1800/13958], Loss: 305.9835\n",
      "Epoch [44/100], Step [1900/13958], Loss: 275.9366\n",
      "Epoch [44/100], Step [2000/13958], Loss: 280.4197\n",
      "Epoch [44/100], Step [2100/13958], Loss: 32.2625\n",
      "Epoch [44/100], Step [2200/13958], Loss: 245.5404\n",
      "Epoch [44/100], Step [2300/13958], Loss: 361.5077\n",
      "Epoch [44/100], Step [2400/13958], Loss: 429.3271\n",
      "Epoch [44/100], Step [2500/13958], Loss: 136.1567\n",
      "Epoch [44/100], Step [2600/13958], Loss: 257.2880\n",
      "Epoch [44/100], Step [2700/13958], Loss: 134.2803\n",
      "Epoch [44/100], Step [2800/13958], Loss: 444.1541\n",
      "Epoch [44/100], Step [2900/13958], Loss: 113.0741\n",
      "Epoch [44/100], Step [3000/13958], Loss: 202.7626\n",
      "Epoch [44/100], Step [3100/13958], Loss: 366.2498\n",
      "Epoch [44/100], Step [3200/13958], Loss: 259.9136\n",
      "Epoch [44/100], Step [3300/13958], Loss: 112.5870\n",
      "Epoch [44/100], Step [3400/13958], Loss: 399.0734\n",
      "Epoch [44/100], Step [3500/13958], Loss: 100.7219\n",
      "Epoch [44/100], Step [3600/13958], Loss: 132.1694\n",
      "Epoch [44/100], Step [3700/13958], Loss: 277.9566\n",
      "Epoch [44/100], Step [3800/13958], Loss: 182.1225\n",
      "Epoch [44/100], Step [3900/13958], Loss: 322.2153\n",
      "Epoch [44/100], Step [4000/13958], Loss: 312.5773\n",
      "Epoch [44/100], Step [4100/13958], Loss: 232.9539\n",
      "Epoch [44/100], Step [4200/13958], Loss: 370.9509\n",
      "Epoch [44/100], Step [4300/13958], Loss: 361.2358\n",
      "Epoch [44/100], Step [4400/13958], Loss: 126.3631\n",
      "Epoch [44/100], Step [4500/13958], Loss: 119.3873\n",
      "Epoch [44/100], Step [4600/13958], Loss: 186.7879\n",
      "Epoch [44/100], Step [4700/13958], Loss: 582.0914\n",
      "Epoch [44/100], Step [4800/13958], Loss: 237.5136\n",
      "Epoch [44/100], Step [4900/13958], Loss: 178.2045\n",
      "Epoch [44/100], Step [5000/13958], Loss: 196.9536\n",
      "Epoch [44/100], Step [5100/13958], Loss: 392.0022\n",
      "Epoch [44/100], Step [5200/13958], Loss: 256.1232\n",
      "Epoch [44/100], Step [5300/13958], Loss: 183.5107\n",
      "Epoch [44/100], Step [5400/13958], Loss: 200.7523\n",
      "Epoch [44/100], Step [5500/13958], Loss: 111.5810\n",
      "Epoch [44/100], Step [5600/13958], Loss: 412.7098\n",
      "Epoch [44/100], Step [5700/13958], Loss: 142.0529\n",
      "Epoch [44/100], Step [5800/13958], Loss: 124.0431\n",
      "Epoch [44/100], Step [5900/13958], Loss: 247.4067\n",
      "Epoch [44/100], Step [6000/13958], Loss: 235.4485\n",
      "Epoch [44/100], Step [6100/13958], Loss: 157.4763\n",
      "Epoch [44/100], Step [6200/13958], Loss: 326.7638\n",
      "Epoch [44/100], Step [6300/13958], Loss: 334.8827\n",
      "Epoch [44/100], Step [6400/13958], Loss: 323.3967\n",
      "Epoch [44/100], Step [6500/13958], Loss: 212.3202\n",
      "Epoch [44/100], Step [6600/13958], Loss: 231.0213\n",
      "Epoch [44/100], Step [6700/13958], Loss: 242.3314\n",
      "Epoch [44/100], Step [6800/13958], Loss: 118.4666\n",
      "Epoch [44/100], Step [6900/13958], Loss: 146.5456\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [44/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [45/100], Step [100/13958], Loss: 118.1103\n",
      "Epoch [45/100], Step [200/13958], Loss: 197.2680\n",
      "Epoch [45/100], Step [300/13958], Loss: 307.1854\n",
      "Epoch [45/100], Step [400/13958], Loss: 474.5558\n",
      "Epoch [45/100], Step [500/13958], Loss: 385.0375\n",
      "Epoch [45/100], Step [600/13958], Loss: 111.6801\n",
      "Epoch [45/100], Step [700/13958], Loss: 494.4788\n",
      "Epoch [45/100], Step [800/13958], Loss: 374.5302\n",
      "Epoch [45/100], Step [900/13958], Loss: 249.5639\n",
      "Epoch [45/100], Step [1000/13958], Loss: 178.2813\n",
      "Epoch [45/100], Step [1100/13958], Loss: 175.5685\n",
      "Epoch [45/100], Step [1200/13958], Loss: 384.4085\n",
      "Epoch [45/100], Step [1300/13958], Loss: 194.3319\n",
      "Epoch [45/100], Step [1400/13958], Loss: 160.2782\n",
      "Epoch [45/100], Step [1500/13958], Loss: 405.6978\n",
      "Epoch [45/100], Step [1600/13958], Loss: 278.6324\n",
      "Epoch [45/100], Step [1700/13958], Loss: 253.6696\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [45/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [46/100], Step [100/13958], Loss: 251.9196\n",
      "Epoch [46/100], Step [200/13958], Loss: 314.3185\n",
      "Epoch [46/100], Step [300/13958], Loss: 162.4707\n",
      "Epoch [46/100], Step [400/13958], Loss: 220.2383\n",
      "Epoch [46/100], Step [500/13958], Loss: 374.7776\n",
      "Epoch [46/100], Step [600/13958], Loss: 373.4129\n",
      "Epoch [46/100], Step [700/13958], Loss: 239.1196\n",
      "Epoch [46/100], Step [800/13958], Loss: 199.7587\n",
      "Epoch [46/100], Step [900/13958], Loss: 268.5033\n",
      "Epoch [46/100], Step [1000/13958], Loss: 529.4961\n",
      "Epoch [46/100], Step [1100/13958], Loss: 140.0724\n",
      "Epoch [46/100], Step [1200/13958], Loss: 144.4707\n",
      "Epoch [46/100], Step [1300/13958], Loss: 238.1341\n",
      "Epoch [46/100], Step [1400/13958], Loss: 486.1903\n",
      "Epoch [46/100], Step [1500/13958], Loss: 127.6602\n",
      "Epoch [46/100], Step [1600/13958], Loss: 201.9828\n",
      "Epoch [46/100], Step [1700/13958], Loss: 515.4581\n",
      "Epoch [46/100], Step [1800/13958], Loss: 170.3769\n",
      "Epoch [46/100], Step [1900/13958], Loss: 242.0527\n",
      "Epoch [46/100], Step [2000/13958], Loss: 239.2870\n",
      "Epoch [46/100], Step [2100/13958], Loss: 246.1011\n",
      "Epoch [46/100], Step [2200/13958], Loss: 243.9577\n",
      "Epoch [46/100], Step [2300/13958], Loss: 537.9406\n",
      "Epoch [46/100], Step [2400/13958], Loss: 379.5903\n",
      "Epoch [46/100], Step [2500/13958], Loss: 444.4347\n",
      "Epoch [46/100], Step [2600/13958], Loss: 274.7700\n",
      "Epoch [46/100], Step [2700/13958], Loss: 355.6518\n",
      "Epoch [46/100], Step [2800/13958], Loss: 203.2753\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [46/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [47/100], Step [100/13958], Loss: 574.7644\n",
      "Epoch [47/100], Step [200/13958], Loss: 126.0262\n",
      "Epoch [47/100], Step [300/13958], Loss: 150.0866\n",
      "Epoch [47/100], Step [400/13958], Loss: 98.4609\n",
      "Epoch [47/100], Step [500/13958], Loss: 211.1943\n",
      "Epoch [47/100], Step [600/13958], Loss: 161.2056\n",
      "Epoch [47/100], Step [700/13958], Loss: 380.0109\n",
      "Epoch [47/100], Step [800/13958], Loss: 57.9829\n",
      "Epoch [47/100], Step [900/13958], Loss: 259.8864\n",
      "Epoch [47/100], Step [1000/13958], Loss: 355.0869\n",
      "Epoch [47/100], Step [1100/13958], Loss: 383.0074\n",
      "Epoch [47/100], Step [1200/13958], Loss: 238.4281\n",
      "Epoch [47/100], Step [1300/13958], Loss: 132.5842\n",
      "Epoch [47/100], Step [1400/13958], Loss: 199.5278\n",
      "Epoch [47/100], Step [1500/13958], Loss: 231.8632\n",
      "Epoch [47/100], Step [1600/13958], Loss: 305.8471\n",
      "Epoch [47/100], Step [1700/13958], Loss: 248.5716\n",
      "Epoch [47/100], Step [1800/13958], Loss: 382.0807\n",
      "Epoch [47/100], Step [1900/13958], Loss: 95.8422\n",
      "Epoch [47/100], Step [2000/13958], Loss: 241.7744\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [47/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [48/100], Step [100/13958], Loss: 365.3241\n",
      "Epoch [48/100], Step [200/13958], Loss: 184.4948\n",
      "Epoch [48/100], Step [300/13958], Loss: 210.4822\n",
      "Epoch [48/100], Step [400/13958], Loss: 156.5574\n",
      "Epoch [48/100], Step [500/13958], Loss: 460.6256\n",
      "Epoch [48/100], Step [600/13958], Loss: 530.0010\n",
      "Epoch [48/100], Step [700/13958], Loss: 159.8761\n",
      "Epoch [48/100], Step [800/13958], Loss: 359.3353\n",
      "Epoch [48/100], Step [900/13958], Loss: 443.1357\n",
      "Epoch [48/100], Step [1000/13958], Loss: 450.4631\n",
      "Epoch [48/100], Step [1100/13958], Loss: 129.3393\n",
      "Epoch [48/100], Step [1200/13958], Loss: 184.3694\n",
      "Epoch [48/100], Step [1300/13958], Loss: 363.8432\n",
      "Epoch [48/100], Step [1400/13958], Loss: 140.9311\n",
      "Epoch [48/100], Step [1500/13958], Loss: 361.5241\n",
      "Epoch [48/100], Step [1600/13958], Loss: 518.0264\n",
      "Epoch [48/100], Step [1700/13958], Loss: 273.5171\n",
      "Epoch [48/100], Step [1800/13958], Loss: 194.2672\n",
      "Epoch [48/100], Step [1900/13958], Loss: 111.7809\n",
      "Epoch [48/100], Step [2000/13958], Loss: 175.9995\n",
      "Epoch [48/100], Step [2100/13958], Loss: 66.8342\n",
      "Epoch [48/100], Step [2200/13958], Loss: 145.8691\n",
      "Epoch [48/100], Step [2300/13958], Loss: 69.0496\n",
      "Epoch [48/100], Step [2400/13958], Loss: 51.8184\n",
      "Epoch [48/100], Step [2500/13958], Loss: 290.6262\n",
      "Epoch [48/100], Step [2600/13958], Loss: 37.7149\n",
      "Epoch [48/100], Step [2700/13958], Loss: 387.9590\n",
      "Epoch [48/100], Step [2800/13958], Loss: 413.9761\n",
      "Epoch [48/100], Step [2900/13958], Loss: 345.5757\n",
      "Epoch [48/100], Step [3000/13958], Loss: 264.1934\n",
      "Epoch [48/100], Step [3100/13958], Loss: 302.6153\n",
      "Epoch [48/100], Step [3200/13958], Loss: 386.9017\n",
      "Epoch [48/100], Step [3300/13958], Loss: 155.2677\n",
      "Epoch [48/100], Step [3400/13958], Loss: 270.4762\n",
      "Epoch [48/100], Step [3500/13958], Loss: 111.9529\n",
      "Epoch [48/100], Step [3600/13958], Loss: 65.0880\n",
      "Epoch [48/100], Step [3700/13958], Loss: 401.7523\n",
      "Epoch [48/100], Step [3800/13958], Loss: 196.0712\n",
      "Epoch [48/100], Step [3900/13958], Loss: 59.3667\n",
      "Epoch [48/100], Step [4000/13958], Loss: 283.3866\n",
      "Epoch [48/100], Step [4100/13958], Loss: 131.2574\n",
      "Epoch [48/100], Step [4200/13958], Loss: 190.0739\n",
      "Epoch [48/100], Step [4300/13958], Loss: 167.2920\n",
      "Epoch [48/100], Step [4400/13958], Loss: 190.7682\n",
      "Epoch [48/100], Step [4500/13958], Loss: 168.0461\n",
      "Epoch [48/100], Step [4600/13958], Loss: 232.2572\n",
      "Epoch [48/100], Step [4700/13958], Loss: 219.6606\n",
      "Epoch [48/100], Step [4800/13958], Loss: 233.7679\n",
      "Epoch [48/100], Step [4900/13958], Loss: 599.0227\n",
      "Epoch [48/100], Step [5000/13958], Loss: 177.1282\n",
      "Epoch [48/100], Step [5100/13958], Loss: 309.8487\n",
      "Epoch [48/100], Step [5200/13958], Loss: 187.7257\n",
      "Epoch [48/100], Step [5300/13958], Loss: 118.2257\n",
      "Epoch [48/100], Step [5400/13958], Loss: 171.2904\n",
      "Epoch [48/100], Step [5500/13958], Loss: 139.0761\n",
      "Epoch [48/100], Step [5600/13958], Loss: 247.5550\n",
      "Epoch [48/100], Step [5700/13958], Loss: 238.2385\n",
      "Epoch [48/100], Step [5800/13958], Loss: 220.5399\n",
      "Epoch [48/100], Step [5900/13958], Loss: 451.5803\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [48/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [49/100], Step [100/13958], Loss: 318.8088\n",
      "Epoch [49/100], Step [200/13958], Loss: 148.1437\n",
      "Epoch [49/100], Step [300/13958], Loss: 129.3593\n",
      "Epoch [49/100], Step [400/13958], Loss: 267.3909\n",
      "Epoch [49/100], Step [500/13958], Loss: 395.0094\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [49/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [50/100], Step [100/13958], Loss: 325.8453\n",
      "Epoch [50/100], Step [200/13958], Loss: 288.3335\n",
      "Epoch [50/100], Step [300/13958], Loss: 244.6901\n",
      "Epoch [50/100], Step [400/13958], Loss: 200.0617\n",
      "Epoch [50/100], Step [500/13958], Loss: 251.3721\n",
      "Epoch [50/100], Step [600/13958], Loss: 821.9124\n",
      "Epoch [50/100], Step [700/13958], Loss: 375.2074\n",
      "Epoch [50/100], Step [800/13958], Loss: 526.8431\n",
      "Epoch [50/100], Step [900/13958], Loss: 134.1585\n",
      "Epoch [50/100], Step [1000/13958], Loss: 106.9316\n",
      "Epoch [50/100], Step [1100/13958], Loss: 209.7748\n",
      "Epoch [50/100], Step [1200/13958], Loss: 50.3149\n",
      "Epoch [50/100], Step [1300/13958], Loss: 182.7782\n",
      "Epoch [50/100], Step [1400/13958], Loss: 237.1130\n",
      "Epoch [50/100], Step [1500/13958], Loss: 209.8379\n",
      "Epoch [50/100], Step [1600/13958], Loss: 203.3080\n",
      "Epoch [50/100], Step [1700/13958], Loss: 530.3422\n",
      "Epoch [50/100], Step [1800/13958], Loss: 153.4803\n",
      "Epoch [50/100], Step [1900/13958], Loss: 137.5569\n",
      "Epoch [50/100], Step [2000/13958], Loss: 224.4900\n",
      "Epoch [50/100], Step [2100/13958], Loss: 204.7202\n",
      "Epoch [50/100], Step [2200/13958], Loss: 143.4088\n",
      "Epoch [50/100], Step [2300/13958], Loss: 225.7126\n",
      "Epoch [50/100], Step [2400/13958], Loss: 563.2104\n",
      "Epoch [50/100], Step [2500/13958], Loss: 85.7561\n",
      "Epoch [50/100], Step [2600/13958], Loss: 533.1091\n",
      "Epoch [50/100], Step [2700/13958], Loss: 145.8004\n",
      "Epoch [50/100], Step [2800/13958], Loss: 87.0599\n",
      "Epoch [50/100], Step [2900/13958], Loss: 264.4545\n",
      "Epoch [50/100], Step [3000/13958], Loss: 146.1602\n",
      "Epoch [50/100], Step [3100/13958], Loss: 153.1534\n",
      "Epoch [50/100], Step [3200/13958], Loss: 83.0083\n",
      "Epoch [50/100], Step [3300/13958], Loss: 219.7343\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [50/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [51/100], Step [100/13958], Loss: 230.6616\n",
      "Epoch [51/100], Step [200/13958], Loss: 132.5835\n",
      "Epoch [51/100], Step [300/13958], Loss: 726.5438\n",
      "Epoch [51/100], Step [400/13958], Loss: 242.5847\n",
      "Epoch [51/100], Step [500/13958], Loss: 482.3398\n",
      "Epoch [51/100], Step [600/13958], Loss: 162.9763\n",
      "Epoch [51/100], Step [700/13958], Loss: 154.5090\n",
      "Epoch [51/100], Step [800/13958], Loss: 505.1054\n",
      "Epoch [51/100], Step [900/13958], Loss: 410.5778\n",
      "Epoch [51/100], Step [1000/13958], Loss: 269.6510\n",
      "Epoch [51/100], Step [1100/13958], Loss: 258.2436\n",
      "Epoch [51/100], Step [1200/13958], Loss: 244.2214\n",
      "Epoch [51/100], Step [1300/13958], Loss: 326.4197\n",
      "Epoch [51/100], Step [1400/13958], Loss: 140.9732\n",
      "Epoch [51/100], Step [1500/13958], Loss: 381.6909\n",
      "Epoch [51/100], Step [1600/13958], Loss: 187.3854\n",
      "Epoch [51/100], Step [1700/13958], Loss: 201.5020\n",
      "Epoch [51/100], Step [1800/13958], Loss: 95.3426\n",
      "Epoch [51/100], Step [1900/13958], Loss: 259.1838\n",
      "Epoch [51/100], Step [2000/13958], Loss: 236.1362\n",
      "Epoch [51/100], Step [2100/13958], Loss: 78.4439\n",
      "Epoch [51/100], Step [2200/13958], Loss: 230.5325\n",
      "Epoch [51/100], Step [2300/13958], Loss: 269.1620\n",
      "Epoch [51/100], Step [2400/13958], Loss: 159.3770\n",
      "Epoch [51/100], Step [2500/13958], Loss: 62.8071\n",
      "Epoch [51/100], Step [2600/13958], Loss: 219.8906\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [51/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [52/100], Step [100/13958], Loss: 211.8265\n",
      "Epoch [52/100], Step [200/13958], Loss: 326.0163\n",
      "Epoch [52/100], Step [300/13958], Loss: 216.3921\n",
      "Epoch [52/100], Step [400/13958], Loss: 185.3667\n",
      "Epoch [52/100], Step [500/13958], Loss: 118.0971\n",
      "Epoch [52/100], Step [600/13958], Loss: 353.3192\n",
      "Epoch [52/100], Step [700/13958], Loss: 275.9576\n",
      "Epoch [52/100], Step [800/13958], Loss: 320.1401\n",
      "Epoch [52/100], Step [900/13958], Loss: 127.7868\n",
      "Epoch [52/100], Step [1000/13958], Loss: 216.0543\n",
      "Epoch [52/100], Step [1100/13958], Loss: 374.4106\n",
      "Epoch [52/100], Step [1200/13958], Loss: 131.9810\n",
      "Epoch [52/100], Step [1300/13958], Loss: 66.2692\n",
      "Epoch [52/100], Step [1400/13958], Loss: 617.7368\n",
      "Epoch [52/100], Step [1500/13958], Loss: 147.2803\n",
      "Epoch [52/100], Step [1600/13958], Loss: 118.2091\n",
      "Epoch [52/100], Step [1700/13958], Loss: 260.7848\n",
      "Epoch [52/100], Step [1800/13958], Loss: 305.4292\n",
      "Epoch [52/100], Step [1900/13958], Loss: 415.7967\n",
      "Epoch [52/100], Step [2000/13958], Loss: 300.9313\n",
      "Epoch [52/100], Step [2100/13958], Loss: 289.6358\n",
      "Epoch [52/100], Step [2200/13958], Loss: 111.2666\n",
      "Epoch [52/100], Step [2300/13958], Loss: 73.7775\n",
      "Epoch [52/100], Step [2400/13958], Loss: 163.9821\n",
      "Epoch [52/100], Step [2500/13958], Loss: 182.5338\n",
      "Epoch [52/100], Step [2600/13958], Loss: 350.1874\n",
      "Epoch [52/100], Step [2700/13958], Loss: 73.6574\n",
      "Epoch [52/100], Step [2800/13958], Loss: 216.7074\n",
      "Epoch [52/100], Step [2900/13958], Loss: 378.0170\n",
      "Epoch [52/100], Step [3000/13958], Loss: 103.3068\n",
      "Epoch [52/100], Step [3100/13958], Loss: 167.7681\n",
      "Epoch [52/100], Step [3200/13958], Loss: 272.9484\n",
      "Epoch [52/100], Step [3300/13958], Loss: 275.1506\n",
      "Epoch [52/100], Step [3400/13958], Loss: 388.1727\n",
      "Epoch [52/100], Step [3500/13958], Loss: 347.9794\n",
      "Epoch [52/100], Step [3600/13958], Loss: 164.9879\n",
      "Epoch [52/100], Step [3700/13958], Loss: 359.5338\n",
      "Epoch [52/100], Step [3800/13958], Loss: 214.8717\n",
      "Epoch [52/100], Step [3900/13958], Loss: 316.5955\n",
      "Epoch [52/100], Step [4000/13958], Loss: 244.0130\n",
      "Epoch [52/100], Step [4100/13958], Loss: 269.0035\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [52/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [53/100], Step [100/13958], Loss: 179.9360\n",
      "Epoch [53/100], Step [200/13958], Loss: 87.9815\n",
      "Epoch [53/100], Step [300/13958], Loss: 356.7595\n",
      "Epoch [53/100], Step [400/13958], Loss: 241.0317\n",
      "Epoch [53/100], Step [500/13958], Loss: 142.2348\n",
      "Epoch [53/100], Step [600/13958], Loss: 118.8753\n",
      "Epoch [53/100], Step [700/13958], Loss: 389.5076\n",
      "Epoch [53/100], Step [800/13958], Loss: 210.3236\n",
      "Epoch [53/100], Step [900/13958], Loss: 506.6301\n",
      "Epoch [53/100], Step [1000/13958], Loss: 109.8685\n",
      "Epoch [53/100], Step [1100/13958], Loss: 156.2914\n",
      "Epoch [53/100], Step [1200/13958], Loss: 208.5214\n",
      "Epoch [53/100], Step [1300/13958], Loss: 311.8722\n",
      "Epoch [53/100], Step [1400/13958], Loss: 54.3289\n",
      "Epoch [53/100], Step [1500/13958], Loss: 159.4804\n",
      "Epoch [53/100], Step [1600/13958], Loss: 122.5800\n",
      "Epoch [53/100], Step [1700/13958], Loss: 530.4379\n",
      "Epoch [53/100], Step [1800/13958], Loss: 366.5399\n",
      "Epoch [53/100], Step [1900/13958], Loss: 127.3680\n",
      "Epoch [53/100], Step [2000/13958], Loss: 282.2212\n",
      "Epoch [53/100], Step [2100/13958], Loss: 84.6430\n",
      "Epoch [53/100], Step [2200/13958], Loss: 358.0626\n",
      "Epoch [53/100], Step [2300/13958], Loss: 178.6540\n",
      "Epoch [53/100], Step [2400/13958], Loss: 402.2473\n",
      "Epoch [53/100], Step [2500/13958], Loss: 188.8339\n",
      "Epoch [53/100], Step [2600/13958], Loss: 237.2737\n",
      "Epoch [53/100], Step [2700/13958], Loss: 80.1134\n",
      "Epoch [53/100], Step [2800/13958], Loss: 315.5627\n",
      "Epoch [53/100], Step [2900/13958], Loss: 252.3920\n",
      "Epoch [53/100], Step [3000/13958], Loss: 91.8068\n",
      "Epoch [53/100], Step [3100/13958], Loss: 63.9180\n",
      "Epoch [53/100], Step [3200/13958], Loss: 240.5934\n",
      "Epoch [53/100], Step [3300/13958], Loss: 385.3785\n",
      "Epoch [53/100], Step [3400/13958], Loss: 91.0419\n",
      "Epoch [53/100], Step [3500/13958], Loss: 356.6438\n",
      "Epoch [53/100], Step [3600/13958], Loss: 268.7047\n",
      "Epoch [53/100], Step [3700/13958], Loss: 235.3824\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [53/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [54/100], Step [100/13958], Loss: 404.2547\n",
      "Epoch [54/100], Step [200/13958], Loss: 358.0955\n",
      "Epoch [54/100], Step [300/13958], Loss: 404.8004\n",
      "Epoch [54/100], Step [400/13958], Loss: 144.8867\n",
      "Epoch [54/100], Step [500/13958], Loss: 299.8088\n",
      "Epoch [54/100], Step [600/13958], Loss: 196.7094\n",
      "Epoch [54/100], Step [700/13958], Loss: 232.6725\n",
      "Epoch [54/100], Step [800/13958], Loss: 387.7983\n",
      "Epoch [54/100], Step [900/13958], Loss: 285.8927\n",
      "Epoch [54/100], Step [1000/13958], Loss: 304.0155\n",
      "Epoch [54/100], Step [1100/13958], Loss: 422.9811\n",
      "Epoch [54/100], Step [1200/13958], Loss: 409.1010\n",
      "Epoch [54/100], Step [1300/13958], Loss: 464.4178\n",
      "Epoch [54/100], Step [1400/13958], Loss: 505.4837\n",
      "Epoch [54/100], Step [1500/13958], Loss: 181.2620\n",
      "Epoch [54/100], Step [1600/13958], Loss: 203.4682\n",
      "Epoch [54/100], Step [1700/13958], Loss: 358.5372\n",
      "Epoch [54/100], Step [1800/13958], Loss: 205.4120\n",
      "Epoch [54/100], Step [1900/13958], Loss: 420.7497\n",
      "Epoch [54/100], Step [2000/13958], Loss: 85.2470\n",
      "Epoch [54/100], Step [2100/13958], Loss: 279.9948\n",
      "Epoch [54/100], Step [2200/13958], Loss: 96.7403\n",
      "Epoch [54/100], Step [2300/13958], Loss: 185.6978\n",
      "Epoch [54/100], Step [2400/13958], Loss: 236.2393\n",
      "Epoch [54/100], Step [2500/13958], Loss: 208.4919\n",
      "Epoch [54/100], Step [2600/13958], Loss: 228.3779\n",
      "Epoch [54/100], Step [2700/13958], Loss: 253.2742\n",
      "Epoch [54/100], Step [2800/13958], Loss: 125.9727\n",
      "Epoch [54/100], Step [2900/13958], Loss: 190.4603\n",
      "Epoch [54/100], Step [3000/13958], Loss: 208.6580\n",
      "Epoch [54/100], Step [3100/13958], Loss: 453.4312\n",
      "Epoch [54/100], Step [3200/13958], Loss: 159.1874\n",
      "Epoch [54/100], Step [3300/13958], Loss: 106.9928\n",
      "Epoch [54/100], Step [3400/13958], Loss: 171.8958\n",
      "Epoch [54/100], Step [3500/13958], Loss: 238.6914\n",
      "Epoch [54/100], Step [3600/13958], Loss: 100.8819\n",
      "Epoch [54/100], Step [3700/13958], Loss: 187.8226\n",
      "Epoch [54/100], Step [3800/13958], Loss: 53.2284\n",
      "Epoch [54/100], Step [3900/13958], Loss: 341.7646\n",
      "Epoch [54/100], Step [4000/13958], Loss: 124.0277\n",
      "Epoch [54/100], Step [4100/13958], Loss: 337.7584\n",
      "Epoch [54/100], Step [4200/13958], Loss: 390.9515\n",
      "Epoch [54/100], Step [4300/13958], Loss: 151.4628\n",
      "Epoch [54/100], Step [4400/13958], Loss: 139.9123\n",
      "Epoch [54/100], Step [4500/13958], Loss: 501.3438\n",
      "Epoch [54/100], Step [4600/13958], Loss: 449.6057\n",
      "Epoch [54/100], Step [4700/13958], Loss: 589.2773\n",
      "Epoch [54/100], Step [4800/13958], Loss: 89.3164\n",
      "Epoch [54/100], Step [4900/13958], Loss: 433.5583\n",
      "Epoch [54/100], Step [5000/13958], Loss: 318.5696\n",
      "Epoch [54/100], Step [5100/13958], Loss: 108.8208\n",
      "Epoch [54/100], Step [5200/13958], Loss: 187.5969\n",
      "Epoch [54/100], Step [5300/13958], Loss: 101.8886\n",
      "Epoch [54/100], Step [5400/13958], Loss: 319.4952\n",
      "Epoch [54/100], Step [5500/13958], Loss: 395.0983\n",
      "Epoch [54/100], Step [5600/13958], Loss: 257.5826\n",
      "Epoch [54/100], Step [5700/13958], Loss: 142.4517\n",
      "Epoch [54/100], Step [5800/13958], Loss: 289.2976\n",
      "Epoch [54/100], Step [5900/13958], Loss: 166.8526\n",
      "Epoch [54/100], Step [6000/13958], Loss: 547.9817\n",
      "Epoch [54/100], Step [6100/13958], Loss: 303.2108\n",
      "Epoch [54/100], Step [6200/13958], Loss: 145.4921\n",
      "Epoch [54/100], Step [6300/13958], Loss: 328.5532\n",
      "Epoch [54/100], Step [6400/13958], Loss: 390.1653\n",
      "Epoch [54/100], Step [6500/13958], Loss: 423.1578\n",
      "Epoch [54/100], Step [6600/13958], Loss: 364.3654\n",
      "Epoch [54/100], Step [6700/13958], Loss: 325.8956\n",
      "Epoch [54/100], Step [6800/13958], Loss: 159.7012\n",
      "Epoch [54/100], Step [6900/13958], Loss: 177.6727\n",
      "Epoch [54/100], Step [7000/13958], Loss: 220.2889\n",
      "Epoch [54/100], Step [7100/13958], Loss: 99.8958\n",
      "Epoch [54/100], Step [7200/13958], Loss: 213.0995\n",
      "Epoch [54/100], Step [7300/13958], Loss: 327.0788\n",
      "Epoch [54/100], Step [7400/13958], Loss: 170.4186\n",
      "Epoch [54/100], Step [7500/13958], Loss: 106.5833\n",
      "Epoch [54/100], Step [7600/13958], Loss: 178.8383\n",
      "Epoch [54/100], Step [7700/13958], Loss: 447.5578\n",
      "Epoch [54/100], Step [7800/13958], Loss: 289.6544\n",
      "Epoch [54/100], Step [7900/13958], Loss: 95.3210\n",
      "Epoch [54/100], Step [8000/13958], Loss: 418.7167\n",
      "Epoch [54/100], Step [8100/13958], Loss: 277.0244\n",
      "Epoch [54/100], Step [8200/13958], Loss: 310.4256\n",
      "Epoch [54/100], Step [8300/13958], Loss: 207.3120\n",
      "Epoch [54/100], Step [8400/13958], Loss: 139.8607\n",
      "Epoch [54/100], Step [8500/13958], Loss: 369.3936\n",
      "Epoch [54/100], Step [8600/13958], Loss: 209.4059\n",
      "Epoch [54/100], Step [8700/13958], Loss: 278.3475\n",
      "Epoch [54/100], Step [8800/13958], Loss: 335.9700\n",
      "Epoch [54/100], Step [8900/13958], Loss: 420.8453\n",
      "Epoch [54/100], Step [9000/13958], Loss: 154.7365\n",
      "Epoch [54/100], Step [9100/13958], Loss: 369.3687\n",
      "Epoch [54/100], Step [9200/13958], Loss: 336.3739\n",
      "Epoch [54/100], Step [9300/13958], Loss: 308.6002\n",
      "Epoch [54/100], Step [9400/13958], Loss: 181.0740\n",
      "Epoch [54/100], Step [9500/13958], Loss: 424.0855\n",
      "Epoch [54/100], Step [9600/13958], Loss: 362.8729\n",
      "Epoch [54/100], Step [9700/13958], Loss: 323.3987\n",
      "Epoch [54/100], Step [9800/13958], Loss: 517.2432\n",
      "Epoch [54/100], Step [9900/13958], Loss: 119.1576\n",
      "Epoch [54/100], Step [10000/13958], Loss: 274.6454\n",
      "Epoch [54/100], Step [10100/13958], Loss: 167.5917\n",
      "Epoch [54/100], Step [10200/13958], Loss: 257.1053\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [54/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [55/100], Step [100/13958], Loss: 281.8140\n",
      "Epoch [55/100], Step [200/13958], Loss: 255.0013\n",
      "Epoch [55/100], Step [300/13958], Loss: 212.2902\n",
      "Epoch [55/100], Step [400/13958], Loss: 152.9539\n",
      "Epoch [55/100], Step [500/13958], Loss: 370.5191\n",
      "Epoch [55/100], Step [600/13958], Loss: 137.2569\n",
      "Epoch [55/100], Step [700/13958], Loss: 401.1532\n",
      "Epoch [55/100], Step [800/13958], Loss: 193.0423\n",
      "Epoch [55/100], Step [900/13958], Loss: 422.2144\n",
      "Epoch [55/100], Step [1000/13958], Loss: 74.7048\n",
      "Epoch [55/100], Step [1100/13958], Loss: 251.3965\n",
      "Epoch [55/100], Step [1200/13958], Loss: 169.0485\n",
      "Epoch [55/100], Step [1300/13958], Loss: 216.2477\n",
      "Epoch [55/100], Step [1400/13958], Loss: 169.2399\n",
      "Epoch [55/100], Step [1500/13958], Loss: 503.5365\n",
      "Epoch [55/100], Step [1600/13958], Loss: 141.1328\n",
      "Epoch [55/100], Step [1700/13958], Loss: 153.1930\n",
      "Epoch [55/100], Step [1800/13958], Loss: 278.5844\n",
      "Epoch [55/100], Step [1900/13958], Loss: 258.9329\n",
      "Epoch [55/100], Step [2000/13958], Loss: 372.7966\n",
      "Epoch [55/100], Step [2100/13958], Loss: 100.9020\n",
      "Epoch [55/100], Step [2200/13958], Loss: 248.2538\n",
      "Epoch [55/100], Step [2300/13958], Loss: 226.5106\n",
      "Epoch [55/100], Step [2400/13958], Loss: 310.1006\n",
      "Epoch [55/100], Step [2500/13958], Loss: 270.9238\n",
      "Epoch [55/100], Step [2600/13958], Loss: 88.7125\n",
      "Epoch [55/100], Step [2700/13958], Loss: 139.1239\n",
      "Epoch [55/100], Step [2800/13958], Loss: 360.8735\n",
      "Epoch [55/100], Step [2900/13958], Loss: 214.9966\n",
      "Epoch [55/100], Step [3000/13958], Loss: 346.1098\n",
      "Epoch [55/100], Step [3100/13958], Loss: 184.0025\n",
      "Epoch [55/100], Step [3200/13958], Loss: 221.1104\n",
      "Epoch [55/100], Step [3300/13958], Loss: 161.6230\n",
      "Epoch [55/100], Step [3400/13958], Loss: 191.7695\n",
      "Epoch [55/100], Step [3500/13958], Loss: 174.6407\n",
      "Epoch [55/100], Step [3600/13958], Loss: 208.5808\n",
      "Epoch [55/100], Step [3700/13958], Loss: 246.3789\n",
      "Epoch [55/100], Step [3800/13958], Loss: 127.2036\n",
      "Epoch [55/100], Step [3900/13958], Loss: 288.4474\n",
      "Epoch [55/100], Step [4000/13958], Loss: 407.2856\n",
      "Epoch [55/100], Step [4100/13958], Loss: 236.5470\n",
      "Epoch [55/100], Step [4200/13958], Loss: 287.4066\n",
      "Epoch [55/100], Step [4300/13958], Loss: 187.4204\n",
      "Epoch [55/100], Step [4400/13958], Loss: 160.1299\n",
      "Epoch [55/100], Step [4500/13958], Loss: 356.2924\n",
      "Epoch [55/100], Step [4600/13958], Loss: 156.6523\n",
      "Epoch [55/100], Step [4700/13958], Loss: 210.9325\n",
      "Epoch [55/100], Step [4800/13958], Loss: 148.7184\n",
      "Epoch [55/100], Step [4900/13958], Loss: 253.7615\n",
      "Epoch [55/100], Step [5000/13958], Loss: 323.0201\n",
      "Epoch [55/100], Step [5100/13958], Loss: 312.4485\n",
      "Epoch [55/100], Step [5200/13958], Loss: 293.3818\n",
      "Epoch [55/100], Step [5300/13958], Loss: 324.6451\n",
      "Epoch [55/100], Step [5400/13958], Loss: 278.6169\n",
      "Epoch [55/100], Step [5500/13958], Loss: 221.0537\n",
      "Epoch [55/100], Step [5600/13958], Loss: 261.8890\n",
      "Epoch [55/100], Step [5700/13958], Loss: 206.3473\n",
      "Epoch [55/100], Step [5800/13958], Loss: 431.8359\n",
      "Epoch [55/100], Step [5900/13958], Loss: 229.3746\n",
      "Epoch [55/100], Step [6000/13958], Loss: 158.4685\n",
      "Epoch [55/100], Step [6100/13958], Loss: 96.2210\n",
      "Epoch [55/100], Step [6200/13958], Loss: 221.8065\n",
      "Epoch [55/100], Step [6300/13958], Loss: 105.4301\n",
      "Epoch [55/100], Step [6400/13958], Loss: 231.7225\n",
      "Epoch [55/100], Step [6500/13958], Loss: 170.4843\n",
      "Epoch [55/100], Step [6600/13958], Loss: 448.1300\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [55/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [56/100], Step [100/13958], Loss: 431.7209\n",
      "Epoch [56/100], Step [200/13958], Loss: 362.7173\n",
      "Epoch [56/100], Step [300/13958], Loss: 264.5301\n",
      "Epoch [56/100], Step [400/13958], Loss: 94.1139\n",
      "Epoch [56/100], Step [500/13958], Loss: 374.1508\n",
      "Epoch [56/100], Step [600/13958], Loss: 324.9530\n",
      "Epoch [56/100], Step [700/13958], Loss: 242.2998\n",
      "Epoch [56/100], Step [800/13958], Loss: 218.2724\n",
      "Epoch [56/100], Step [900/13958], Loss: 140.9712\n",
      "Epoch [56/100], Step [1000/13958], Loss: 612.3089\n",
      "Epoch [56/100], Step [1100/13958], Loss: 432.3678\n",
      "Epoch [56/100], Step [1200/13958], Loss: 431.2186\n",
      "Epoch [56/100], Step [1300/13958], Loss: 172.1059\n",
      "Epoch [56/100], Step [1400/13958], Loss: 356.5734\n",
      "Epoch [56/100], Step [1500/13958], Loss: 138.1129\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [56/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [57/100], Step [100/13958], Loss: 70.8565\n",
      "Epoch [57/100], Step [200/13958], Loss: 451.0567\n",
      "Epoch [57/100], Step [300/13958], Loss: 289.3181\n",
      "Epoch [57/100], Step [400/13958], Loss: 209.9962\n",
      "Epoch [57/100], Step [500/13958], Loss: 127.8338\n",
      "Epoch [57/100], Step [600/13958], Loss: 107.0639\n",
      "Epoch [57/100], Step [700/13958], Loss: 145.3787\n",
      "Epoch [57/100], Step [800/13958], Loss: 109.5569\n",
      "Epoch [57/100], Step [900/13958], Loss: 114.8559\n",
      "Epoch [57/100], Step [1000/13958], Loss: 97.0313\n",
      "Epoch [57/100], Step [1100/13958], Loss: 318.6223\n",
      "Epoch [57/100], Step [1200/13958], Loss: 175.1042\n",
      "Epoch [57/100], Step [1300/13958], Loss: 190.4314\n",
      "Epoch [57/100], Step [1400/13958], Loss: 202.3770\n",
      "Epoch [57/100], Step [1500/13958], Loss: 247.6306\n",
      "Epoch [57/100], Step [1600/13958], Loss: 116.8174\n",
      "Epoch [57/100], Step [1700/13958], Loss: 394.1984\n",
      "Epoch [57/100], Step [1800/13958], Loss: 157.9674\n",
      "Epoch [57/100], Step [1900/13958], Loss: 80.3209\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [57/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [58/100], Step [100/13958], Loss: 297.4560\n",
      "Epoch [58/100], Step [200/13958], Loss: 186.8554\n",
      "Epoch [58/100], Step [300/13958], Loss: 463.0307\n",
      "Epoch [58/100], Step [400/13958], Loss: 377.5413\n",
      "Epoch [58/100], Step [500/13958], Loss: 242.4868\n",
      "Epoch [58/100], Step [600/13958], Loss: 107.7357\n",
      "Epoch [58/100], Step [700/13958], Loss: 277.5870\n",
      "Epoch [58/100], Step [800/13958], Loss: 321.2212\n",
      "Epoch [58/100], Step [900/13958], Loss: 424.2646\n",
      "Epoch [58/100], Step [1000/13958], Loss: 67.8910\n",
      "Epoch [58/100], Step [1100/13958], Loss: 171.8246\n",
      "Epoch [58/100], Step [1200/13958], Loss: 119.2242\n",
      "Epoch [58/100], Step [1300/13958], Loss: 163.2664\n",
      "Epoch [58/100], Step [1400/13958], Loss: 224.8504\n",
      "Epoch [58/100], Step [1500/13958], Loss: 270.7713\n",
      "Epoch [58/100], Step [1600/13958], Loss: 111.4192\n",
      "Epoch [58/100], Step [1700/13958], Loss: 177.5625\n",
      "Epoch [58/100], Step [1800/13958], Loss: 198.1219\n",
      "Epoch [58/100], Step [1900/13958], Loss: 334.4819\n",
      "Epoch [58/100], Step [2000/13958], Loss: 113.2695\n",
      "Epoch [58/100], Step [2100/13958], Loss: 210.6513\n",
      "Epoch [58/100], Step [2200/13958], Loss: 387.8748\n",
      "Epoch [58/100], Step [2300/13958], Loss: 118.0330\n",
      "Epoch [58/100], Step [2400/13958], Loss: 350.1621\n",
      "Epoch [58/100], Step [2500/13958], Loss: 174.9507\n",
      "Epoch [58/100], Step [2600/13958], Loss: 84.1039\n",
      "Epoch [58/100], Step [2700/13958], Loss: 177.7707\n",
      "Epoch [58/100], Step [2800/13958], Loss: 290.1565\n",
      "Epoch [58/100], Step [2900/13958], Loss: 222.8885\n",
      "Epoch [58/100], Step [3000/13958], Loss: 152.0016\n",
      "Epoch [58/100], Step [3100/13958], Loss: 120.3303\n",
      "Epoch [58/100], Step [3200/13958], Loss: 339.5524\n",
      "Epoch [58/100], Step [3300/13958], Loss: 286.5345\n",
      "Epoch [58/100], Step [3400/13958], Loss: 530.2488\n",
      "Epoch [58/100], Step [3500/13958], Loss: 175.9622\n",
      "Epoch [58/100], Step [3600/13958], Loss: 296.0540\n",
      "Epoch [58/100], Step [3700/13958], Loss: 290.0594\n",
      "Epoch [58/100], Step [3800/13958], Loss: 339.2887\n",
      "Epoch [58/100], Step [3900/13958], Loss: 254.2610\n",
      "Epoch [58/100], Step [4000/13958], Loss: 142.0289\n",
      "Epoch [58/100], Step [4100/13958], Loss: 370.7286\n",
      "Epoch [58/100], Step [4200/13958], Loss: 62.7414\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [58/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [59/100], Step [100/13958], Loss: 170.1334\n",
      "Epoch [59/100], Step [200/13958], Loss: 139.9445\n",
      "Epoch [59/100], Step [300/13958], Loss: 329.3952\n",
      "Epoch [59/100], Step [400/13958], Loss: 179.1336\n",
      "Epoch [59/100], Step [500/13958], Loss: 148.6198\n",
      "Epoch [59/100], Step [600/13958], Loss: 240.7717\n",
      "Epoch [59/100], Step [700/13958], Loss: 317.7433\n",
      "Epoch [59/100], Step [800/13958], Loss: 418.2976\n",
      "Epoch [59/100], Step [900/13958], Loss: 263.1446\n",
      "Epoch [59/100], Step [1000/13958], Loss: 99.7315\n",
      "Epoch [59/100], Step [1100/13958], Loss: 409.7845\n",
      "Epoch [59/100], Step [1200/13958], Loss: 147.3942\n",
      "Epoch [59/100], Step [1300/13958], Loss: 426.1993\n",
      "Epoch [59/100], Step [1400/13958], Loss: 141.6019\n",
      "Epoch [59/100], Step [1500/13958], Loss: 284.4859\n",
      "Epoch [59/100], Step [1600/13958], Loss: 203.7722\n",
      "Epoch [59/100], Step [1700/13958], Loss: 383.6697\n",
      "Epoch [59/100], Step [1800/13958], Loss: 253.3622\n",
      "Epoch [59/100], Step [1900/13958], Loss: 215.5325\n",
      "Epoch [59/100], Step [2000/13958], Loss: 385.0322\n",
      "Epoch [59/100], Step [2100/13958], Loss: 124.0503\n",
      "Epoch [59/100], Step [2200/13958], Loss: 475.3703\n",
      "Epoch [59/100], Step [2300/13958], Loss: 223.7619\n",
      "Epoch [59/100], Step [2400/13958], Loss: 176.4273\n",
      "Epoch [59/100], Step [2500/13958], Loss: 171.4315\n",
      "Epoch [59/100], Step [2600/13958], Loss: 225.7621\n",
      "Epoch [59/100], Step [2700/13958], Loss: 108.5348\n",
      "Epoch [59/100], Step [2800/13958], Loss: 223.4798\n",
      "Epoch [59/100], Step [2900/13958], Loss: 232.0254\n",
      "Epoch [59/100], Step [3000/13958], Loss: 337.3201\n",
      "Epoch [59/100], Step [3100/13958], Loss: 331.4849\n",
      "Epoch [59/100], Step [3200/13958], Loss: 162.4044\n",
      "Epoch [59/100], Step [3300/13958], Loss: 246.3260\n",
      "Epoch [59/100], Step [3400/13958], Loss: 278.5005\n",
      "Epoch [59/100], Step [3500/13958], Loss: 147.0979\n",
      "Epoch [59/100], Step [3600/13958], Loss: 435.8486\n",
      "Epoch [59/100], Step [3700/13958], Loss: 313.1137\n",
      "Epoch [59/100], Step [3800/13958], Loss: 144.8761\n",
      "Epoch [59/100], Step [3900/13958], Loss: 261.7065\n",
      "Epoch [59/100], Step [4000/13958], Loss: 101.8062\n",
      "Epoch [59/100], Step [4100/13958], Loss: 119.8262\n",
      "Epoch [59/100], Step [4200/13958], Loss: 138.1935\n",
      "Epoch [59/100], Step [4300/13958], Loss: 218.5179\n",
      "Epoch [59/100], Step [4400/13958], Loss: 183.9807\n",
      "Epoch [59/100], Step [4500/13958], Loss: 225.3902\n",
      "Epoch [59/100], Step [4600/13958], Loss: 417.4002\n",
      "Epoch [59/100], Step [4700/13958], Loss: 126.1644\n",
      "Epoch [59/100], Step [4800/13958], Loss: 575.4695\n",
      "Epoch [59/100], Step [4900/13958], Loss: 137.8948\n",
      "Epoch [59/100], Step [5000/13958], Loss: 120.7150\n",
      "Epoch [59/100], Step [5100/13958], Loss: 357.3698\n",
      "Epoch [59/100], Step [5200/13958], Loss: 198.0424\n",
      "Epoch [59/100], Step [5300/13958], Loss: 310.9955\n",
      "Epoch [59/100], Step [5400/13958], Loss: 174.4427\n",
      "Epoch [59/100], Step [5500/13958], Loss: 278.5973\n",
      "Epoch [59/100], Step [5600/13958], Loss: 176.6237\n",
      "Epoch [59/100], Step [5700/13958], Loss: 294.0254\n",
      "Epoch [59/100], Step [5800/13958], Loss: 235.9564\n",
      "Epoch [59/100], Step [5900/13958], Loss: 174.5403\n",
      "Epoch [59/100], Step [6000/13958], Loss: 664.5192\n",
      "Epoch [59/100], Step [6100/13958], Loss: 371.1923\n",
      "Epoch [59/100], Step [6200/13958], Loss: 141.5581\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [59/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [60/100], Step [100/13958], Loss: 327.0015\n",
      "Epoch [60/100], Step [200/13958], Loss: 105.1507\n",
      "Epoch [60/100], Step [300/13958], Loss: 241.3172\n",
      "Epoch [60/100], Step [400/13958], Loss: 298.6443\n",
      "Epoch [60/100], Step [500/13958], Loss: 176.6812\n",
      "Epoch [60/100], Step [600/13958], Loss: 225.0820\n",
      "Epoch [60/100], Step [700/13958], Loss: 529.7034\n",
      "Epoch [60/100], Step [800/13958], Loss: 513.7092\n",
      "Epoch [60/100], Step [900/13958], Loss: 44.4931\n",
      "Epoch [60/100], Step [1000/13958], Loss: 165.5833\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [60/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [61/100], Step [100/13958], Loss: 173.8111\n",
      "Epoch [61/100], Step [200/13958], Loss: 300.3065\n",
      "Epoch [61/100], Step [300/13958], Loss: 274.8002\n",
      "Epoch [61/100], Step [400/13958], Loss: 322.3962\n",
      "Epoch [61/100], Step [500/13958], Loss: 187.0124\n",
      "Epoch [61/100], Step [600/13958], Loss: 248.5257\n",
      "Epoch [61/100], Step [700/13958], Loss: 308.6273\n",
      "Epoch [61/100], Step [800/13958], Loss: 232.2991\n",
      "Epoch [61/100], Step [900/13958], Loss: 289.7571\n",
      "Epoch [61/100], Step [1000/13958], Loss: 135.9652\n",
      "Epoch [61/100], Step [1100/13958], Loss: 254.9658\n",
      "Epoch [61/100], Step [1200/13958], Loss: 453.4183\n",
      "Epoch [61/100], Step [1300/13958], Loss: 754.5599\n",
      "Epoch [61/100], Step [1400/13958], Loss: 46.8255\n",
      "Epoch [61/100], Step [1500/13958], Loss: 213.4772\n",
      "Epoch [61/100], Step [1600/13958], Loss: 150.5440\n",
      "Epoch [61/100], Step [1700/13958], Loss: 174.4962\n",
      "Epoch [61/100], Step [1800/13958], Loss: 267.4156\n",
      "Epoch [61/100], Step [1900/13958], Loss: 202.4054\n",
      "Epoch [61/100], Step [2000/13958], Loss: 192.5906\n",
      "Epoch [61/100], Step [2100/13958], Loss: 310.5074\n",
      "Epoch [61/100], Step [2200/13958], Loss: 147.8968\n",
      "Epoch [61/100], Step [2300/13958], Loss: 320.1697\n",
      "Epoch [61/100], Step [2400/13958], Loss: 118.5109\n",
      "Epoch [61/100], Step [2500/13958], Loss: 424.2006\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [61/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [62/100], Step [100/13958], Loss: 222.3708\n",
      "Epoch [62/100], Step [200/13958], Loss: 198.0093\n",
      "Epoch [62/100], Step [300/13958], Loss: 238.7724\n",
      "Epoch [62/100], Step [400/13958], Loss: 268.4848\n",
      "Epoch [62/100], Step [500/13958], Loss: 356.6566\n",
      "Epoch [62/100], Step [600/13958], Loss: 146.5126\n",
      "Epoch [62/100], Step [700/13958], Loss: 139.7723\n",
      "Epoch [62/100], Step [800/13958], Loss: 535.1961\n",
      "Epoch [62/100], Step [900/13958], Loss: 234.3918\n",
      "Epoch [62/100], Step [1000/13958], Loss: 581.8985\n",
      "Epoch [62/100], Step [1100/13958], Loss: 513.4119\n",
      "Epoch [62/100], Step [1200/13958], Loss: 532.3633\n",
      "Epoch [62/100], Step [1300/13958], Loss: 121.8013\n",
      "Epoch [62/100], Step [1400/13958], Loss: 254.1618\n",
      "Epoch [62/100], Step [1500/13958], Loss: 343.3478\n",
      "Epoch [62/100], Step [1600/13958], Loss: 255.7737\n",
      "Epoch [62/100], Step [1700/13958], Loss: 391.6041\n",
      "Epoch [62/100], Step [1800/13958], Loss: 265.9540\n",
      "Epoch [62/100], Step [1900/13958], Loss: 366.9936\n",
      "Epoch [62/100], Step [2000/13958], Loss: 177.1924\n",
      "Epoch [62/100], Step [2100/13958], Loss: 177.6451\n",
      "Epoch [62/100], Step [2200/13958], Loss: 165.1753\n",
      "Epoch [62/100], Step [2300/13958], Loss: 224.1920\n",
      "Epoch [62/100], Step [2400/13958], Loss: 209.0445\n",
      "Epoch [62/100], Step [2500/13958], Loss: 299.8156\n",
      "Epoch [62/100], Step [2600/13958], Loss: 297.9290\n",
      "Epoch [62/100], Step [2700/13958], Loss: 207.5433\n",
      "Epoch [62/100], Step [2800/13958], Loss: 303.5902\n",
      "Epoch [62/100], Step [2900/13958], Loss: 414.1123\n",
      "Epoch [62/100], Step [3000/13958], Loss: 388.7658\n",
      "Epoch [62/100], Step [3100/13958], Loss: 144.0112\n",
      "Epoch [62/100], Step [3200/13958], Loss: 254.8024\n",
      "Epoch [62/100], Step [3300/13958], Loss: 377.4980\n",
      "Epoch [62/100], Step [3400/13958], Loss: 138.6321\n",
      "Epoch [62/100], Step [3500/13958], Loss: 322.5993\n",
      "Epoch [62/100], Step [3600/13958], Loss: 148.1987\n",
      "Epoch [62/100], Step [3700/13958], Loss: 178.0271\n",
      "Epoch [62/100], Step [3800/13958], Loss: 97.5521\n",
      "Epoch [62/100], Step [3900/13958], Loss: 383.3297\n",
      "Epoch [62/100], Step [4000/13958], Loss: 230.1330\n",
      "Epoch [62/100], Step [4100/13958], Loss: 156.5779\n",
      "Epoch [62/100], Step [4200/13958], Loss: 441.5465\n",
      "Epoch [62/100], Step [4300/13958], Loss: 148.6298\n",
      "Epoch [62/100], Step [4400/13958], Loss: 356.7596\n",
      "Epoch [62/100], Step [4500/13958], Loss: 203.3881\n",
      "Epoch [62/100], Step [4600/13958], Loss: 512.4098\n",
      "Epoch [62/100], Step [4700/13958], Loss: 392.2449\n",
      "Epoch [62/100], Step [4800/13958], Loss: 282.5744\n",
      "Epoch [62/100], Step [4900/13958], Loss: 141.1128\n",
      "Epoch [62/100], Step [5000/13958], Loss: 262.5905\n",
      "Epoch [62/100], Step [5100/13958], Loss: 196.0077\n",
      "Epoch [62/100], Step [5200/13958], Loss: 396.9576\n",
      "Epoch [62/100], Step [5300/13958], Loss: 155.8891\n",
      "Epoch [62/100], Step [5400/13958], Loss: 373.9919\n",
      "Epoch [62/100], Step [5500/13958], Loss: 534.3066\n",
      "Epoch [62/100], Step [5600/13958], Loss: 403.7802\n",
      "Epoch [62/100], Step [5700/13958], Loss: 255.8287\n",
      "Epoch [62/100], Step [5800/13958], Loss: 251.3245\n",
      "Epoch [62/100], Step [5900/13958], Loss: 98.0040\n",
      "Epoch [62/100], Step [6000/13958], Loss: 217.3341\n",
      "Epoch [62/100], Step [6100/13958], Loss: 349.8926\n",
      "Epoch [62/100], Step [6200/13958], Loss: 267.8120\n",
      "Epoch [62/100], Step [6300/13958], Loss: 210.4660\n",
      "Epoch [62/100], Step [6400/13958], Loss: 353.2481\n",
      "Epoch [62/100], Step [6500/13958], Loss: 308.6699\n",
      "Epoch [62/100], Step [6600/13958], Loss: 283.6317\n",
      "Epoch [62/100], Step [6700/13958], Loss: 231.1797\n",
      "Epoch [62/100], Step [6800/13958], Loss: 283.9850\n",
      "Epoch [62/100], Step [6900/13958], Loss: 158.3246\n",
      "Epoch [62/100], Step [7000/13958], Loss: 156.4628\n",
      "Epoch [62/100], Step [7100/13958], Loss: 198.0526\n",
      "Epoch [62/100], Step [7200/13958], Loss: 456.6624\n",
      "Epoch [62/100], Step [7300/13958], Loss: 145.5365\n",
      "Epoch [62/100], Step [7400/13958], Loss: 147.8626\n",
      "Epoch [62/100], Step [7500/13958], Loss: 182.6750\n",
      "Epoch [62/100], Step [7600/13958], Loss: 362.2264\n",
      "Epoch [62/100], Step [7700/13958], Loss: 173.4182\n",
      "Epoch [62/100], Step [7800/13958], Loss: 195.9796\n",
      "Epoch [62/100], Step [7900/13958], Loss: 248.4931\n",
      "Epoch [62/100], Step [8000/13958], Loss: 165.3268\n",
      "Epoch [62/100], Step [8100/13958], Loss: 535.6877\n",
      "Epoch [62/100], Step [8200/13958], Loss: 321.2137\n",
      "Epoch [62/100], Step [8300/13958], Loss: 391.3792\n",
      "Epoch [62/100], Step [8400/13958], Loss: 403.3698\n",
      "Epoch [62/100], Step [8500/13958], Loss: 115.8001\n",
      "Epoch [62/100], Step [8600/13958], Loss: 93.9691\n",
      "Epoch [62/100], Step [8700/13958], Loss: 152.2146\n",
      "Epoch [62/100], Step [8800/13958], Loss: 111.8178\n",
      "Epoch [62/100], Step [8900/13958], Loss: 322.9037\n",
      "Epoch [62/100], Step [9000/13958], Loss: 150.1747\n",
      "Epoch [62/100], Step [9100/13958], Loss: 355.4951\n",
      "Epoch [62/100], Step [9200/13958], Loss: 241.8667\n",
      "Epoch [62/100], Step [9300/13958], Loss: 236.7051\n",
      "Epoch [62/100], Step [9400/13958], Loss: 124.6246\n",
      "Epoch [62/100], Step [9500/13958], Loss: 359.7867\n",
      "Epoch [62/100], Step [9600/13958], Loss: 334.2244\n",
      "Epoch [62/100], Step [9700/13958], Loss: 127.1767\n",
      "Epoch [62/100], Step [9800/13958], Loss: 105.0691\n",
      "Epoch [62/100], Step [9900/13958], Loss: 254.4201\n",
      "Epoch [62/100], Step [10000/13958], Loss: 215.0358\n",
      "Epoch [62/100], Step [10100/13958], Loss: 227.9326\n",
      "Epoch [62/100], Step [10200/13958], Loss: 242.6508\n",
      "Epoch [62/100], Step [10300/13958], Loss: 279.6173\n",
      "Epoch [62/100], Step [10400/13958], Loss: 212.1100\n",
      "Epoch [62/100], Step [10500/13958], Loss: 196.4366\n",
      "Epoch [62/100], Step [10600/13958], Loss: 250.6051\n",
      "Epoch [62/100], Step [10700/13958], Loss: 106.1256\n",
      "Epoch [62/100], Step [10800/13958], Loss: 285.5840\n",
      "Epoch [62/100], Step [10900/13958], Loss: 153.1311\n",
      "Epoch [62/100], Step [11000/13958], Loss: 347.5985\n",
      "Epoch [62/100], Step [11100/13958], Loss: 211.9229\n",
      "Epoch [62/100], Step [11200/13958], Loss: 296.9753\n",
      "Epoch [62/100], Step [11300/13958], Loss: 329.7039\n",
      "Epoch [62/100], Step [11400/13958], Loss: 149.2571\n",
      "Epoch [62/100], Step [11500/13958], Loss: 162.5390\n",
      "Epoch [62/100], Step [11600/13958], Loss: 520.5189\n",
      "Epoch [62/100], Step [11700/13958], Loss: 207.2142\n",
      "Epoch [62/100], Step [11800/13958], Loss: 197.0758\n",
      "Epoch [62/100], Step [11900/13958], Loss: 270.8817\n",
      "Epoch [62/100], Step [12000/13958], Loss: 239.5801\n",
      "Epoch [62/100], Step [12100/13958], Loss: 376.1697\n",
      "Epoch [62/100], Step [12200/13958], Loss: 352.0326\n",
      "Epoch [62/100], Step [12300/13958], Loss: 234.0871\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [62/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [63/100], Step [100/13958], Loss: 51.0732\n",
      "Epoch [63/100], Step [200/13958], Loss: 235.5497\n",
      "Epoch [63/100], Step [300/13958], Loss: 308.3647\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [63/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [64/100], Step [100/13958], Loss: 348.0482\n",
      "Epoch [64/100], Step [200/13958], Loss: 194.0668\n",
      "Epoch [64/100], Step [300/13958], Loss: 423.4556\n",
      "Epoch [64/100], Step [400/13958], Loss: 106.6560\n",
      "Epoch [64/100], Step [500/13958], Loss: 429.9764\n",
      "Epoch [64/100], Step [600/13958], Loss: 287.8556\n",
      "Epoch [64/100], Step [700/13958], Loss: 319.6887\n",
      "Epoch [64/100], Step [800/13958], Loss: 192.8337\n",
      "Epoch [64/100], Step [900/13958], Loss: 190.8577\n",
      "Epoch [64/100], Step [1000/13958], Loss: 620.2175\n",
      "Epoch [64/100], Step [1100/13958], Loss: 363.4833\n",
      "Epoch [64/100], Step [1200/13958], Loss: 340.1308\n",
      "Epoch [64/100], Step [1300/13958], Loss: 163.0594\n",
      "Epoch [64/100], Step [1400/13958], Loss: 237.9667\n",
      "Epoch [64/100], Step [1500/13958], Loss: 98.4080\n",
      "Epoch [64/100], Step [1600/13958], Loss: 147.0085\n",
      "Epoch [64/100], Step [1700/13958], Loss: 130.0589\n",
      "Epoch [64/100], Step [1800/13958], Loss: 129.6020\n",
      "Epoch [64/100], Step [1900/13958], Loss: 373.7056\n",
      "Epoch [64/100], Step [2000/13958], Loss: 528.5005\n",
      "Epoch [64/100], Step [2100/13958], Loss: 355.6510\n",
      "Epoch [64/100], Step [2200/13958], Loss: 163.9939\n",
      "Epoch [64/100], Step [2300/13958], Loss: 380.4129\n",
      "Epoch [64/100], Step [2400/13958], Loss: 281.2068\n",
      "Epoch [64/100], Step [2500/13958], Loss: 317.5372\n",
      "Epoch [64/100], Step [2600/13958], Loss: 241.5522\n",
      "Epoch [64/100], Step [2700/13958], Loss: 415.3126\n",
      "Epoch [64/100], Step [2800/13958], Loss: 299.2583\n",
      "Epoch [64/100], Step [2900/13958], Loss: 280.1614\n",
      "Epoch [64/100], Step [3000/13958], Loss: 249.9393\n",
      "Epoch [64/100], Step [3100/13958], Loss: 474.6907\n",
      "Epoch [64/100], Step [3200/13958], Loss: 82.8587\n",
      "Epoch [64/100], Step [3300/13958], Loss: 202.8484\n",
      "Epoch [64/100], Step [3400/13958], Loss: 200.5354\n",
      "Epoch [64/100], Step [3500/13958], Loss: 150.8785\n",
      "Epoch [64/100], Step [3600/13958], Loss: 359.2338\n",
      "Epoch [64/100], Step [3700/13958], Loss: 299.5201\n",
      "Epoch [64/100], Step [3800/13958], Loss: 150.8115\n",
      "Epoch [64/100], Step [3900/13958], Loss: 105.7675\n",
      "Epoch [64/100], Step [4000/13958], Loss: 154.2241\n",
      "Epoch [64/100], Step [4100/13958], Loss: 164.7455\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [64/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [65/100], Step [100/13958], Loss: 274.5636\n",
      "Epoch [65/100], Step [200/13958], Loss: 210.4727\n",
      "Epoch [65/100], Step [300/13958], Loss: 105.1421\n",
      "Epoch [65/100], Step [400/13958], Loss: 338.1282\n",
      "Epoch [65/100], Step [500/13958], Loss: 185.0612\n",
      "Epoch [65/100], Step [600/13958], Loss: 80.3715\n",
      "Epoch [65/100], Step [700/13958], Loss: 179.9924\n",
      "Epoch [65/100], Step [800/13958], Loss: 528.4344\n",
      "Epoch [65/100], Step [900/13958], Loss: 538.0746\n",
      "Epoch [65/100], Step [1000/13958], Loss: 547.5615\n",
      "Epoch [65/100], Step [1100/13958], Loss: 265.2041\n",
      "Epoch [65/100], Step [1200/13958], Loss: 281.6815\n",
      "Epoch [65/100], Step [1300/13958], Loss: 271.6933\n",
      "Epoch [65/100], Step [1400/13958], Loss: 85.4133\n",
      "Epoch [65/100], Step [1500/13958], Loss: 209.8076\n",
      "Epoch [65/100], Step [1600/13958], Loss: 80.8295\n",
      "Epoch [65/100], Step [1700/13958], Loss: 194.8058\n",
      "Epoch [65/100], Step [1800/13958], Loss: 124.9742\n",
      "Epoch [65/100], Step [1900/13958], Loss: 300.0446\n",
      "Epoch [65/100], Step [2000/13958], Loss: 424.1259\n",
      "Epoch [65/100], Step [2100/13958], Loss: 274.3181\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [65/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [66/100], Step [100/13958], Loss: 190.6932\n",
      "Epoch [66/100], Step [200/13958], Loss: 167.4119\n",
      "Epoch [66/100], Step [300/13958], Loss: 192.7808\n",
      "Epoch [66/100], Step [400/13958], Loss: 291.5951\n",
      "Epoch [66/100], Step [500/13958], Loss: 299.3503\n",
      "Epoch [66/100], Step [600/13958], Loss: 319.5231\n",
      "Epoch [66/100], Step [700/13958], Loss: 286.9273\n",
      "Epoch [66/100], Step [800/13958], Loss: 379.7307\n",
      "Epoch [66/100], Step [900/13958], Loss: 179.4574\n",
      "Epoch [66/100], Step [1000/13958], Loss: 328.4711\n",
      "Epoch [66/100], Step [1100/13958], Loss: 256.6316\n",
      "Epoch [66/100], Step [1200/13958], Loss: 145.7613\n",
      "Epoch [66/100], Step [1300/13958], Loss: 405.6423\n",
      "Epoch [66/100], Step [1400/13958], Loss: 309.6530\n",
      "Epoch [66/100], Step [1500/13958], Loss: 256.8365\n",
      "Epoch [66/100], Step [1600/13958], Loss: 178.9284\n",
      "Epoch [66/100], Step [1700/13958], Loss: 212.1427\n",
      "Epoch [66/100], Step [1800/13958], Loss: 107.6028\n",
      "Epoch [66/100], Step [1900/13958], Loss: 129.2581\n",
      "Epoch [66/100], Step [2000/13958], Loss: 195.5222\n",
      "Epoch [66/100], Step [2100/13958], Loss: 106.7823\n",
      "Epoch [66/100], Step [2200/13958], Loss: 62.9004\n",
      "Epoch [66/100], Step [2300/13958], Loss: 163.9381\n",
      "Epoch [66/100], Step [2400/13958], Loss: 267.5353\n",
      "Epoch [66/100], Step [2500/13958], Loss: 124.5980\n",
      "Epoch [66/100], Step [2600/13958], Loss: 131.0922\n",
      "Epoch [66/100], Step [2700/13958], Loss: 125.9509\n",
      "Epoch [66/100], Step [2800/13958], Loss: 189.6633\n",
      "Epoch [66/100], Step [2900/13958], Loss: 263.3791\n",
      "Epoch [66/100], Step [3000/13958], Loss: 219.1268\n",
      "Epoch [66/100], Step [3100/13958], Loss: 166.3423\n",
      "Epoch [66/100], Step [3200/13958], Loss: 269.8622\n",
      "Epoch [66/100], Step [3300/13958], Loss: 347.9941\n",
      "Epoch [66/100], Step [3400/13958], Loss: 331.8707\n",
      "Epoch [66/100], Step [3500/13958], Loss: 230.8136\n",
      "Epoch [66/100], Step [3600/13958], Loss: 395.8416\n",
      "Epoch [66/100], Step [3700/13958], Loss: 401.5148\n",
      "Epoch [66/100], Step [3800/13958], Loss: 298.2771\n",
      "Epoch [66/100], Step [3900/13958], Loss: 167.2433\n",
      "Epoch [66/100], Step [4000/13958], Loss: 283.9965\n",
      "Epoch [66/100], Step [4100/13958], Loss: 414.9617\n",
      "Epoch [66/100], Step [4200/13958], Loss: 103.3844\n",
      "Epoch [66/100], Step [4300/13958], Loss: 246.5367\n",
      "Epoch [66/100], Step [4400/13958], Loss: 139.6014\n",
      "Epoch [66/100], Step [4500/13958], Loss: 318.8838\n",
      "Epoch [66/100], Step [4600/13958], Loss: 209.7224\n",
      "Epoch [66/100], Step [4700/13958], Loss: 254.9414\n",
      "Epoch [66/100], Step [4800/13958], Loss: 179.0044\n",
      "Epoch [66/100], Step [4900/13958], Loss: 209.2458\n",
      "Epoch [66/100], Step [5000/13958], Loss: 301.3731\n",
      "Epoch [66/100], Step [5100/13958], Loss: 373.9355\n",
      "Epoch [66/100], Step [5200/13958], Loss: 131.6042\n",
      "Epoch [66/100], Step [5300/13958], Loss: 139.6303\n",
      "Epoch [66/100], Step [5400/13958], Loss: 119.4372\n",
      "Epoch [66/100], Step [5500/13958], Loss: 404.2702\n",
      "Epoch [66/100], Step [5600/13958], Loss: 246.7609\n",
      "Epoch [66/100], Step [5700/13958], Loss: 288.1734\n",
      "Epoch [66/100], Step [5800/13958], Loss: 430.7447\n",
      "Epoch [66/100], Step [5900/13958], Loss: 324.8084\n",
      "Epoch [66/100], Step [6000/13958], Loss: 289.1378\n",
      "Epoch [66/100], Step [6100/13958], Loss: 442.0665\n",
      "Epoch [66/100], Step [6200/13958], Loss: 464.7503\n",
      "Epoch [66/100], Step [6300/13958], Loss: 298.7017\n",
      "Epoch [66/100], Step [6400/13958], Loss: 290.2765\n",
      "Epoch [66/100], Step [6500/13958], Loss: 280.7586\n",
      "Epoch [66/100], Step [6600/13958], Loss: 130.1227\n",
      "Epoch [66/100], Step [6700/13958], Loss: 508.2092\n",
      "Epoch [66/100], Step [6800/13958], Loss: 226.4351\n",
      "Epoch [66/100], Step [6900/13958], Loss: 228.8370\n",
      "Epoch [66/100], Step [7000/13958], Loss: 198.5377\n",
      "Epoch [66/100], Step [7100/13958], Loss: 264.6405\n",
      "Epoch [66/100], Step [7200/13958], Loss: 618.6517\n",
      "Epoch [66/100], Step [7300/13958], Loss: 304.8503\n",
      "Epoch [66/100], Step [7400/13958], Loss: 248.2060\n",
      "Epoch [66/100], Step [7500/13958], Loss: 418.9325\n",
      "Epoch [66/100], Step [7600/13958], Loss: 187.4980\n",
      "Epoch [66/100], Step [7700/13958], Loss: 363.8820\n",
      "Epoch [66/100], Step [7800/13958], Loss: 107.2786\n",
      "Epoch [66/100], Step [7900/13958], Loss: 214.4650\n",
      "Epoch [66/100], Step [8000/13958], Loss: 318.5208\n",
      "Epoch [66/100], Step [8100/13958], Loss: 73.5233\n",
      "Epoch [66/100], Step [8200/13958], Loss: 159.9389\n",
      "Epoch [66/100], Step [8300/13958], Loss: 443.2299\n",
      "Epoch [66/100], Step [8400/13958], Loss: 612.8007\n",
      "Epoch [66/100], Step [8500/13958], Loss: 488.0708\n",
      "Epoch [66/100], Step [8600/13958], Loss: 265.0463\n",
      "Epoch [66/100], Step [8700/13958], Loss: 482.5995\n",
      "Epoch [66/100], Step [8800/13958], Loss: 155.2312\n",
      "Epoch [66/100], Step [8900/13958], Loss: 287.6705\n",
      "Epoch [66/100], Step [9000/13958], Loss: 330.4497\n",
      "Epoch [66/100], Step [9100/13958], Loss: 270.5924\n",
      "Epoch [66/100], Step [9200/13958], Loss: 203.6570\n",
      "Epoch [66/100], Step [9300/13958], Loss: 218.8399\n",
      "Epoch [66/100], Step [9400/13958], Loss: 262.2437\n",
      "Epoch [66/100], Step [9500/13958], Loss: 448.9708\n",
      "Epoch [66/100], Step [9600/13958], Loss: 99.3001\n",
      "Epoch [66/100], Step [9700/13958], Loss: 405.6129\n",
      "Epoch [66/100], Step [9800/13958], Loss: 101.8281\n",
      "Epoch [66/100], Step [9900/13958], Loss: 294.8815\n",
      "Epoch [66/100], Step [10000/13958], Loss: 471.3884\n",
      "Epoch [66/100], Step [10100/13958], Loss: 142.0429\n",
      "Epoch [66/100], Step [10200/13958], Loss: 209.2850\n",
      "Epoch [66/100], Step [10300/13958], Loss: 110.5790\n",
      "Epoch [66/100], Step [10400/13958], Loss: 233.1881\n",
      "Epoch [66/100], Step [10500/13958], Loss: 182.1798\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [66/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [67/100], Step [100/13958], Loss: 147.0278\n",
      "Epoch [67/100], Step [200/13958], Loss: 194.4851\n",
      "Epoch [67/100], Step [300/13958], Loss: 295.6858\n",
      "Epoch [67/100], Step [400/13958], Loss: 115.4789\n",
      "Epoch [67/100], Step [500/13958], Loss: 151.4252\n",
      "Epoch [67/100], Step [600/13958], Loss: 75.4808\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [67/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [68/100], Step [100/13958], Loss: 286.2331\n",
      "Epoch [68/100], Step [200/13958], Loss: 277.6789\n",
      "Epoch [68/100], Step [300/13958], Loss: 224.1446\n",
      "Epoch [68/100], Step [400/13958], Loss: 338.6948\n",
      "Epoch [68/100], Step [500/13958], Loss: 299.7185\n",
      "Epoch [68/100], Step [600/13958], Loss: 232.5687\n",
      "Epoch [68/100], Step [700/13958], Loss: 174.9805\n",
      "Epoch [68/100], Step [800/13958], Loss: 266.1302\n",
      "Epoch [68/100], Step [900/13958], Loss: 82.7572\n",
      "Epoch [68/100], Step [1000/13958], Loss: 149.1718\n",
      "Epoch [68/100], Step [1100/13958], Loss: 356.4010\n",
      "Epoch [68/100], Step [1200/13958], Loss: 107.5130\n",
      "Epoch [68/100], Step [1300/13958], Loss: 68.7216\n",
      "Epoch [68/100], Step [1400/13958], Loss: 119.4627\n",
      "Epoch [68/100], Step [1500/13958], Loss: 126.1601\n",
      "Epoch [68/100], Step [1600/13958], Loss: 230.8788\n",
      "Epoch [68/100], Step [1700/13958], Loss: 142.6818\n",
      "Epoch [68/100], Step [1800/13958], Loss: 378.6652\n",
      "Epoch [68/100], Step [1900/13958], Loss: 72.3238\n",
      "Epoch [68/100], Step [2000/13958], Loss: 373.3609\n",
      "Epoch [68/100], Step [2100/13958], Loss: 310.1507\n",
      "Epoch [68/100], Step [2200/13958], Loss: 83.3079\n",
      "Epoch [68/100], Step [2300/13958], Loss: 482.5162\n",
      "Epoch [68/100], Step [2400/13958], Loss: 153.4463\n",
      "Epoch [68/100], Step [2500/13958], Loss: 190.8470\n",
      "Epoch [68/100], Step [2600/13958], Loss: 192.4014\n",
      "Epoch [68/100], Step [2700/13958], Loss: 220.6288\n",
      "Epoch [68/100], Step [2800/13958], Loss: 315.0722\n",
      "Epoch [68/100], Step [2900/13958], Loss: 249.3512\n",
      "Epoch [68/100], Step [3000/13958], Loss: 187.5482\n",
      "Epoch [68/100], Step [3100/13958], Loss: 99.5871\n",
      "Epoch [68/100], Step [3200/13958], Loss: 163.4404\n",
      "Epoch [68/100], Step [3300/13958], Loss: 329.5086\n",
      "Epoch [68/100], Step [3400/13958], Loss: 173.4453\n",
      "Epoch [68/100], Step [3500/13958], Loss: 305.1843\n",
      "Epoch [68/100], Step [3600/13958], Loss: 152.3273\n",
      "Epoch [68/100], Step [3700/13958], Loss: 115.2667\n",
      "Epoch [68/100], Step [3800/13958], Loss: 218.0085\n",
      "Epoch [68/100], Step [3900/13958], Loss: 179.9790\n",
      "Epoch [68/100], Step [4000/13958], Loss: 421.3054\n",
      "Epoch [68/100], Step [4100/13958], Loss: 77.9178\n",
      "Epoch [68/100], Step [4200/13958], Loss: 477.5921\n",
      "Epoch [68/100], Step [4300/13958], Loss: 170.6679\n",
      "Epoch [68/100], Step [4400/13958], Loss: 186.7638\n",
      "Epoch [68/100], Step [4500/13958], Loss: 332.5713\n",
      "Epoch [68/100], Step [4600/13958], Loss: 382.9411\n",
      "Epoch [68/100], Step [4700/13958], Loss: 204.6668\n",
      "Epoch [68/100], Step [4800/13958], Loss: 364.7649\n",
      "Epoch [68/100], Step [4900/13958], Loss: 165.6232\n",
      "Epoch [68/100], Step [5000/13958], Loss: 446.9374\n",
      "Epoch [68/100], Step [5100/13958], Loss: 266.7699\n",
      "Epoch [68/100], Step [5200/13958], Loss: 310.0664\n",
      "Epoch [68/100], Step [5300/13958], Loss: 214.7663\n",
      "Epoch [68/100], Step [5400/13958], Loss: 110.3218\n",
      "Epoch [68/100], Step [5500/13958], Loss: 138.9664\n",
      "Epoch [68/100], Step [5600/13958], Loss: 164.1143\n",
      "Epoch [68/100], Step [5700/13958], Loss: 232.7244\n",
      "Epoch [68/100], Step [5800/13958], Loss: 528.9835\n",
      "Epoch [68/100], Step [5900/13958], Loss: 84.4662\n",
      "Epoch [68/100], Step [6000/13958], Loss: 149.6838\n",
      "Epoch [68/100], Step [6100/13958], Loss: 126.9707\n",
      "Epoch [68/100], Step [6200/13958], Loss: 323.8347\n",
      "Epoch [68/100], Step [6300/13958], Loss: 184.2778\n",
      "Epoch [68/100], Step [6400/13958], Loss: 433.1935\n",
      "Epoch [68/100], Step [6500/13958], Loss: 393.3601\n",
      "Epoch [68/100], Step [6600/13958], Loss: 268.4623\n",
      "Epoch [68/100], Step [6700/13958], Loss: 155.5541\n",
      "Epoch [68/100], Step [6800/13958], Loss: 512.2978\n",
      "Epoch [68/100], Step [6900/13958], Loss: 204.2142\n",
      "Epoch [68/100], Step [7000/13958], Loss: 314.7587\n",
      "Epoch [68/100], Step [7100/13958], Loss: 135.7764\n",
      "Epoch [68/100], Step [7200/13958], Loss: 296.4816\n",
      "Epoch [68/100], Step [7300/13958], Loss: 426.4532\n",
      "Epoch [68/100], Step [7400/13958], Loss: 526.1945\n",
      "Epoch [68/100], Step [7500/13958], Loss: 227.1458\n",
      "Epoch [68/100], Step [7600/13958], Loss: 318.4488\n",
      "Epoch [68/100], Step [7700/13958], Loss: 320.8491\n",
      "Epoch [68/100], Step [7800/13958], Loss: 160.8001\n",
      "Epoch [68/100], Step [7900/13958], Loss: 270.2926\n",
      "Epoch [68/100], Step [8000/13958], Loss: 57.4579\n",
      "Epoch [68/100], Step [8100/13958], Loss: 707.2834\n",
      "Epoch [68/100], Step [8200/13958], Loss: 260.9168\n",
      "Epoch [68/100], Step [8300/13958], Loss: 197.1295\n",
      "Epoch [68/100], Step [8400/13958], Loss: 190.7393\n",
      "Epoch [68/100], Step [8500/13958], Loss: 379.5992\n",
      "Epoch [68/100], Step [8600/13958], Loss: 130.7774\n",
      "Epoch [68/100], Step [8700/13958], Loss: 263.6400\n",
      "Epoch [68/100], Step [8800/13958], Loss: 257.6431\n",
      "Epoch [68/100], Step [8900/13958], Loss: 333.0435\n",
      "Epoch [68/100], Step [9000/13958], Loss: 221.6647\n",
      "Epoch [68/100], Step [9100/13958], Loss: 165.9503\n",
      "Epoch [68/100], Step [9200/13958], Loss: 367.7803\n",
      "Epoch [68/100], Step [9300/13958], Loss: 131.2339\n",
      "Epoch [68/100], Step [9400/13958], Loss: 189.5243\n",
      "Epoch [68/100], Step [9500/13958], Loss: 337.5703\n",
      "Epoch [68/100], Step [9600/13958], Loss: 134.7980\n",
      "Epoch [68/100], Step [9700/13958], Loss: 238.4888\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [68/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [69/100], Step [100/13958], Loss: 261.4777\n",
      "Epoch [69/100], Step [200/13958], Loss: 53.8512\n",
      "Epoch [69/100], Step [300/13958], Loss: 461.3769\n",
      "Epoch [69/100], Step [400/13958], Loss: 345.8691\n",
      "Epoch [69/100], Step [500/13958], Loss: 109.0734\n",
      "Epoch [69/100], Step [600/13958], Loss: 246.1413\n",
      "Epoch [69/100], Step [700/13958], Loss: 418.2739\n",
      "Epoch [69/100], Step [800/13958], Loss: 328.3527\n",
      "Epoch [69/100], Step [900/13958], Loss: 108.0862\n",
      "Epoch [69/100], Step [1000/13958], Loss: 153.6739\n",
      "Epoch [69/100], Step [1100/13958], Loss: 138.0497\n",
      "Epoch [69/100], Step [1200/13958], Loss: 298.7050\n",
      "Epoch [69/100], Step [1300/13958], Loss: 270.3610\n",
      "Epoch [69/100], Step [1400/13958], Loss: 203.3177\n",
      "Epoch [69/100], Step [1500/13958], Loss: 265.4708\n",
      "Epoch [69/100], Step [1600/13958], Loss: 281.5285\n",
      "Epoch [69/100], Step [1700/13958], Loss: 73.0006\n",
      "Epoch [69/100], Step [1800/13958], Loss: 365.9244\n",
      "Epoch [69/100], Step [1900/13958], Loss: 265.4777\n",
      "Epoch [69/100], Step [2000/13958], Loss: 528.5346\n",
      "Epoch [69/100], Step [2100/13958], Loss: 182.7383\n",
      "Epoch [69/100], Step [2200/13958], Loss: 232.6190\n",
      "Epoch [69/100], Step [2300/13958], Loss: 212.4796\n",
      "Epoch [69/100], Step [2400/13958], Loss: 207.0716\n",
      "Epoch [69/100], Step [2500/13958], Loss: 196.8826\n",
      "Epoch [69/100], Step [2600/13958], Loss: 361.2833\n",
      "Epoch [69/100], Step [2700/13958], Loss: 659.9633\n",
      "Epoch [69/100], Step [2800/13958], Loss: 147.4527\n",
      "Epoch [69/100], Step [2900/13958], Loss: 175.9957\n",
      "Epoch [69/100], Step [3000/13958], Loss: 167.3727\n",
      "Epoch [69/100], Step [3100/13958], Loss: 182.8447\n",
      "Epoch [69/100], Step [3200/13958], Loss: 345.3072\n",
      "Epoch [69/100], Step [3300/13958], Loss: 220.2697\n",
      "Epoch [69/100], Step [3400/13958], Loss: 205.4737\n",
      "Epoch [69/100], Step [3500/13958], Loss: 409.6829\n",
      "Epoch [69/100], Step [3600/13958], Loss: 180.0322\n",
      "Epoch [69/100], Step [3700/13958], Loss: 125.1609\n",
      "Epoch [69/100], Step [3800/13958], Loss: 185.3764\n",
      "Epoch [69/100], Step [3900/13958], Loss: 231.9107\n",
      "Epoch [69/100], Step [4000/13958], Loss: 189.0832\n",
      "Epoch [69/100], Step [4100/13958], Loss: 417.6789\n",
      "Epoch [69/100], Step [4200/13958], Loss: 166.9188\n",
      "Epoch [69/100], Step [4300/13958], Loss: 456.3309\n",
      "Epoch [69/100], Step [4400/13958], Loss: 143.7471\n",
      "Epoch [69/100], Step [4500/13958], Loss: 225.6060\n",
      "Epoch [69/100], Step [4600/13958], Loss: 95.5415\n",
      "Epoch [69/100], Step [4700/13958], Loss: 381.1760\n",
      "Epoch [69/100], Step [4800/13958], Loss: 151.9970\n",
      "Epoch [69/100], Step [4900/13958], Loss: 238.8662\n",
      "Epoch [69/100], Step [5000/13958], Loss: 205.0720\n",
      "Epoch [69/100], Step [5100/13958], Loss: 259.8649\n",
      "Epoch [69/100], Step [5200/13958], Loss: 444.2054\n",
      "Epoch [69/100], Step [5300/13958], Loss: 178.8999\n",
      "Epoch [69/100], Step [5400/13958], Loss: 239.7610\n",
      "Epoch [69/100], Step [5500/13958], Loss: 427.0171\n",
      "Epoch [69/100], Step [5600/13958], Loss: 366.3877\n",
      "Epoch [69/100], Step [5700/13958], Loss: 231.8279\n",
      "Epoch [69/100], Step [5800/13958], Loss: 337.0159\n",
      "Epoch [69/100], Step [5900/13958], Loss: 172.9046\n",
      "Epoch [69/100], Step [6000/13958], Loss: 93.0597\n",
      "Epoch [69/100], Step [6100/13958], Loss: 118.7976\n",
      "Epoch [69/100], Step [6200/13958], Loss: 578.9443\n",
      "Epoch [69/100], Step [6300/13958], Loss: 303.0099\n",
      "Epoch [69/100], Step [6400/13958], Loss: 136.2904\n",
      "Epoch [69/100], Step [6500/13958], Loss: 383.7484\n",
      "Epoch [69/100], Step [6600/13958], Loss: 89.3819\n",
      "Epoch [69/100], Step [6700/13958], Loss: 221.8741\n",
      "Epoch [69/100], Step [6800/13958], Loss: 566.9287\n",
      "Epoch [69/100], Step [6900/13958], Loss: 76.8576\n",
      "Epoch [69/100], Step [7000/13958], Loss: 489.5697\n",
      "Epoch [69/100], Step [7100/13958], Loss: 96.2322\n",
      "Epoch [69/100], Step [7200/13958], Loss: 418.8400\n",
      "Epoch [69/100], Step [7300/13958], Loss: 106.2033\n",
      "Epoch [69/100], Step [7400/13958], Loss: 280.5092\n",
      "Epoch [69/100], Step [7500/13958], Loss: 371.0978\n",
      "Epoch [69/100], Step [7600/13958], Loss: 219.5820\n",
      "Epoch [69/100], Step [7700/13958], Loss: 265.0607\n",
      "Epoch [69/100], Step [7800/13958], Loss: 187.3829\n",
      "Epoch [69/100], Step [7900/13958], Loss: 504.6688\n",
      "Epoch [69/100], Step [8000/13958], Loss: 328.6453\n",
      "Epoch [69/100], Step [8100/13958], Loss: 205.8654\n",
      "Epoch [69/100], Step [8200/13958], Loss: 240.0062\n",
      "Epoch [69/100], Step [8300/13958], Loss: 153.8436\n",
      "Epoch [69/100], Step [8400/13958], Loss: 141.7680\n",
      "Epoch [69/100], Step [8500/13958], Loss: 93.2662\n",
      "Epoch [69/100], Step [8600/13958], Loss: 246.3798\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [69/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [70/100], Step [100/13958], Loss: 143.5052\n",
      "Epoch [70/100], Step [200/13958], Loss: 340.8683\n",
      "Epoch [70/100], Step [300/13958], Loss: 303.7074\n",
      "Epoch [70/100], Step [400/13958], Loss: 533.7701\n",
      "Epoch [70/100], Step [500/13958], Loss: 208.8428\n",
      "Epoch [70/100], Step [600/13958], Loss: 319.2323\n",
      "Epoch [70/100], Step [700/13958], Loss: 447.6973\n",
      "Epoch [70/100], Step [800/13958], Loss: 63.6185\n",
      "Epoch [70/100], Step [900/13958], Loss: 129.1166\n",
      "Epoch [70/100], Step [1000/13958], Loss: 576.7317\n",
      "Epoch [70/100], Step [1100/13958], Loss: 280.2309\n",
      "Epoch [70/100], Step [1200/13958], Loss: 95.5468\n",
      "Epoch [70/100], Step [1300/13958], Loss: 515.5399\n",
      "Epoch [70/100], Step [1400/13958], Loss: 266.2477\n",
      "Epoch [70/100], Step [1500/13958], Loss: 243.8120\n",
      "Epoch [70/100], Step [1600/13958], Loss: 316.2573\n",
      "Epoch [70/100], Step [1700/13958], Loss: 101.4526\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [70/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [71/100], Step [100/13958], Loss: 503.7627\n",
      "Epoch [71/100], Step [200/13958], Loss: 208.5386\n",
      "Epoch [71/100], Step [300/13958], Loss: 323.6782\n",
      "Epoch [71/100], Step [400/13958], Loss: 126.3339\n",
      "Epoch [71/100], Step [500/13958], Loss: 218.3034\n",
      "Epoch [71/100], Step [600/13958], Loss: 276.3828\n",
      "Epoch [71/100], Step [700/13958], Loss: 403.8482\n",
      "Epoch [71/100], Step [800/13958], Loss: 257.2893\n",
      "Epoch [71/100], Step [900/13958], Loss: 266.1313\n",
      "Epoch [71/100], Step [1000/13958], Loss: 193.3859\n",
      "Epoch [71/100], Step [1100/13958], Loss: 441.5935\n",
      "Epoch [71/100], Step [1200/13958], Loss: 457.4768\n",
      "Epoch [71/100], Step [1300/13958], Loss: 226.7718\n",
      "Epoch [71/100], Step [1400/13958], Loss: 258.1437\n",
      "Epoch [71/100], Step [1500/13958], Loss: 239.9435\n",
      "Epoch [71/100], Step [1600/13958], Loss: 492.5650\n",
      "Epoch [71/100], Step [1700/13958], Loss: 395.0260\n",
      "Epoch [71/100], Step [1800/13958], Loss: 234.1860\n",
      "Epoch [71/100], Step [1900/13958], Loss: 385.3284\n",
      "Epoch [71/100], Step [2000/13958], Loss: 550.6336\n",
      "Epoch [71/100], Step [2100/13958], Loss: 101.4896\n",
      "Epoch [71/100], Step [2200/13958], Loss: 472.4315\n",
      "Epoch [71/100], Step [2300/13958], Loss: 401.0511\n",
      "Epoch [71/100], Step [2400/13958], Loss: 370.0242\n",
      "Epoch [71/100], Step [2500/13958], Loss: 472.5679\n",
      "Epoch [71/100], Step [2600/13958], Loss: 229.1569\n",
      "Epoch [71/100], Step [2700/13958], Loss: 362.4468\n",
      "Epoch [71/100], Step [2800/13958], Loss: 460.5943\n",
      "Epoch [71/100], Step [2900/13958], Loss: 178.6527\n",
      "Epoch [71/100], Step [3000/13958], Loss: 275.2582\n",
      "Epoch [71/100], Step [3100/13958], Loss: 132.7955\n",
      "Epoch [71/100], Step [3200/13958], Loss: 153.3142\n",
      "Epoch [71/100], Step [3300/13958], Loss: 481.4334\n",
      "Epoch [71/100], Step [3400/13958], Loss: 561.1868\n",
      "Epoch [71/100], Step [3500/13958], Loss: 271.3591\n",
      "Epoch [71/100], Step [3600/13958], Loss: 201.6775\n",
      "Epoch [71/100], Step [3700/13958], Loss: 363.2043\n",
      "Epoch [71/100], Step [3800/13958], Loss: 228.2419\n",
      "Epoch [71/100], Step [3900/13958], Loss: 306.7960\n",
      "Epoch [71/100], Step [4000/13958], Loss: 298.1356\n",
      "Epoch [71/100], Step [4100/13958], Loss: 232.0963\n",
      "Epoch [71/100], Step [4200/13958], Loss: 268.0931\n",
      "Epoch [71/100], Step [4300/13958], Loss: 137.1095\n",
      "Epoch [71/100], Step [4400/13958], Loss: 106.6687\n",
      "Epoch [71/100], Step [4500/13958], Loss: 212.5282\n",
      "Epoch [71/100], Step [4600/13958], Loss: 335.0128\n",
      "Epoch [71/100], Step [4700/13958], Loss: 186.2895\n",
      "Epoch [71/100], Step [4800/13958], Loss: 200.4206\n",
      "Epoch [71/100], Step [4900/13958], Loss: 159.3387\n",
      "Epoch [71/100], Step [5000/13958], Loss: 341.8070\n",
      "Epoch [71/100], Step [5100/13958], Loss: 467.2859\n",
      "Epoch [71/100], Step [5200/13958], Loss: 269.2759\n",
      "Epoch [71/100], Step [5300/13958], Loss: 230.6281\n",
      "Epoch [71/100], Step [5400/13958], Loss: 334.8315\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [71/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [72/100], Step [100/13958], Loss: 321.3235\n",
      "Epoch [72/100], Step [200/13958], Loss: 179.9964\n",
      "Epoch [72/100], Step [300/13958], Loss: 254.6552\n",
      "Epoch [72/100], Step [400/13958], Loss: 102.5237\n",
      "Epoch [72/100], Step [500/13958], Loss: 340.8156\n",
      "Epoch [72/100], Step [600/13958], Loss: 321.0677\n",
      "Epoch [72/100], Step [700/13958], Loss: 401.8705\n",
      "Epoch [72/100], Step [800/13958], Loss: 329.7696\n",
      "Epoch [72/100], Step [900/13958], Loss: 181.7080\n",
      "Epoch [72/100], Step [1000/13958], Loss: 480.9193\n",
      "Epoch [72/100], Step [1100/13958], Loss: 324.8984\n",
      "Epoch [72/100], Step [1200/13958], Loss: 472.1167\n",
      "Epoch [72/100], Step [1300/13958], Loss: 188.4004\n",
      "Epoch [72/100], Step [1400/13958], Loss: 166.4430\n",
      "Epoch [72/100], Step [1500/13958], Loss: 142.5007\n",
      "Epoch [72/100], Step [1600/13958], Loss: 302.7194\n",
      "Epoch [72/100], Step [1700/13958], Loss: 269.8441\n",
      "Epoch [72/100], Step [1800/13958], Loss: 364.3659\n",
      "Epoch [72/100], Step [1900/13958], Loss: 283.6191\n",
      "Epoch [72/100], Step [2000/13958], Loss: 196.7843\n",
      "Epoch [72/100], Step [2100/13958], Loss: 196.2703\n",
      "Epoch [72/100], Step [2200/13958], Loss: 197.9308\n",
      "Epoch [72/100], Step [2300/13958], Loss: 460.5274\n",
      "Epoch [72/100], Step [2400/13958], Loss: 343.5321\n",
      "Epoch [72/100], Step [2500/13958], Loss: 200.4144\n",
      "Epoch [72/100], Step [2600/13958], Loss: 150.1773\n",
      "Epoch [72/100], Step [2700/13958], Loss: 256.4684\n",
      "Epoch [72/100], Step [2800/13958], Loss: 318.6591\n",
      "Epoch [72/100], Step [2900/13958], Loss: 154.3391\n",
      "Epoch [72/100], Step [3000/13958], Loss: 322.6661\n",
      "Epoch [72/100], Step [3100/13958], Loss: 570.2505\n",
      "Epoch [72/100], Step [3200/13958], Loss: 227.7367\n",
      "Epoch [72/100], Step [3300/13958], Loss: 90.1572\n",
      "Epoch [72/100], Step [3400/13958], Loss: 153.9880\n",
      "Epoch [72/100], Step [3500/13958], Loss: 210.9221\n",
      "Epoch [72/100], Step [3600/13958], Loss: 80.2411\n",
      "Epoch [72/100], Step [3700/13958], Loss: 282.3385\n",
      "Epoch [72/100], Step [3800/13958], Loss: 303.1124\n",
      "Epoch [72/100], Step [3900/13958], Loss: 75.1646\n",
      "Epoch [72/100], Step [4000/13958], Loss: 103.7905\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [72/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [73/100], Step [100/13958], Loss: 162.9897\n",
      "Epoch [73/100], Step [200/13958], Loss: 446.7666\n",
      "Epoch [73/100], Step [300/13958], Loss: 242.3488\n",
      "Epoch [73/100], Step [400/13958], Loss: 364.2977\n",
      "Epoch [73/100], Step [500/13958], Loss: 346.1817\n",
      "Epoch [73/100], Step [600/13958], Loss: 142.4852\n",
      "Epoch [73/100], Step [700/13958], Loss: 201.2647\n",
      "Epoch [73/100], Step [800/13958], Loss: 90.1407\n",
      "Epoch [73/100], Step [900/13958], Loss: 373.3659\n",
      "Epoch [73/100], Step [1000/13958], Loss: 257.2353\n",
      "Epoch [73/100], Step [1100/13958], Loss: 328.1213\n",
      "Epoch [73/100], Step [1200/13958], Loss: 388.2711\n",
      "Epoch [73/100], Step [1300/13958], Loss: 479.1605\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [73/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [74/100], Step [100/13958], Loss: 182.0400\n",
      "Epoch [74/100], Step [200/13958], Loss: 261.8806\n",
      "Epoch [74/100], Step [300/13958], Loss: 195.0217\n",
      "Epoch [74/100], Step [400/13958], Loss: 311.2266\n",
      "Epoch [74/100], Step [500/13958], Loss: 458.7844\n",
      "Epoch [74/100], Step [600/13958], Loss: 281.2491\n",
      "Epoch [74/100], Step [700/13958], Loss: 226.2749\n",
      "Epoch [74/100], Step [800/13958], Loss: 176.3861\n",
      "Epoch [74/100], Step [900/13958], Loss: 184.7630\n",
      "Epoch [74/100], Step [1000/13958], Loss: 299.1537\n",
      "Epoch [74/100], Step [1100/13958], Loss: 230.3965\n",
      "Epoch [74/100], Step [1200/13958], Loss: 199.4377\n",
      "Epoch [74/100], Step [1300/13958], Loss: 245.1132\n",
      "Epoch [74/100], Step [1400/13958], Loss: 84.2117\n",
      "Epoch [74/100], Step [1500/13958], Loss: 210.9353\n",
      "Epoch [74/100], Step [1600/13958], Loss: 222.5641\n",
      "Epoch [74/100], Step [1700/13958], Loss: 172.9340\n",
      "Epoch [74/100], Step [1800/13958], Loss: 244.0246\n",
      "Epoch [74/100], Step [1900/13958], Loss: 281.4018\n",
      "Epoch [74/100], Step [2000/13958], Loss: 170.6861\n",
      "Epoch [74/100], Step [2100/13958], Loss: 121.7670\n",
      "Epoch [74/100], Step [2200/13958], Loss: 152.4765\n",
      "Epoch [74/100], Step [2300/13958], Loss: 204.4939\n",
      "Epoch [74/100], Step [2400/13958], Loss: 135.5867\n",
      "Epoch [74/100], Step [2500/13958], Loss: 252.9634\n",
      "Epoch [74/100], Step [2600/13958], Loss: 330.9294\n",
      "Epoch [74/100], Step [2700/13958], Loss: 300.3693\n",
      "Epoch [74/100], Step [2800/13958], Loss: 531.5792\n",
      "Epoch [74/100], Step [2900/13958], Loss: 336.4448\n",
      "Epoch [74/100], Step [3000/13958], Loss: 265.1386\n",
      "Epoch [74/100], Step [3100/13958], Loss: 156.4648\n",
      "Epoch [74/100], Step [3200/13958], Loss: 268.6181\n",
      "Epoch [74/100], Step [3300/13958], Loss: 251.2307\n",
      "Epoch [74/100], Step [3400/13958], Loss: 228.2929\n",
      "Epoch [74/100], Step [3500/13958], Loss: 103.3984\n",
      "Epoch [74/100], Step [3600/13958], Loss: 137.0135\n",
      "Epoch [74/100], Step [3700/13958], Loss: 352.4295\n",
      "Epoch [74/100], Step [3800/13958], Loss: 135.8208\n",
      "Epoch [74/100], Step [3900/13958], Loss: 238.9605\n",
      "Epoch [74/100], Step [4000/13958], Loss: 99.6106\n",
      "Epoch [74/100], Step [4100/13958], Loss: 284.9367\n",
      "Epoch [74/100], Step [4200/13958], Loss: 258.9080\n",
      "Epoch [74/100], Step [4300/13958], Loss: 301.7082\n",
      "Epoch [74/100], Step [4400/13958], Loss: 560.2833\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [74/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [75/100], Step [100/13958], Loss: 192.3763\n",
      "Epoch [75/100], Step [200/13958], Loss: 516.3629\n",
      "Epoch [75/100], Step [300/13958], Loss: 433.0046\n",
      "Epoch [75/100], Step [400/13958], Loss: 422.6168\n",
      "Epoch [75/100], Step [500/13958], Loss: 285.0304\n",
      "Epoch [75/100], Step [600/13958], Loss: 426.7221\n",
      "Epoch [75/100], Step [700/13958], Loss: 375.6073\n",
      "Epoch [75/100], Step [800/13958], Loss: 239.6757\n",
      "Epoch [75/100], Step [900/13958], Loss: 114.0999\n",
      "Epoch [75/100], Step [1000/13958], Loss: 169.2779\n",
      "Epoch [75/100], Step [1100/13958], Loss: 385.7950\n",
      "Epoch [75/100], Step [1200/13958], Loss: 270.4118\n",
      "Epoch [75/100], Step [1300/13958], Loss: 169.0037\n",
      "Epoch [75/100], Step [1400/13958], Loss: 239.9268\n",
      "Epoch [75/100], Step [1500/13958], Loss: 319.5859\n",
      "Epoch [75/100], Step [1600/13958], Loss: 102.2098\n",
      "Epoch [75/100], Step [1700/13958], Loss: 279.1237\n",
      "Epoch [75/100], Step [1800/13958], Loss: 287.8957\n",
      "Epoch [75/100], Step [1900/13958], Loss: 213.5745\n",
      "Epoch [75/100], Step [2000/13958], Loss: 200.9633\n",
      "Epoch [75/100], Step [2100/13958], Loss: 182.0342\n",
      "Epoch [75/100], Step [2200/13958], Loss: 288.6571\n",
      "Epoch [75/100], Step [2300/13958], Loss: 127.7016\n",
      "Epoch [75/100], Step [2400/13958], Loss: 140.7452\n",
      "Epoch [75/100], Step [2500/13958], Loss: 385.1254\n",
      "Epoch [75/100], Step [2600/13958], Loss: 274.0339\n",
      "Epoch [75/100], Step [2700/13958], Loss: 275.1313\n",
      "Epoch [75/100], Step [2800/13958], Loss: 323.6427\n",
      "Epoch [75/100], Step [2900/13958], Loss: 241.9174\n",
      "Epoch [75/100], Step [3000/13958], Loss: 205.5435\n",
      "Epoch [75/100], Step [3100/13958], Loss: 193.1483\n",
      "Epoch [75/100], Step [3200/13958], Loss: 113.9218\n",
      "Epoch [75/100], Step [3300/13958], Loss: 197.2425\n",
      "Epoch [75/100], Step [3400/13958], Loss: 172.6114\n",
      "Epoch [75/100], Step [3500/13958], Loss: 388.3289\n",
      "Epoch [75/100], Step [3600/13958], Loss: 382.5706\n",
      "Epoch [75/100], Step [3700/13958], Loss: 376.6547\n",
      "Epoch [75/100], Step [3800/13958], Loss: 357.3021\n",
      "Epoch [75/100], Step [3900/13958], Loss: 66.3335\n",
      "Epoch [75/100], Step [4000/13958], Loss: 296.0577\n",
      "Epoch [75/100], Step [4100/13958], Loss: 532.3138\n",
      "Epoch [75/100], Step [4200/13958], Loss: 426.0571\n",
      "Epoch [75/100], Step [4300/13958], Loss: 146.9919\n",
      "Epoch [75/100], Step [4400/13958], Loss: 184.2035\n",
      "Epoch [75/100], Step [4500/13958], Loss: 195.4771\n",
      "Epoch [75/100], Step [4600/13958], Loss: 318.1664\n",
      "Epoch [75/100], Step [4700/13958], Loss: 446.6641\n",
      "Epoch [75/100], Step [4800/13958], Loss: 199.2751\n",
      "Epoch [75/100], Step [4900/13958], Loss: 211.4604\n",
      "Epoch [75/100], Step [5000/13958], Loss: 247.5070\n",
      "Epoch [75/100], Step [5100/13958], Loss: 235.6136\n",
      "Epoch [75/100], Step [5200/13958], Loss: 469.3483\n",
      "Epoch [75/100], Step [5300/13958], Loss: 294.7533\n",
      "Epoch [75/100], Step [5400/13958], Loss: 53.0713\n",
      "Epoch [75/100], Step [5500/13958], Loss: 355.3243\n",
      "Epoch [75/100], Step [5600/13958], Loss: 139.8594\n",
      "Epoch [75/100], Step [5700/13958], Loss: 182.9862\n",
      "Epoch [75/100], Step [5800/13958], Loss: 209.7308\n",
      "Epoch [75/100], Step [5900/13958], Loss: 251.2918\n",
      "Epoch [75/100], Step [6000/13958], Loss: 166.0339\n",
      "Epoch [75/100], Step [6100/13958], Loss: 241.5306\n",
      "Epoch [75/100], Step [6200/13958], Loss: 151.1384\n",
      "Epoch [75/100], Step [6300/13958], Loss: 258.0021\n",
      "Epoch [75/100], Step [6400/13958], Loss: 297.7423\n",
      "Epoch [75/100], Step [6500/13958], Loss: 425.1502\n",
      "Epoch [75/100], Step [6600/13958], Loss: 632.5909\n",
      "Epoch [75/100], Step [6700/13958], Loss: 367.2532\n",
      "Epoch [75/100], Step [6800/13958], Loss: 475.6632\n",
      "Epoch [75/100], Step [6900/13958], Loss: 241.8142\n",
      "Epoch [75/100], Step [7000/13958], Loss: 288.6537\n",
      "Epoch [75/100], Step [7100/13958], Loss: 371.6579\n",
      "Epoch [75/100], Step [7200/13958], Loss: 258.5871\n",
      "Epoch [75/100], Step [7300/13958], Loss: 432.1423\n",
      "Epoch [75/100], Step [7400/13958], Loss: 89.6498\n",
      "Epoch [75/100], Step [7500/13958], Loss: 109.6893\n",
      "Epoch [75/100], Step [7600/13958], Loss: 135.8251\n",
      "Epoch [75/100], Step [7700/13958], Loss: 442.4577\n",
      "Epoch [75/100], Step [7800/13958], Loss: 162.4314\n",
      "Epoch [75/100], Step [7900/13958], Loss: 261.1723\n",
      "Epoch [75/100], Step [8000/13958], Loss: 228.3784\n",
      "Epoch [75/100], Step [8100/13958], Loss: 422.2329\n",
      "Epoch [75/100], Step [8200/13958], Loss: 282.3228\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [75/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [76/100], Step [100/13958], Loss: 277.9831\n",
      "Epoch [76/100], Step [200/13958], Loss: 286.2836\n",
      "Epoch [76/100], Step [300/13958], Loss: 363.0628\n",
      "Epoch [76/100], Step [400/13958], Loss: 85.3089\n",
      "Epoch [76/100], Step [500/13958], Loss: 367.5543\n",
      "Epoch [76/100], Step [600/13958], Loss: 140.1646\n",
      "Epoch [76/100], Step [700/13958], Loss: 220.4587\n",
      "Epoch [76/100], Step [800/13958], Loss: 107.6839\n",
      "Epoch [76/100], Step [900/13958], Loss: 63.0721\n",
      "Epoch [76/100], Step [1000/13958], Loss: 198.3712\n",
      "Epoch [76/100], Step [1100/13958], Loss: 158.0461\n",
      "Epoch [76/100], Step [1200/13958], Loss: 231.6225\n",
      "Epoch [76/100], Step [1300/13958], Loss: 135.3963\n",
      "Epoch [76/100], Step [1400/13958], Loss: 247.9376\n",
      "Epoch [76/100], Step [1500/13958], Loss: 236.7081\n",
      "Epoch [76/100], Step [1600/13958], Loss: 179.5823\n",
      "Epoch [76/100], Step [1700/13958], Loss: 152.8124\n",
      "Epoch [76/100], Step [1800/13958], Loss: 315.2321\n",
      "Epoch [76/100], Step [1900/13958], Loss: 199.3623\n",
      "Epoch [76/100], Step [2000/13958], Loss: 103.7615\n",
      "Epoch [76/100], Step [2100/13958], Loss: 113.0516\n",
      "Epoch [76/100], Step [2200/13958], Loss: 124.7594\n",
      "Epoch [76/100], Step [2300/13958], Loss: 165.9061\n",
      "Epoch [76/100], Step [2400/13958], Loss: 364.8778\n",
      "Epoch [76/100], Step [2500/13958], Loss: 123.8777\n",
      "Epoch [76/100], Step [2600/13958], Loss: 296.3563\n",
      "Epoch [76/100], Step [2700/13958], Loss: 223.1754\n",
      "Epoch [76/100], Step [2800/13958], Loss: 243.0461\n",
      "Epoch [76/100], Step [2900/13958], Loss: 298.4147\n",
      "Epoch [76/100], Step [3000/13958], Loss: 184.8595\n",
      "Epoch [76/100], Step [3100/13958], Loss: 181.4565\n",
      "Epoch [76/100], Step [3200/13958], Loss: 130.5340\n",
      "Epoch [76/100], Step [3300/13958], Loss: 284.9363\n",
      "Epoch [76/100], Step [3400/13958], Loss: 181.6152\n",
      "Epoch [76/100], Step [3500/13958], Loss: 312.0970\n",
      "Epoch [76/100], Step [3600/13958], Loss: 156.1929\n",
      "Epoch [76/100], Step [3700/13958], Loss: 164.4978\n",
      "Epoch [76/100], Step [3800/13958], Loss: 148.2803\n",
      "Epoch [76/100], Step [3900/13958], Loss: 391.1522\n",
      "Epoch [76/100], Step [4000/13958], Loss: 227.5037\n",
      "Epoch [76/100], Step [4100/13958], Loss: 84.5283\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [76/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [77/100], Step [100/13958], Loss: 159.1375\n",
      "Epoch [77/100], Step [200/13958], Loss: 258.4387\n",
      "Epoch [77/100], Step [300/13958], Loss: 437.4576\n",
      "Epoch [77/100], Step [400/13958], Loss: 442.8833\n",
      "Epoch [77/100], Step [500/13958], Loss: 455.3575\n",
      "Epoch [77/100], Step [600/13958], Loss: 399.9072\n",
      "Epoch [77/100], Step [700/13958], Loss: 187.6286\n",
      "Epoch [77/100], Step [800/13958], Loss: 314.4788\n",
      "Epoch [77/100], Step [900/13958], Loss: 196.9966\n",
      "Epoch [77/100], Step [1000/13958], Loss: 161.9075\n",
      "Epoch [77/100], Step [1100/13958], Loss: 378.2633\n",
      "Epoch [77/100], Step [1200/13958], Loss: 255.1814\n",
      "Epoch [77/100], Step [1300/13958], Loss: 309.3018\n",
      "Epoch [77/100], Step [1400/13958], Loss: 322.1115\n",
      "Epoch [77/100], Step [1500/13958], Loss: 218.8932\n",
      "Epoch [77/100], Step [1600/13958], Loss: 274.1676\n",
      "Epoch [77/100], Step [1700/13958], Loss: 229.4428\n",
      "Epoch [77/100], Step [1800/13958], Loss: 112.2370\n",
      "Epoch [77/100], Step [1900/13958], Loss: 81.3997\n",
      "Epoch [77/100], Step [2000/13958], Loss: 151.9087\n",
      "Epoch [77/100], Step [2100/13958], Loss: 54.1117\n",
      "Epoch [77/100], Step [2200/13958], Loss: 172.3382\n",
      "Epoch [77/100], Step [2300/13958], Loss: 240.3852\n",
      "Epoch [77/100], Step [2400/13958], Loss: 301.7480\n",
      "Epoch [77/100], Step [2500/13958], Loss: 286.4367\n",
      "Epoch [77/100], Step [2600/13958], Loss: 338.6469\n",
      "Epoch [77/100], Step [2700/13958], Loss: 304.5474\n",
      "Epoch [77/100], Step [2800/13958], Loss: 385.4715\n",
      "Epoch [77/100], Step [2900/13958], Loss: 158.4470\n",
      "Epoch [77/100], Step [3000/13958], Loss: 179.3708\n",
      "Epoch [77/100], Step [3100/13958], Loss: 181.3063\n",
      "Epoch [77/100], Step [3200/13958], Loss: 301.9389\n",
      "Epoch [77/100], Step [3300/13958], Loss: 289.1309\n",
      "Epoch [77/100], Step [3400/13958], Loss: 309.3031\n",
      "Epoch [77/100], Step [3500/13958], Loss: 494.6011\n",
      "Epoch [77/100], Step [3600/13958], Loss: 308.4962\n",
      "Epoch [77/100], Step [3700/13958], Loss: 220.3103\n",
      "Epoch [77/100], Step [3800/13958], Loss: 417.5595\n",
      "Epoch [77/100], Step [3900/13958], Loss: 272.5847\n",
      "Epoch [77/100], Step [4000/13958], Loss: 307.2671\n",
      "Epoch [77/100], Step [4100/13958], Loss: 449.6334\n",
      "Epoch [77/100], Step [4200/13958], Loss: 342.3383\n",
      "Epoch [77/100], Step [4300/13958], Loss: 196.6448\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [77/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [78/100], Step [100/13958], Loss: 523.4954\n",
      "Epoch [78/100], Step [200/13958], Loss: 424.8622\n",
      "Epoch [78/100], Step [300/13958], Loss: 163.5936\n",
      "Epoch [78/100], Step [400/13958], Loss: 153.1088\n",
      "Epoch [78/100], Step [500/13958], Loss: 260.3881\n",
      "Epoch [78/100], Step [600/13958], Loss: 156.5388\n",
      "Epoch [78/100], Step [700/13958], Loss: 128.1631\n",
      "Epoch [78/100], Step [800/13958], Loss: 378.9478\n",
      "Epoch [78/100], Step [900/13958], Loss: 648.3044\n",
      "Epoch [78/100], Step [1000/13958], Loss: 320.2671\n",
      "Epoch [78/100], Step [1100/13958], Loss: 182.8971\n",
      "Epoch [78/100], Step [1200/13958], Loss: 204.7285\n",
      "Epoch [78/100], Step [1300/13958], Loss: 295.3978\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [78/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [79/100], Step [100/13958], Loss: 437.2342\n",
      "Epoch [79/100], Step [200/13958], Loss: 395.1245\n",
      "Epoch [79/100], Step [300/13958], Loss: 121.3023\n",
      "Epoch [79/100], Step [400/13958], Loss: 174.5600\n",
      "Epoch [79/100], Step [500/13958], Loss: 160.5186\n",
      "Epoch [79/100], Step [600/13958], Loss: 312.9643\n",
      "Epoch [79/100], Step [700/13958], Loss: 219.5590\n",
      "Epoch [79/100], Step [800/13958], Loss: 248.9552\n",
      "Epoch [79/100], Step [900/13958], Loss: 389.4652\n",
      "Epoch [79/100], Step [1000/13958], Loss: 197.4238\n",
      "Epoch [79/100], Step [1100/13958], Loss: 685.5076\n",
      "Epoch [79/100], Step [1200/13958], Loss: 483.3780\n",
      "Epoch [79/100], Step [1300/13958], Loss: 159.7075\n",
      "Epoch [79/100], Step [1400/13958], Loss: 400.2309\n",
      "Epoch [79/100], Step [1500/13958], Loss: 123.3808\n",
      "Epoch [79/100], Step [1600/13958], Loss: 212.5485\n",
      "Epoch [79/100], Step [1700/13958], Loss: 433.0928\n",
      "Epoch [79/100], Step [1800/13958], Loss: 295.6084\n",
      "Epoch [79/100], Step [1900/13958], Loss: 403.8139\n",
      "Epoch [79/100], Step [2000/13958], Loss: 292.7561\n",
      "Epoch [79/100], Step [2100/13958], Loss: 165.8648\n",
      "Epoch [79/100], Step [2200/13958], Loss: 97.7620\n",
      "Epoch [79/100], Step [2300/13958], Loss: 200.2443\n",
      "Epoch [79/100], Step [2400/13958], Loss: 168.7367\n",
      "Epoch [79/100], Step [2500/13958], Loss: 237.9546\n",
      "Epoch [79/100], Step [2600/13958], Loss: 172.7046\n",
      "Epoch [79/100], Step [2700/13958], Loss: 175.6933\n",
      "Epoch [79/100], Step [2800/13958], Loss: 385.6117\n",
      "Epoch [79/100], Step [2900/13958], Loss: 197.2704\n",
      "Epoch [79/100], Step [3000/13958], Loss: 244.5105\n",
      "Epoch [79/100], Step [3100/13958], Loss: 179.5510\n",
      "Epoch [79/100], Step [3200/13958], Loss: 307.4338\n",
      "Epoch [79/100], Step [3300/13958], Loss: 164.4347\n",
      "Epoch [79/100], Step [3400/13958], Loss: 65.1504\n",
      "Epoch [79/100], Step [3500/13958], Loss: 190.5221\n",
      "Epoch [79/100], Step [3600/13958], Loss: 130.3844\n",
      "Epoch [79/100], Step [3700/13958], Loss: 68.6551\n",
      "Epoch [79/100], Step [3800/13958], Loss: 348.5091\n",
      "Epoch [79/100], Step [3900/13958], Loss: 400.0247\n",
      "Epoch [79/100], Step [4000/13958], Loss: 350.1987\n",
      "Epoch [79/100], Step [4100/13958], Loss: 152.2657\n",
      "Epoch [79/100], Step [4200/13958], Loss: 267.0974\n",
      "Epoch [79/100], Step [4300/13958], Loss: 594.5439\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [79/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [80/100], Step [100/13958], Loss: 174.2533\n",
      "Epoch [80/100], Step [200/13958], Loss: 197.2378\n",
      "Epoch [80/100], Step [300/13958], Loss: 119.8155\n",
      "Epoch [80/100], Step [400/13958], Loss: 97.8482\n",
      "Epoch [80/100], Step [500/13958], Loss: 416.7669\n",
      "Epoch [80/100], Step [600/13958], Loss: 286.8087\n",
      "Epoch [80/100], Step [700/13958], Loss: 205.6600\n",
      "Epoch [80/100], Step [800/13958], Loss: 423.7932\n",
      "Epoch [80/100], Step [900/13958], Loss: 433.1620\n",
      "Epoch [80/100], Step [1000/13958], Loss: 294.6813\n",
      "Epoch [80/100], Step [1100/13958], Loss: 480.4856\n",
      "Epoch [80/100], Step [1200/13958], Loss: 145.6357\n",
      "Epoch [80/100], Step [1300/13958], Loss: 152.2319\n",
      "Epoch [80/100], Step [1400/13958], Loss: 183.4227\n",
      "Epoch [80/100], Step [1500/13958], Loss: 244.5817\n",
      "Epoch [80/100], Step [1600/13958], Loss: 294.7481\n",
      "Epoch [80/100], Step [1700/13958], Loss: 430.1934\n",
      "Epoch [80/100], Step [1800/13958], Loss: 319.7204\n",
      "Epoch [80/100], Step [1900/13958], Loss: 277.1319\n",
      "Epoch [80/100], Step [2000/13958], Loss: 474.8936\n",
      "Epoch [80/100], Step [2100/13958], Loss: 178.0968\n",
      "Epoch [80/100], Step [2200/13958], Loss: 432.6232\n",
      "Epoch [80/100], Step [2300/13958], Loss: 161.3143\n",
      "Epoch [80/100], Step [2400/13958], Loss: 446.4593\n",
      "Epoch [80/100], Step [2500/13958], Loss: 259.9599\n",
      "Epoch [80/100], Step [2600/13958], Loss: 304.4337\n",
      "Epoch [80/100], Step [2700/13958], Loss: 195.8596\n",
      "Epoch [80/100], Step [2800/13958], Loss: 438.1551\n",
      "Epoch [80/100], Step [2900/13958], Loss: 413.3955\n",
      "Epoch [80/100], Step [3000/13958], Loss: 287.4467\n",
      "Epoch [80/100], Step [3100/13958], Loss: 279.4244\n",
      "Epoch [80/100], Step [3200/13958], Loss: 62.9046\n",
      "Epoch [80/100], Step [3300/13958], Loss: 337.6469\n",
      "Epoch [80/100], Step [3400/13958], Loss: 189.5860\n",
      "Epoch [80/100], Step [3500/13958], Loss: 221.9151\n",
      "Epoch [80/100], Step [3600/13958], Loss: 338.0837\n",
      "Epoch [80/100], Step [3700/13958], Loss: 123.9724\n",
      "Epoch [80/100], Step [3800/13958], Loss: 318.1189\n",
      "Epoch [80/100], Step [3900/13958], Loss: 206.1008\n",
      "Epoch [80/100], Step [4000/13958], Loss: 202.9611\n",
      "Epoch [80/100], Step [4100/13958], Loss: 491.5446\n",
      "Epoch [80/100], Step [4200/13958], Loss: 595.1716\n",
      "Epoch [80/100], Step [4300/13958], Loss: 183.3908\n",
      "Epoch [80/100], Step [4400/13958], Loss: 156.3850\n",
      "Epoch [80/100], Step [4500/13958], Loss: 147.4174\n",
      "Epoch [80/100], Step [4600/13958], Loss: 283.0883\n",
      "Epoch [80/100], Step [4700/13958], Loss: 183.6544\n",
      "Epoch [80/100], Step [4800/13958], Loss: 292.5097\n",
      "Epoch [80/100], Step [4900/13958], Loss: 140.6229\n",
      "Epoch [80/100], Step [5000/13958], Loss: 255.7912\n",
      "Epoch [80/100], Step [5100/13958], Loss: 284.1365\n",
      "Epoch [80/100], Step [5200/13958], Loss: 440.5683\n",
      "Epoch [80/100], Step [5300/13958], Loss: 275.6018\n",
      "Epoch [80/100], Step [5400/13958], Loss: 177.6289\n",
      "Epoch [80/100], Step [5500/13958], Loss: 249.8878\n",
      "Epoch [80/100], Step [5600/13958], Loss: 76.9059\n",
      "Epoch [80/100], Step [5700/13958], Loss: 183.7143\n",
      "Epoch [80/100], Step [5800/13958], Loss: 307.4895\n",
      "Epoch [80/100], Step [5900/13958], Loss: 236.1739\n",
      "Epoch [80/100], Step [6000/13958], Loss: 346.5209\n",
      "Epoch [80/100], Step [6100/13958], Loss: 204.0356\n",
      "Epoch [80/100], Step [6200/13958], Loss: 270.0296\n",
      "Epoch [80/100], Step [6300/13958], Loss: 264.6645\n",
      "Epoch [80/100], Step [6400/13958], Loss: 631.6550\n",
      "Epoch [80/100], Step [6500/13958], Loss: 142.2095\n",
      "Epoch [80/100], Step [6600/13958], Loss: 313.8547\n",
      "Epoch [80/100], Step [6700/13958], Loss: 147.3054\n",
      "Epoch [80/100], Step [6800/13958], Loss: 228.2549\n",
      "Epoch [80/100], Step [6900/13958], Loss: 114.8287\n",
      "Epoch [80/100], Step [7000/13958], Loss: 129.9129\n",
      "Epoch [80/100], Step [7100/13958], Loss: 195.4395\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [80/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [81/100], Step [100/13958], Loss: 155.0390\n",
      "Epoch [81/100], Step [200/13958], Loss: 74.6551\n",
      "Epoch [81/100], Step [300/13958], Loss: 196.8714\n",
      "Epoch [81/100], Step [400/13958], Loss: 269.6399\n",
      "Epoch [81/100], Step [500/13958], Loss: 357.6905\n",
      "Epoch [81/100], Step [600/13958], Loss: 365.2957\n",
      "Epoch [81/100], Step [700/13958], Loss: 204.0762\n",
      "Epoch [81/100], Step [800/13958], Loss: 170.6349\n",
      "Epoch [81/100], Step [900/13958], Loss: 244.0778\n",
      "Epoch [81/100], Step [1000/13958], Loss: 243.8354\n",
      "Epoch [81/100], Step [1100/13958], Loss: 214.0372\n",
      "Epoch [81/100], Step [1200/13958], Loss: 131.2336\n",
      "Epoch [81/100], Step [1300/13958], Loss: 116.6249\n",
      "Epoch [81/100], Step [1400/13958], Loss: 366.2252\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [81/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [82/100], Step [100/13958], Loss: 153.7768\n",
      "Epoch [82/100], Step [200/13958], Loss: 220.3401\n",
      "Epoch [82/100], Step [300/13958], Loss: 254.7954\n",
      "Epoch [82/100], Step [400/13958], Loss: 188.1375\n",
      "Epoch [82/100], Step [500/13958], Loss: 322.3119\n",
      "Epoch [82/100], Step [600/13958], Loss: 303.5846\n",
      "Epoch [82/100], Step [700/13958], Loss: 167.1416\n",
      "Epoch [82/100], Step [800/13958], Loss: 379.5616\n",
      "Epoch [82/100], Step [900/13958], Loss: 228.1232\n",
      "Epoch [82/100], Step [1000/13958], Loss: 198.0080\n",
      "Epoch [82/100], Step [1100/13958], Loss: 411.3809\n",
      "Epoch [82/100], Step [1200/13958], Loss: 532.5518\n",
      "Epoch [82/100], Step [1300/13958], Loss: 353.6946\n",
      "Epoch [82/100], Step [1400/13958], Loss: 299.8145\n",
      "Epoch [82/100], Step [1500/13958], Loss: 292.6651\n",
      "Epoch [82/100], Step [1600/13958], Loss: 219.3939\n",
      "Epoch [82/100], Step [1700/13958], Loss: 179.3665\n",
      "Epoch [82/100], Step [1800/13958], Loss: 130.7404\n",
      "Epoch [82/100], Step [1900/13958], Loss: 72.7381\n",
      "Epoch [82/100], Step [2000/13958], Loss: 379.2298\n",
      "Epoch [82/100], Step [2100/13958], Loss: 184.7826\n",
      "Epoch [82/100], Step [2200/13958], Loss: 395.7292\n",
      "Epoch [82/100], Step [2300/13958], Loss: 495.3658\n",
      "Epoch [82/100], Step [2400/13958], Loss: 169.6696\n",
      "Epoch [82/100], Step [2500/13958], Loss: 194.1355\n",
      "Epoch [82/100], Step [2600/13958], Loss: 152.5068\n",
      "Epoch [82/100], Step [2700/13958], Loss: 208.6191\n",
      "Epoch [82/100], Step [2800/13958], Loss: 218.1369\n",
      "Epoch [82/100], Step [2900/13958], Loss: 376.4213\n",
      "Epoch [82/100], Step [3000/13958], Loss: 397.0787\n",
      "Epoch [82/100], Step [3100/13958], Loss: 218.1183\n",
      "Epoch [82/100], Step [3200/13958], Loss: 264.9893\n",
      "Epoch [82/100], Step [3300/13958], Loss: 306.9226\n",
      "Epoch [82/100], Step [3400/13958], Loss: 308.4434\n",
      "Epoch [82/100], Step [3500/13958], Loss: 366.9308\n",
      "Epoch [82/100], Step [3600/13958], Loss: 116.9295\n",
      "Epoch [82/100], Step [3700/13958], Loss: 300.5883\n",
      "Epoch [82/100], Step [3800/13958], Loss: 263.7545\n",
      "Epoch [82/100], Step [3900/13958], Loss: 521.9850\n",
      "Epoch [82/100], Step [4000/13958], Loss: 372.6942\n",
      "Epoch [82/100], Step [4100/13958], Loss: 185.0984\n",
      "Epoch [82/100], Step [4200/13958], Loss: 223.7277\n",
      "Epoch [82/100], Step [4300/13958], Loss: 301.3683\n",
      "Epoch [82/100], Step [4400/13958], Loss: 135.5726\n",
      "Epoch [82/100], Step [4500/13958], Loss: 300.2034\n",
      "Epoch [82/100], Step [4600/13958], Loss: 489.7506\n",
      "Epoch [82/100], Step [4700/13958], Loss: 279.8361\n",
      "Epoch [82/100], Step [4800/13958], Loss: 233.1818\n",
      "Epoch [82/100], Step [4900/13958], Loss: 150.3637\n",
      "Epoch [82/100], Step [5000/13958], Loss: 256.4339\n",
      "Epoch [82/100], Step [5100/13958], Loss: 164.8553\n",
      "Epoch [82/100], Step [5200/13958], Loss: 292.6271\n",
      "Epoch [82/100], Step [5300/13958], Loss: 271.8605\n",
      "Epoch [82/100], Step [5400/13958], Loss: 238.0560\n",
      "Epoch [82/100], Step [5500/13958], Loss: 321.0322\n",
      "Epoch [82/100], Step [5600/13958], Loss: 359.2856\n",
      "Epoch [82/100], Step [5700/13958], Loss: 128.3842\n",
      "Epoch [82/100], Step [5800/13958], Loss: 137.6898\n",
      "Epoch [82/100], Step [5900/13958], Loss: 85.4380\n",
      "Epoch [82/100], Step [6000/13958], Loss: 279.7206\n",
      "Epoch [82/100], Step [6100/13958], Loss: 500.5224\n",
      "Epoch [82/100], Step [6200/13958], Loss: 139.3609\n",
      "Epoch [82/100], Step [6300/13958], Loss: 390.4127\n",
      "Epoch [82/100], Step [6400/13958], Loss: 275.4525\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [82/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [83/100], Step [100/13958], Loss: 136.7221\n",
      "Epoch [83/100], Step [200/13958], Loss: 133.8697\n",
      "Epoch [83/100], Step [300/13958], Loss: 390.7924\n",
      "Epoch [83/100], Step [400/13958], Loss: 401.6339\n",
      "Epoch [83/100], Step [500/13958], Loss: 125.1516\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [83/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [84/100], Step [100/13958], Loss: 204.9722\n",
      "Epoch [84/100], Step [200/13958], Loss: 231.3174\n",
      "Epoch [84/100], Step [300/13958], Loss: 285.9604\n",
      "Epoch [84/100], Step [400/13958], Loss: 248.9182\n",
      "Epoch [84/100], Step [500/13958], Loss: 399.3595\n",
      "Epoch [84/100], Step [600/13958], Loss: 162.3329\n",
      "Epoch [84/100], Step [700/13958], Loss: 306.9713\n",
      "Epoch [84/100], Step [800/13958], Loss: 130.1425\n",
      "Epoch [84/100], Step [900/13958], Loss: 148.5164\n",
      "Epoch [84/100], Step [1000/13958], Loss: 262.0049\n",
      "Epoch [84/100], Step [1100/13958], Loss: 85.7032\n",
      "Epoch [84/100], Step [1200/13958], Loss: 317.4193\n",
      "Epoch [84/100], Step [1300/13958], Loss: 338.6119\n",
      "Epoch [84/100], Step [1400/13958], Loss: 165.5453\n",
      "Epoch [84/100], Step [1500/13958], Loss: 370.3019\n",
      "Epoch [84/100], Step [1600/13958], Loss: 388.7806\n",
      "Epoch [84/100], Step [1700/13958], Loss: 199.6718\n",
      "Epoch [84/100], Step [1800/13958], Loss: 78.1584\n",
      "Epoch [84/100], Step [1900/13958], Loss: 157.6665\n",
      "Epoch [84/100], Step [2000/13958], Loss: 142.6200\n",
      "Epoch [84/100], Step [2100/13958], Loss: 121.9456\n",
      "Epoch [84/100], Step [2200/13958], Loss: 272.2878\n",
      "Epoch [84/100], Step [2300/13958], Loss: 82.6252\n",
      "Epoch [84/100], Step [2400/13958], Loss: 303.9743\n",
      "Epoch [84/100], Step [2500/13958], Loss: 47.6928\n",
      "Epoch [84/100], Step [2600/13958], Loss: 250.7826\n",
      "Epoch [84/100], Step [2700/13958], Loss: 265.1141\n",
      "Epoch [84/100], Step [2800/13958], Loss: 300.7690\n",
      "Epoch [84/100], Step [2900/13958], Loss: 152.7454\n",
      "Epoch [84/100], Step [3000/13958], Loss: 184.3818\n",
      "Epoch [84/100], Step [3100/13958], Loss: 216.2973\n",
      "Epoch [84/100], Step [3200/13958], Loss: 326.6152\n",
      "Epoch [84/100], Step [3300/13958], Loss: 238.5153\n",
      "Epoch [84/100], Step [3400/13958], Loss: 207.2968\n",
      "Epoch [84/100], Step [3500/13958], Loss: 286.5928\n",
      "Epoch [84/100], Step [3600/13958], Loss: 102.3729\n",
      "Epoch [84/100], Step [3700/13958], Loss: 301.4404\n",
      "Epoch [84/100], Step [3800/13958], Loss: 60.1010\n",
      "Epoch [84/100], Step [3900/13958], Loss: 211.4574\n",
      "Epoch [84/100], Step [4000/13958], Loss: 188.8741\n",
      "Epoch [84/100], Step [4100/13958], Loss: 178.1712\n",
      "Epoch [84/100], Step [4200/13958], Loss: 216.1409\n",
      "Epoch [84/100], Step [4300/13958], Loss: 196.5584\n",
      "Epoch [84/100], Step [4400/13958], Loss: 187.0572\n",
      "Epoch [84/100], Step [4500/13958], Loss: 268.8013\n",
      "Epoch [84/100], Step [4600/13958], Loss: 231.0880\n",
      "Epoch [84/100], Step [4700/13958], Loss: 228.8929\n",
      "Epoch [84/100], Step [4800/13958], Loss: 295.6807\n",
      "Epoch [84/100], Step [4900/13958], Loss: 146.8309\n",
      "Epoch [84/100], Step [5000/13958], Loss: 159.7774\n",
      "Epoch [84/100], Step [5100/13958], Loss: 305.8966\n",
      "Epoch [84/100], Step [5200/13958], Loss: 112.9983\n",
      "Epoch [84/100], Step [5300/13958], Loss: 195.2335\n",
      "Epoch [84/100], Step [5400/13958], Loss: 298.6190\n",
      "Epoch [84/100], Step [5500/13958], Loss: 94.7028\n",
      "Epoch [84/100], Step [5600/13958], Loss: 200.6588\n",
      "Epoch [84/100], Step [5700/13958], Loss: 414.0069\n",
      "Epoch [84/100], Step [5800/13958], Loss: 317.8117\n",
      "Epoch [84/100], Step [5900/13958], Loss: 79.2275\n",
      "Epoch [84/100], Step [6000/13958], Loss: 180.9931\n",
      "Epoch [84/100], Step [6100/13958], Loss: 317.1819\n",
      "Epoch [84/100], Step [6200/13958], Loss: 365.4595\n",
      "Epoch [84/100], Step [6300/13958], Loss: 448.5502\n",
      "Epoch [84/100], Step [6400/13958], Loss: 160.5587\n",
      "Epoch [84/100], Step [6500/13958], Loss: 31.3926\n",
      "Epoch [84/100], Step [6600/13958], Loss: 263.1617\n",
      "Epoch [84/100], Step [6700/13958], Loss: 190.8362\n",
      "Epoch [84/100], Step [6800/13958], Loss: 226.9643\n",
      "Epoch [84/100], Step [6900/13958], Loss: 218.5649\n",
      "Epoch [84/100], Step [7000/13958], Loss: 185.8712\n",
      "Epoch [84/100], Step [7100/13958], Loss: 172.9741\n",
      "Epoch [84/100], Step [7200/13958], Loss: 357.0786\n",
      "Epoch [84/100], Step [7300/13958], Loss: 185.6931\n",
      "Epoch [84/100], Step [7400/13958], Loss: 183.7907\n",
      "Epoch [84/100], Step [7500/13958], Loss: 507.4438\n",
      "Epoch [84/100], Step [7600/13958], Loss: 171.4835\n",
      "Epoch [84/100], Step [7700/13958], Loss: 133.0802\n",
      "Epoch [84/100], Step [7800/13958], Loss: 176.7592\n",
      "Epoch [84/100], Step [7900/13958], Loss: 187.1245\n",
      "Epoch [84/100], Step [8000/13958], Loss: 142.2755\n",
      "Epoch [84/100], Step [8100/13958], Loss: 129.2971\n",
      "Epoch [84/100], Step [8200/13958], Loss: 552.0406\n",
      "Epoch [84/100], Step [8300/13958], Loss: 108.7648\n",
      "Epoch [84/100], Step [8400/13958], Loss: 269.1426\n",
      "Epoch [84/100], Step [8500/13958], Loss: 282.3646\n",
      "Epoch [84/100], Step [8600/13958], Loss: 287.5744\n",
      "Epoch [84/100], Step [8700/13958], Loss: 87.3660\n",
      "Epoch [84/100], Step [8800/13958], Loss: 317.7769\n",
      "Epoch [84/100], Step [8900/13958], Loss: 149.0091\n",
      "Epoch [84/100], Step [9000/13958], Loss: 342.6737\n",
      "Epoch [84/100], Step [9100/13958], Loss: 172.3717\n",
      "Epoch [84/100], Step [9200/13958], Loss: 293.5729\n",
      "Epoch [84/100], Step [9300/13958], Loss: 131.6754\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [84/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [85/100], Step [100/13958], Loss: 166.6605\n",
      "Epoch [85/100], Step [200/13958], Loss: 373.0107\n",
      "Epoch [85/100], Step [300/13958], Loss: 294.3270\n",
      "Epoch [85/100], Step [400/13958], Loss: 324.3896\n",
      "Epoch [85/100], Step [500/13958], Loss: 230.8011\n",
      "Epoch [85/100], Step [600/13958], Loss: 434.5802\n",
      "Epoch [85/100], Step [700/13958], Loss: 197.5102\n",
      "Epoch [85/100], Step [800/13958], Loss: 219.2925\n",
      "Epoch [85/100], Step [900/13958], Loss: 170.4062\n",
      "Epoch [85/100], Step [1000/13958], Loss: 106.6648\n",
      "Epoch [85/100], Step [1100/13958], Loss: 375.9523\n",
      "Epoch [85/100], Step [1200/13958], Loss: 181.9775\n",
      "Epoch [85/100], Step [1300/13958], Loss: 242.6862\n",
      "Epoch [85/100], Step [1400/13958], Loss: 174.9628\n",
      "Epoch [85/100], Step [1500/13958], Loss: 125.8901\n",
      "Epoch [85/100], Step [1600/13958], Loss: 208.2017\n",
      "Epoch [85/100], Step [1700/13958], Loss: 343.4417\n",
      "Epoch [85/100], Step [1800/13958], Loss: 472.1646\n",
      "Epoch [85/100], Step [1900/13958], Loss: 283.7283\n",
      "Epoch [85/100], Step [2000/13958], Loss: 120.7122\n",
      "Epoch [85/100], Step [2100/13958], Loss: 258.1957\n",
      "Epoch [85/100], Step [2200/13958], Loss: 78.0002\n",
      "Epoch [85/100], Step [2300/13958], Loss: 213.5621\n",
      "Epoch [85/100], Step [2400/13958], Loss: 367.0230\n",
      "Epoch [85/100], Step [2500/13958], Loss: 192.8975\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [85/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [86/100], Step [100/13958], Loss: 336.0701\n",
      "Epoch [86/100], Step [200/13958], Loss: 172.9345\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [86/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [87/100], Step [100/13958], Loss: 97.6188\n",
      "Epoch [87/100], Step [200/13958], Loss: 294.1291\n",
      "Epoch [87/100], Step [300/13958], Loss: 389.5495\n",
      "Epoch [87/100], Step [400/13958], Loss: 446.7007\n",
      "Epoch [87/100], Step [500/13958], Loss: 58.8568\n",
      "Epoch [87/100], Step [600/13958], Loss: 90.2015\n",
      "Epoch [87/100], Step [700/13958], Loss: 382.3750\n",
      "Epoch [87/100], Step [800/13958], Loss: 205.4177\n",
      "Epoch [87/100], Step [900/13958], Loss: 409.6887\n",
      "Epoch [87/100], Step [1000/13958], Loss: 515.5999\n",
      "Epoch [87/100], Step [1100/13958], Loss: 236.2853\n",
      "Epoch [87/100], Step [1200/13958], Loss: 151.4322\n",
      "Epoch [87/100], Step [1300/13958], Loss: 180.4314\n",
      "Epoch [87/100], Step [1400/13958], Loss: 189.2832\n",
      "Epoch [87/100], Step [1500/13958], Loss: 133.3953\n",
      "Epoch [87/100], Step [1600/13958], Loss: 499.7298\n",
      "Epoch [87/100], Step [1700/13958], Loss: 215.7094\n",
      "Epoch [87/100], Step [1800/13958], Loss: 577.3307\n",
      "Epoch [87/100], Step [1900/13958], Loss: 147.4016\n",
      "Epoch [87/100], Step [2000/13958], Loss: 184.1713\n",
      "Epoch [87/100], Step [2100/13958], Loss: 446.0098\n",
      "Epoch [87/100], Step [2200/13958], Loss: 596.9490\n",
      "Epoch [87/100], Step [2300/13958], Loss: 273.2695\n",
      "Epoch [87/100], Step [2400/13958], Loss: 216.5375\n",
      "Epoch [87/100], Step [2500/13958], Loss: 191.5571\n",
      "Epoch [87/100], Step [2600/13958], Loss: 406.3775\n",
      "Epoch [87/100], Step [2700/13958], Loss: 373.8195\n",
      "Epoch [87/100], Step [2800/13958], Loss: 306.3255\n",
      "Epoch [87/100], Step [2900/13958], Loss: 308.4781\n",
      "Epoch [87/100], Step [3000/13958], Loss: 73.5285\n",
      "Epoch [87/100], Step [3100/13958], Loss: 167.1417\n",
      "Epoch [87/100], Step [3200/13958], Loss: 73.1809\n",
      "Epoch [87/100], Step [3300/13958], Loss: 320.2017\n",
      "Epoch [87/100], Step [3400/13958], Loss: 302.2714\n",
      "Epoch [87/100], Step [3500/13958], Loss: 188.7354\n",
      "Epoch [87/100], Step [3600/13958], Loss: 524.3641\n",
      "Epoch [87/100], Step [3700/13958], Loss: 634.5863\n",
      "Epoch [87/100], Step [3800/13958], Loss: 148.4042\n",
      "Epoch [87/100], Step [3900/13958], Loss: 481.9365\n",
      "Epoch [87/100], Step [4000/13958], Loss: 282.4653\n",
      "Epoch [87/100], Step [4100/13958], Loss: 198.4566\n",
      "Epoch [87/100], Step [4200/13958], Loss: 244.5611\n",
      "Epoch [87/100], Step [4300/13958], Loss: 253.2378\n",
      "Epoch [87/100], Step [4400/13958], Loss: 292.8441\n",
      "Epoch [87/100], Step [4500/13958], Loss: 384.4501\n",
      "Epoch [87/100], Step [4600/13958], Loss: 416.5557\n",
      "Epoch [87/100], Step [4700/13958], Loss: 73.4635\n",
      "Epoch [87/100], Step [4800/13958], Loss: 204.9203\n",
      "Epoch [87/100], Step [4900/13958], Loss: 136.3450\n",
      "Epoch [87/100], Step [5000/13958], Loss: 134.1364\n",
      "Epoch [87/100], Step [5100/13958], Loss: 329.8338\n",
      "Epoch [87/100], Step [5200/13958], Loss: 186.3395\n",
      "Epoch [87/100], Step [5300/13958], Loss: 197.2120\n",
      "Epoch [87/100], Step [5400/13958], Loss: 168.1224\n",
      "Epoch [87/100], Step [5500/13958], Loss: 153.1636\n",
      "Epoch [87/100], Step [5600/13958], Loss: 464.5111\n",
      "Epoch [87/100], Step [5700/13958], Loss: 235.4637\n",
      "Epoch [87/100], Step [5800/13958], Loss: 370.6968\n",
      "Epoch [87/100], Step [5900/13958], Loss: 212.9798\n",
      "Epoch [87/100], Step [6000/13958], Loss: 297.9343\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [87/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [88/100], Step [100/13958], Loss: 245.0720\n",
      "Epoch [88/100], Step [200/13958], Loss: 80.2270\n",
      "Epoch [88/100], Step [300/13958], Loss: 250.6123\n",
      "Epoch [88/100], Step [400/13958], Loss: 247.7350\n",
      "Epoch [88/100], Step [500/13958], Loss: 317.4276\n",
      "Epoch [88/100], Step [600/13958], Loss: 110.5070\n",
      "Epoch [88/100], Step [700/13958], Loss: 376.9212\n",
      "Epoch [88/100], Step [800/13958], Loss: 464.4625\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [88/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [89/100], Step [100/13958], Loss: 302.2125\n",
      "Epoch [89/100], Step [200/13958], Loss: 258.6017\n",
      "Epoch [89/100], Step [300/13958], Loss: 495.7310\n",
      "Epoch [89/100], Step [400/13958], Loss: 509.1704\n",
      "Epoch [89/100], Step [500/13958], Loss: 243.1169\n",
      "Epoch [89/100], Step [600/13958], Loss: 279.1732\n",
      "Epoch [89/100], Step [700/13958], Loss: 105.1978\n",
      "Epoch [89/100], Step [800/13958], Loss: 323.7429\n",
      "Epoch [89/100], Step [900/13958], Loss: 125.1667\n",
      "Epoch [89/100], Step [1000/13958], Loss: 134.5899\n",
      "Epoch [89/100], Step [1100/13958], Loss: 193.4074\n",
      "Epoch [89/100], Step [1200/13958], Loss: 276.0913\n",
      "Epoch [89/100], Step [1300/13958], Loss: 332.0685\n",
      "Epoch [89/100], Step [1400/13958], Loss: 127.1705\n",
      "Epoch [89/100], Step [1500/13958], Loss: 169.1544\n",
      "Epoch [89/100], Step [1600/13958], Loss: 157.9160\n",
      "Epoch [89/100], Step [1700/13958], Loss: 121.5037\n",
      "Epoch [89/100], Step [1800/13958], Loss: 331.1789\n",
      "Epoch [89/100], Step [1900/13958], Loss: 314.7385\n",
      "Epoch [89/100], Step [2000/13958], Loss: 376.2385\n",
      "Epoch [89/100], Step [2100/13958], Loss: 252.3399\n",
      "Epoch [89/100], Step [2200/13958], Loss: 339.7627\n",
      "Epoch [89/100], Step [2300/13958], Loss: 191.7775\n",
      "Epoch [89/100], Step [2400/13958], Loss: 138.9682\n",
      "Epoch [89/100], Step [2500/13958], Loss: 430.5006\n",
      "Epoch [89/100], Step [2600/13958], Loss: 341.0616\n",
      "Epoch [89/100], Step [2700/13958], Loss: 483.0096\n",
      "Epoch [89/100], Step [2800/13958], Loss: 176.7245\n",
      "Epoch [89/100], Step [2900/13958], Loss: 104.3778\n",
      "Epoch [89/100], Step [3000/13958], Loss: 329.6535\n",
      "Epoch [89/100], Step [3100/13958], Loss: 202.9106\n",
      "Epoch [89/100], Step [3200/13958], Loss: 72.9679\n",
      "Epoch [89/100], Step [3300/13958], Loss: 290.9841\n",
      "Epoch [89/100], Step [3400/13958], Loss: 96.3841\n",
      "Epoch [89/100], Step [3500/13958], Loss: 136.5135\n",
      "Epoch [89/100], Step [3600/13958], Loss: 200.9562\n",
      "Epoch [89/100], Step [3700/13958], Loss: 155.2664\n",
      "Epoch [89/100], Step [3800/13958], Loss: 239.5907\n",
      "Epoch [89/100], Step [3900/13958], Loss: 177.8245\n",
      "Epoch [89/100], Step [4000/13958], Loss: 140.7750\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [89/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [90/100], Step [100/13958], Loss: 264.4792\n",
      "Epoch [90/100], Step [200/13958], Loss: 221.7692\n",
      "Epoch [90/100], Step [300/13958], Loss: 193.8549\n",
      "Epoch [90/100], Step [400/13958], Loss: 111.8438\n",
      "Epoch [90/100], Step [500/13958], Loss: 319.9310\n",
      "Epoch [90/100], Step [600/13958], Loss: 204.4889\n",
      "Epoch [90/100], Step [700/13958], Loss: 170.1335\n",
      "Epoch [90/100], Step [800/13958], Loss: 214.6823\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [90/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [91/100], Step [100/13958], Loss: 171.1014\n",
      "Epoch [91/100], Step [200/13958], Loss: 163.3734\n",
      "Epoch [91/100], Step [300/13958], Loss: 225.1569\n",
      "Epoch [91/100], Step [400/13958], Loss: 251.8404\n",
      "Epoch [91/100], Step [500/13958], Loss: 209.6341\n",
      "Epoch [91/100], Step [600/13958], Loss: 197.9995\n",
      "Epoch [91/100], Step [700/13958], Loss: 369.3708\n",
      "Epoch [91/100], Step [800/13958], Loss: 202.2252\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [91/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [92/100], Step [100/13958], Loss: 277.1831\n",
      "Epoch [92/100], Step [200/13958], Loss: 234.3266\n",
      "Epoch [92/100], Step [300/13958], Loss: 152.5038\n",
      "Epoch [92/100], Step [400/13958], Loss: 310.9736\n",
      "Epoch [92/100], Step [500/13958], Loss: 162.2470\n",
      "Epoch [92/100], Step [600/13958], Loss: 336.2496\n",
      "Epoch [92/100], Step [700/13958], Loss: 429.2551\n",
      "Epoch [92/100], Step [800/13958], Loss: 126.8200\n",
      "Epoch [92/100], Step [900/13958], Loss: 167.3821\n",
      "Epoch [92/100], Step [1000/13958], Loss: 156.3281\n",
      "Epoch [92/100], Step [1100/13958], Loss: 353.6574\n",
      "Epoch [92/100], Step [1200/13958], Loss: 105.6092\n",
      "Epoch [92/100], Step [1300/13958], Loss: 256.1134\n",
      "Epoch [92/100], Step [1400/13958], Loss: 216.6305\n",
      "Epoch [92/100], Step [1500/13958], Loss: 162.9024\n",
      "Epoch [92/100], Step [1600/13958], Loss: 495.4410\n",
      "Epoch [92/100], Step [1700/13958], Loss: 233.7535\n",
      "Epoch [92/100], Step [1800/13958], Loss: 416.7981\n",
      "Epoch [92/100], Step [1900/13958], Loss: 104.2347\n",
      "Epoch [92/100], Step [2000/13958], Loss: 171.2068\n",
      "Epoch [92/100], Step [2100/13958], Loss: 197.7097\n",
      "Epoch [92/100], Step [2200/13958], Loss: 273.9411\n",
      "Epoch [92/100], Step [2300/13958], Loss: 202.8601\n",
      "Epoch [92/100], Step [2400/13958], Loss: 378.8291\n",
      "Epoch [92/100], Step [2500/13958], Loss: 402.9228\n",
      "Epoch [92/100], Step [2600/13958], Loss: 366.2369\n",
      "Epoch [92/100], Step [2700/13958], Loss: 165.8131\n",
      "Epoch [92/100], Step [2800/13958], Loss: 176.4490\n",
      "Epoch [92/100], Step [2900/13958], Loss: 175.2131\n",
      "Epoch [92/100], Step [3000/13958], Loss: 286.5064\n",
      "Epoch [92/100], Step [3100/13958], Loss: 111.0890\n",
      "Epoch [92/100], Step [3200/13958], Loss: 337.9446\n",
      "Epoch [92/100], Step [3300/13958], Loss: 190.0918\n",
      "Epoch [92/100], Step [3400/13958], Loss: 270.0542\n",
      "Epoch [92/100], Step [3500/13958], Loss: 202.9184\n",
      "Epoch [92/100], Step [3600/13958], Loss: 203.0303\n",
      "Epoch [92/100], Step [3700/13958], Loss: 327.6994\n",
      "Epoch [92/100], Step [3800/13958], Loss: 236.6062\n",
      "Epoch [92/100], Step [3900/13958], Loss: 179.1125\n",
      "Epoch [92/100], Step [4000/13958], Loss: 253.7781\n",
      "Epoch [92/100], Step [4100/13958], Loss: 353.8006\n",
      "Epoch [92/100], Step [4200/13958], Loss: 433.4013\n",
      "Epoch [92/100], Step [4300/13958], Loss: 266.4004\n",
      "Epoch [92/100], Step [4400/13958], Loss: 184.2689\n",
      "Epoch [92/100], Step [4500/13958], Loss: 384.6520\n",
      "Epoch [92/100], Step [4600/13958], Loss: 257.9779\n",
      "Epoch [92/100], Step [4700/13958], Loss: 132.4875\n",
      "Epoch [92/100], Step [4800/13958], Loss: 407.3188\n",
      "Epoch [92/100], Step [4900/13958], Loss: 84.1480\n",
      "Epoch [92/100], Step [5000/13958], Loss: 243.5052\n",
      "Epoch [92/100], Step [5100/13958], Loss: 439.9838\n",
      "Epoch [92/100], Step [5200/13958], Loss: 47.5798\n",
      "Epoch [92/100], Step [5300/13958], Loss: 471.5602\n",
      "Epoch [92/100], Step [5400/13958], Loss: 186.3266\n",
      "Epoch [92/100], Step [5500/13958], Loss: 287.6473\n",
      "Epoch [92/100], Step [5600/13958], Loss: 307.9858\n",
      "Epoch [92/100], Step [5700/13958], Loss: 223.6277\n",
      "Epoch [92/100], Step [5800/13958], Loss: 359.8055\n",
      "Epoch [92/100], Step [5900/13958], Loss: 163.4798\n",
      "Epoch [92/100], Step [6000/13958], Loss: 295.3148\n",
      "Epoch [92/100], Step [6100/13958], Loss: 326.3077\n",
      "Epoch [92/100], Step [6200/13958], Loss: 102.3115\n",
      "Epoch [92/100], Step [6300/13958], Loss: 265.0047\n",
      "Epoch [92/100], Step [6400/13958], Loss: 344.3786\n",
      "Epoch [92/100], Step [6500/13958], Loss: 154.8381\n",
      "Epoch [92/100], Step [6600/13958], Loss: 265.7815\n",
      "Epoch [92/100], Step [6700/13958], Loss: 209.2155\n",
      "Epoch [92/100], Step [6800/13958], Loss: 323.6436\n",
      "Epoch [92/100], Step [6900/13958], Loss: 351.0953\n",
      "Epoch [92/100], Step [7000/13958], Loss: 237.1843\n",
      "Epoch [92/100], Step [7100/13958], Loss: 450.5815\n",
      "Epoch [92/100], Step [7200/13958], Loss: 446.6126\n",
      "Epoch [92/100], Step [7300/13958], Loss: 200.8354\n",
      "Epoch [92/100], Step [7400/13958], Loss: 151.7108\n",
      "Epoch [92/100], Step [7500/13958], Loss: 342.1961\n",
      "Epoch [92/100], Step [7600/13958], Loss: 273.8798\n",
      "Epoch [92/100], Step [7700/13958], Loss: 301.5142\n",
      "Epoch [92/100], Step [7800/13958], Loss: 270.5609\n",
      "Epoch [92/100], Step [7900/13958], Loss: 342.9401\n",
      "Epoch [92/100], Step [8000/13958], Loss: 313.6400\n",
      "Epoch [92/100], Step [8100/13958], Loss: 160.1248\n",
      "Epoch [92/100], Step [8200/13958], Loss: 374.0744\n",
      "Epoch [92/100], Step [8300/13958], Loss: 318.6545\n",
      "Epoch [92/100], Step [8400/13958], Loss: 582.0508\n",
      "Epoch [92/100], Step [8500/13958], Loss: 88.1041\n",
      "Epoch [92/100], Step [8600/13958], Loss: 118.9666\n",
      "Epoch [92/100], Step [8700/13958], Loss: 137.6808\n",
      "Epoch [92/100], Step [8800/13958], Loss: 163.5503\n",
      "Epoch [92/100], Step [8900/13958], Loss: 188.2155\n",
      "Epoch [92/100], Step [9000/13958], Loss: 99.1707\n",
      "Epoch [92/100], Step [9100/13958], Loss: 531.4105\n",
      "Epoch [92/100], Step [9200/13958], Loss: 132.6400\n",
      "Epoch [92/100], Step [9300/13958], Loss: 270.7518\n",
      "Epoch [92/100], Step [9400/13958], Loss: 152.7618\n",
      "Epoch [92/100], Step [9500/13958], Loss: 164.3098\n",
      "Epoch [92/100], Step [9600/13958], Loss: 268.5937\n",
      "Epoch [92/100], Step [9700/13958], Loss: 362.4565\n",
      "Epoch [92/100], Step [9800/13958], Loss: 328.1476\n",
      "Epoch [92/100], Step [9900/13958], Loss: 414.1295\n",
      "Epoch [92/100], Step [10000/13958], Loss: 194.2698\n",
      "Epoch [92/100], Step [10100/13958], Loss: 398.0953\n",
      "Epoch [92/100], Step [10200/13958], Loss: 360.9862\n",
      "Epoch [92/100], Step [10300/13958], Loss: 355.8289\n",
      "Epoch [92/100], Step [10400/13958], Loss: 89.5735\n",
      "Epoch [92/100], Step [10500/13958], Loss: 126.6859\n",
      "Epoch [92/100], Step [10600/13958], Loss: 250.5494\n",
      "Epoch [92/100], Step [10700/13958], Loss: 288.5683\n",
      "Epoch [92/100], Step [10800/13958], Loss: 699.7723\n",
      "Epoch [92/100], Step [10900/13958], Loss: 567.1799\n",
      "Epoch [92/100], Step [11000/13958], Loss: 138.3101\n",
      "Epoch [92/100], Step [11100/13958], Loss: 176.8180\n",
      "Epoch [92/100], Step [11200/13958], Loss: 305.9259\n",
      "Epoch [92/100], Step [11300/13958], Loss: 155.4982\n",
      "Epoch [92/100], Step [11400/13958], Loss: 275.4566\n",
      "Epoch [92/100], Step [11500/13958], Loss: 460.5402\n",
      "Epoch [92/100], Step [11600/13958], Loss: 303.7238\n",
      "Epoch [92/100], Step [11700/13958], Loss: 442.0395\n",
      "NaN loss encountered. Exiting training loop.\n",
      "Epoch [92/100], Train Loss: nan, Val Loss: nan\n",
      "Epoch [93/100], Step [100/13958], Loss: 395.4028\n",
      "Epoch [93/100], Step [200/13958], Loss: 166.2169\n",
      "Epoch [93/100], Step [300/13958], Loss: 547.8346\n",
      "Epoch [93/100], Step [400/13958], Loss: 316.8019\n",
      "Epoch [93/100], Step [500/13958], Loss: 446.5779\n",
      "Epoch [93/100], Step [600/13958], Loss: 508.8667\n",
      "Epoch [93/100], Step [700/13958], Loss: 104.3539\n",
      "Epoch [93/100], Step [800/13958], Loss: 388.2467\n",
      "Epoch [93/100], Step [900/13958], Loss: 199.5858\n",
      "Epoch [93/100], Step [1000/13958], Loss: 194.5894\n",
      "Epoch [93/100], Step [1100/13958], Loss: 323.9638\n",
      "Epoch [93/100], Step [1200/13958], Loss: 241.4023\n",
      "Epoch [93/100], Step [1300/13958], Loss: 291.8821\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Performance Tracking\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Validation DataLoader: 'val_loader'\n",
    "learning_rate = 0.1\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Reset optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        train_loss = loss_fn(outputs, targets)\n",
    "\n",
    "        if torch.isnan(train_loss):\n",
    "            print(\"NaN loss encountered. Exiting training loop.\")\n",
    "            break\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        # train_loss.backward()\n",
    "        train_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record training loss\n",
    "        training_losses.append(train_loss.item())  \n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {train_loss.item():.4f}')\n",
    "\n",
    "    # Validation Loop\n",
    "    with torch.no_grad():  # No gradient calculations during validation\n",
    "        val_loss = 0.0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += loss_fn(outputs, targets).item() \n",
    "        val_loss /= len(val_loader)\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Visualization\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     y_pred = model(X)\n",
    "#     loss = loss_fn(y_pred, y.view(-1, 1))\n",
    "#     print(f'Loss: {loss.item():.4f}')\n",
    "    \n",
    "#     # Print the first 5 predictions\n",
    "#     y_pred = y_pred.cpu()\n",
    "#     y = y.cpu()\n",
    "#     print(y_pred[:5].numpy().flatten())\n",
    "#     print(y[:5].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
