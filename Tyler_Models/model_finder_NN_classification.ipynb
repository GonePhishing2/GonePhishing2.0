{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML libraries\n",
    "import pycaret\n",
    "import xgboost\n",
    "\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.functions import datediff, to_date, col, expr\n",
    "\n",
    "# Import Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "from pycaret.classification import setup, compare_models\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open(\"connection.json\"))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the correct table\n",
    "tableName = 'PURCHASE_ORDER_HISTORY'\n",
    "dataframe = session.table(tableName)\n",
    "\n",
    "# Calculation to find the lag between Planned Delivery from Actual Delivery\n",
    "dataframe = dataframe.withColumn(\"target_feature\",\n",
    "                                    datediff('day', \n",
    "                                            col(\"DELIVERY_DATE_ML\"), \n",
    "                                            col(\"FIRST_GR_POSTING_DATE_ML\")))\n",
    "\n",
    "\n",
    "# Example: Selecting specific columns\n",
    "# This selects only a subset of columns. Adjust the column names as needed.\n",
    "filtered_dataframe = dataframe.select(\n",
    "    col(\"PURCHASE_DOCUMENT_ITEM_ID\"), # ID for purchase order\n",
    "    col(\"CREATE_DATE_ML\"),            # day purchase order was created\n",
    "    col(\"COMPANY_CODE_ID\"),           # copmany w/in INVISTA making purchase\n",
    "    col(\"VENDOR_ID\"),                 # ID of the vendor \"we\" are purchasing from\n",
    "    col(\"POSTAL_CD\"),                 # postal code associated w company code ID\n",
    "    col(\"MATERIAL_ID\"),               # ID of material being purchase\n",
    "    col(\"SUB_COMMODITY_DESC\"),        # description of sub commodity\n",
    "    col(\"MRP_TYPE_ID\"),               # determined if material is reordered manually or automatically\n",
    "    col(\"PLANT_ID\"),                  # ID of plant making purchase\n",
    "    col(\"REQUESTED_DELIVERY_DATE_ML\"),# delivery date from requisition\n",
    "    col(\"INBOUND_DELIVERY_ID\"),       # ID for delivery\n",
    "    col(\"INBOUND_DELIVERY_ITEM_ID\"),  # ID of item w/in delivery\n",
    "    col(\"PLANNED_DELIVERY_DAYS\"),     # Amount of days expected to take\n",
    "    col(\"FIRST_GR_POSTING_DATE_ML\"),  # expected delivery date        \n",
    "    col(\"target_feature\")             # Lag between Planned Delivery from Actual Delivery \n",
    ")\n",
    "\n",
    "\n",
    "# Print a sample of the filtered dataframe to standard output.\n",
    "filtered_dataframe.show()\n",
    "\n",
    "# Optionally, you might want to filter rows based on some conditions\n",
    "# Example: Filtering out rows where FIRST_GR_POSTING_DATE_ML is NULL\n",
    "filtered_dataframe = filtered_dataframe.filter(col(\"FIRST_GR_POSTING_DATE_ML\").is_not_null())\n",
    "\n",
    "# Show the DataFrame after filtering\n",
    "filtered_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'filtered_dataframe' is the DataFrame you've prepared in Snowflake\n",
    "# Convert the Snowpark DataFrame to a Pandas DataFrame with consideration for NULL values\n",
    "\n",
    "# Convert DataFrame to Pandas, handling NULL values by allowing float conversion\n",
    "df = filtered_dataframe.fillna(0).to_pandas()  # This replaces NULL with 0 before conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform 'target_feature' to binary: 0 if value is positive, 1 if value is negative\n",
    "df['binary_target'] = df['TARGET_FEATURE'].apply(lambda x: 0 if x > 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the original 'target_feature' column from the DataFrame\n",
    "df = df.drop(columns=['TARGET_FEATURE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "# Splitting the data into features and target variable\n",
    "X = df.drop('binary_target', axis=1)\n",
    "y = df['binary_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying categorical columns for one-hot encoding\n",
    "categorical_features = ['COMPANY_CODE_ID', 'VENDOR_ID', 'POSTAL_CD', 'SUB_COMMODITY_DESC', 'MRP_TYPE_ID', 'PLANT_ID']\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Creating a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to training data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Assume num_features is known\n",
    "model = Net(num_features=X_train_tensor.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Model Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
